{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1441eca-c957-478f-82a2-42681ed2bbd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ed4bbb-eeb4-48cd-8746-af30d44e031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from pybtex.database import parse_file\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5284f-ef84-4e81-bd64-0c35762de00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ad0465-7ed6-4893-b9bf-448f1e77f3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bib_data = parse_file('data/anthology+abstracts.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba9e1ea-6fba-4a08-8930-32a26ef0a604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lieberman-etal-1965-automatic'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the last entry\n",
    "list(bib_data.entries.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a333e9-2641-4878-a3aa-78d28de781cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries in the anthology\n",
    "len(list(bib_data.entries.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ada274-efff-416b-85b3-a234e374d2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create raw .txt datasets for each of the past 5 years (2016-2021)\n",
    "for k in bib_data.entries.keys():\n",
    "    try:\n",
    "        year = bib_data.entries[k].fields['year']\n",
    "        abstract = bib_data.entries[k].fields['abstract']\n",
    "        \n",
    "        if year > '2015':\n",
    "            a = open('data/datasets/abstracts.txt', 'a')\n",
    "            a.write(abstract + '\\n')\n",
    "            a.close()\n",
    "    \n",
    "    # entries with non-Unicode characters / entries without abstracts are skipped\n",
    "    except (KeyError, UnicodeEncodeError): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315924a3-292d-4185-948a-5a11fa7e15c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f81be3-5459-41ff-a257-7d11b83f44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of abstracts\n",
    "with open('data/datasets/abstracts.txt') as f:\n",
    "    text = f.read()  \n",
    "    abstracts = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d855b69a-7be3-4aaf-89bd-2171ae4ea4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87769"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of abstracts\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d36897-cf78-4117-8ca6-bbf0e825b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_text):\n",
    "    \n",
    "    # makes text lowercase\n",
    "    input_lower = input_text.lower()\n",
    "    \n",
    "    # only letters (remove numerical and special characters)\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(input_lower)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d262b5-1dba-464f-a94a-6bd9fb369dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 20% of abstracts\n",
    "tokenized = [tokenize(a) for a in abstracts[::5]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f717bae1-93fa-4311-8609-82f5d0e9f1a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'present',\n",
       " 'a',\n",
       " 'scaffolded',\n",
       " 'discovery',\n",
       " 'learning',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'introducing',\n",
       " 'concepts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'course',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'students',\n",
       " 'at',\n",
       " 'liberal',\n",
       " 'arts',\n",
       " 'institutions',\n",
       " 'we',\n",
       " 'describe',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'objectives',\n",
       " 'of',\n",
       " 'this',\n",
       " 'approach',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'presenting',\n",
       " 'specific',\n",
       " 'ways',\n",
       " 'that',\n",
       " 'four',\n",
       " 'of',\n",
       " 'our',\n",
       " 'discovery',\n",
       " 'based',\n",
       " 'assignments',\n",
       " 'combine',\n",
       " 'specific',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'concepts',\n",
       " 'with',\n",
       " 'broader',\n",
       " 'analytic',\n",
       " 'skills',\n",
       " 'we',\n",
       " 'argue',\n",
       " 'this',\n",
       " 'approach',\n",
       " 'helps',\n",
       " 'prepare',\n",
       " 'students',\n",
       " 'for',\n",
       " 'many',\n",
       " 'possible',\n",
       " 'future',\n",
       " 'paths',\n",
       " 'involving',\n",
       " 'both',\n",
       " 'application',\n",
       " 'and',\n",
       " 'innovation',\n",
       " 'of',\n",
       " 'nlp',\n",
       " 'technology',\n",
       " 'by',\n",
       " 'emphasizing',\n",
       " 'experimental',\n",
       " 'data',\n",
       " 'navigation',\n",
       " 'experiment',\n",
       " 'design',\n",
       " 'and',\n",
       " 'awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complexities',\n",
       " 'and',\n",
       " 'challenges',\n",
       " 'of',\n",
       " 'analysis']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tokenized abstract\n",
    "tokenized[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700294ac-cc97-4dbb-bc1e-654f5bfc8e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokenized abstracts\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118b8552-886a-42aa-b67f-458bfa87624e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of single token\n",
    "tokenized[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f529f641-f451-4b7e-bb7b-b6900cc123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single list of tokens\n",
    "tokens = [word for abstract in tokenized for word in abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91183a05-fb8c-4677-8276-f4bce411808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'than'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check single token\n",
    "tokens[2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a75dd80b-83ba-4f07-9bb2-851bfa07881b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362213"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6a2206-e9cc-419a-8464-9cc05099c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file \n",
    "with open('data/tokens.txt', 'w') as f:\n",
    "    f.write(str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3235d190-92e7-4dd6-83b6-cd0fd0643c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenized to file\n",
    "with open('data/tokenized.txt', 'w') as f:\n",
    "    f.write(str(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077ba7b-7f99-4944-8c05-b80fbc6b3b19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbf4379-ce53-49d9-805b-e211a0081d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.56836048763813"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average words per tokenized abstract\n",
    "abs_avg_len = sum(map(len, tokenized))/float(len(tokenized))\n",
    "abs_avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0523fd-73ca-44c1-b21f-6d3bd3efb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length is the average length of tokenized abstract in corpus\n",
    "seq_len = int(abs_avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d447cfa-06a1-4878-8d20-6d456df7aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences from tokens\n",
    "def create_seq(tokens, seq_len):\n",
    "    for i in range(0, len(tokens), seq_len): \n",
    "        yield tokens[i:i + seq_len]\n",
    "        \n",
    "seqs = list(create_seq(tokens, seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1960f7-69c1-480d-a8d4-c36a3ce4e219",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf58e52-9300-4dfc-95f3-2104bea4a4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train and save word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(seqs, vector_size=128, min_count=1, window=10, epochs=100)\n",
    "w2v_model.save('w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb00ef0-6b75-4b3f-8aa2-dbf6a63d8572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33831, 128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size,emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd9a4657-8906-4a92-9072-2fc27db77353",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.5153201 ,   0.04374544,   4.3532104 ,  -3.0908656 ,\n",
       "         7.378356  ,   1.7359018 ,   2.2519982 ,  -3.5178337 ,\n",
       "         1.789913  ,  -1.1127359 ,   3.608325  ,   2.295705  ,\n",
       "         1.4197519 ,  -0.11039281,  -0.36852434,   0.3635096 ,\n",
       "        -3.6040933 ,  -2.6250308 ,  -4.045992  ,   3.8224006 ,\n",
       "        -2.2330127 ,  -0.20725523,   0.49607345,  -2.8950317 ,\n",
       "        -1.9173801 ,   1.1601223 ,   1.636696  ,   3.1894147 ,\n",
       "         3.6535957 ,   2.733748  ,   1.313707  ,  -0.9115405 ,\n",
       "         1.1350554 ,  -2.550564  ,  -0.46035883,   0.513885  ,\n",
       "         2.6159823 ,  -0.7121785 ,   4.6503906 ,   0.3735015 ,\n",
       "        -4.649176  ,  -2.8768854 ,   0.2758945 ,  -4.438189  ,\n",
       "        -0.05888746,   0.33716238,  -1.0184946 ,   1.1004277 ,\n",
       "        -2.7221441 ,   3.8740125 ,   1.5404776 ,  -1.9539399 ,\n",
       "        -4.6756444 ,   1.3007131 ,  -1.3804142 ,   1.468619  ,\n",
       "         0.44544432,  -3.7589514 ,   1.4479334 ,  -3.114318  ,\n",
       "        -1.2487234 ,   0.6174153 ,  -0.44737792,   0.9240885 ,\n",
       "        -3.6739967 ,  -0.21727915,  -2.5341291 ,  -2.263374  ,\n",
       "        -2.756766  ,  -3.0289614 ,   4.6508756 ,  -2.169368  ,\n",
       "         2.9440885 ,   0.71131855,  -3.558656  ,  -4.9004602 ,\n",
       "         2.0831382 ,   1.1407952 ,   0.15819265,   3.1283853 ,\n",
       "         1.8025159 ,   5.4622874 ,   3.6482053 ,   0.4898664 ,\n",
       "        -3.2752097 ,   1.9569235 ,   0.15572962,  -4.707715  ,\n",
       "         0.706046  ,   2.5647283 ,   0.24706863,  -3.3574846 ,\n",
       "         0.03794068,   0.4048897 ,  -0.28873363,  -2.142332  ,\n",
       "        -0.516686  ,   3.8340085 ,  -4.196807  ,   0.85479856,\n",
       "        -0.09200642,  -1.2640425 ,   2.8408957 ,  -2.1728616 ,\n",
       "        -4.890874  ,   2.373852  ,   2.7833169 ,  -1.2115302 ,\n",
       "        -2.199992  ,   3.3833437 ,  -2.2972527 ,   3.6289923 ,\n",
       "         2.5207925 ,   4.5324616 ,   3.5888486 ,   5.3039823 ,\n",
       "        -1.07901   ,  -1.7901323 ,  -4.9575734 ,  -1.5244889 ,\n",
       "         1.2615342 ,   2.6973374 ,   0.841914  ,   1.164677  ,\n",
       "        -0.2864118 , -11.094677  ,   0.5211384 ,  -1.6245023 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vector = w2v_model.wv['word']\n",
    "example_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "892e42ae-9743-49c1-99ed-62398ed7d229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holographic', 0.5947607159614563),\n",
       " ('attr', 0.5925477147102356),\n",
       " ('sentence', 0.5900499820709229),\n",
       " ('token', 0.5833660960197449),\n",
       " ('character', 0.5825883746147156),\n",
       " ('words', 0.560817539691925),\n",
       " ('glove', 0.5558523535728455),\n",
       " ('trouillon', 0.5343304872512817),\n",
       " ('wav', 0.511766791343689),\n",
       " ('imputation', 0.4909842610359192)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_similar = w2v_model.wv.most_similar('word', topn=10) \n",
    "example_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c6d54-f4ae-4c3b-9841-961af26d8edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "568c5bed-a925-40af-addb-cfe1fc3f76e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17629, 17629)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "    x.append(\" \".join(s[:-1]))\n",
    "    y.append(\" \".join(s[1:]))\n",
    "    \n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda295d8-ae50-4d7b-9708-163ec6a20d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 'propose')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]\n",
    "\n",
    "word_to_id('nlp'), id_to_word(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "091e7b1e-245e-4fb7-b561-d35d92c95a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17629, 17629)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_integer_seq(seq):\n",
    "    return [word_to_id(w) for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x = [get_integer_seq(i) for i in x]\n",
    "y = [get_integer_seq(i) for i in y]\n",
    "\n",
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6ea852-2c18-436b-8d70-44adfa8c4683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17628, 17628)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of last sequence, remove if not == seq_len\n",
    "def check_len(seq):\n",
    "    if len(seq[-1]) != seq_len-1:\n",
    "        del seq[-1]\n",
    "    return seq\n",
    "    \n",
    "x = check_len(x)\n",
    "y = check_len(y)\n",
    "\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197d16c6-4c4e-463b-9776-6fae884cebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb43fb17-104d-468b-884a-2f132d115495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save x and y to file\n",
    "np.save('data/x.npy', x)\n",
    "np.save('data/y.npy', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
