{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1441eca-c957-478f-82a2-42681ed2bbd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5284f-ef84-4e81-bd64-0c35762de00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ed4bbb-eeb4-48cd-8746-af30d44e031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from pybtex.database import parse_file\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ad0465-7ed6-4893-b9bf-448f1e77f3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bib_data = parse_file('data/anthology+abstracts.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba9e1ea-6fba-4a08-8930-32a26ef0a604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lieberman-etal-1965-automatic'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the last entry\n",
    "list(bib_data.entries.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a333e9-2641-4878-a3aa-78d28de781cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries in the anthology\n",
    "len(list(bib_data.entries.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ada274-efff-416b-85b3-a234e374d2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create raw .txt datasets for each of the past 5 years (2016-2021)\n",
    "for k in bib_data.entries.keys():\n",
    "    try:\n",
    "        year = bib_data.entries[k].fields['year']\n",
    "        abstract = bib_data.entries[k].fields['abstract']\n",
    "        \n",
    "        if year > '2015':\n",
    "            a = open('data/datasets/abstracts.txt', 'a')\n",
    "            a.write(abstract + '\\n')\n",
    "            a.close()\n",
    "    \n",
    "    # corrupted entries / entries without abstracts are skipped\n",
    "    except (KeyError, UnicodeEncodeError): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315924a3-292d-4185-948a-5a11fa7e15c4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f81be3-5459-41ff-a257-7d11b83f44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of abstracts\n",
    "with open('data/datasets/abstracts.txt') as f:\n",
    "    text = f.read()  \n",
    "    abstracts = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d855b69a-7be3-4aaf-89bd-2171ae4ea4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21943"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of abstract entry\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26d36897-cf78-4117-8ca6-bbf0e825b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_text):\n",
    "    \n",
    "    # makes text lowercase\n",
    "    input_lower = input_text.lower()\n",
    "    \n",
    "    # only letters (remove numerical and special characters)\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(input_lower)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4d262b5-1dba-464f-a94a-6bd9fb369dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenize(a) for a in abstracts[1::20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f717bae1-93fa-4311-8609-82f5d0e9f1a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'new',\n",
       " 'dataset',\n",
       " 'for',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'in',\n",
       " 'conversational',\n",
       " 'context',\n",
       " 'qrecc',\n",
       " 'which',\n",
       " 'contains',\n",
       " 'k',\n",
       " 'conversations',\n",
       " 'with',\n",
       " 'k',\n",
       " 'question',\n",
       " 'answer',\n",
       " 'pairs',\n",
       " 'the',\n",
       " 'task',\n",
       " 'in',\n",
       " 'qrecc',\n",
       " 'is',\n",
       " 'to',\n",
       " 'find',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'conversational',\n",
       " 'questions',\n",
       " 'within',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'm',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'split',\n",
       " 'into',\n",
       " 'm',\n",
       " 'passages',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'questions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'conversation',\n",
       " 'may',\n",
       " 'be',\n",
       " 'distributed',\n",
       " 'across',\n",
       " 'several',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'qrecc',\n",
       " 'provides',\n",
       " 'annotations',\n",
       " 'that',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'train',\n",
       " 'and',\n",
       " 'evaluate',\n",
       " 'individual',\n",
       " 'subtasks',\n",
       " 'of',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'passage',\n",
       " 'retrieval',\n",
       " 'and',\n",
       " 'reading',\n",
       " 'comprehension',\n",
       " 'required',\n",
       " 'for',\n",
       " 'the',\n",
       " 'end',\n",
       " 'to',\n",
       " 'end',\n",
       " 'conversational',\n",
       " 'question',\n",
       " 'answering',\n",
       " 'qa',\n",
       " 'task',\n",
       " 'we',\n",
       " 'report',\n",
       " 'the',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'baseline',\n",
       " 'approach',\n",
       " 'that',\n",
       " 'combines',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'model',\n",
       " 'for',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'and',\n",
       " 'competitive',\n",
       " 'models',\n",
       " 'for',\n",
       " 'open',\n",
       " 'domain',\n",
       " 'qa',\n",
       " 'our',\n",
       " 'results',\n",
       " 'set',\n",
       " 'the',\n",
       " 'first',\n",
       " 'baseline',\n",
       " 'for',\n",
       " 'the',\n",
       " 'qrecc',\n",
       " 'dataset',\n",
       " 'with',\n",
       " 'f',\n",
       " 'of',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'human',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'of',\n",
       " 'indicating',\n",
       " 'the',\n",
       " 'difficulty',\n",
       " 'of',\n",
       " 'the',\n",
       " 'setup',\n",
       " 'and',\n",
       " 'a',\n",
       " 'large',\n",
       " 'room',\n",
       " 'for',\n",
       " 'improvement']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tokenized abstract\n",
    "tokenized[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700294ac-cc97-4dbb-bc1e-654f5bfc8e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokenized abstracts\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "118b8552-886a-42aa-b67f-458bfa87624e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'describes'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single token\n",
    "tokenized[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f529f641-f451-4b7e-bb7b-b6900cc123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single list of tokens\n",
    "tokens = [word for abstract in tokenized for word in abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91183a05-fb8c-4677-8276-f4bce411808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check single token\n",
    "tokens[2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a75dd80b-83ba-4f07-9bb2-851bfa07881b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148567"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c6a2206-e9cc-419a-8464-9cc05099c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file \n",
    "with open('data/tokens.txt', 'w') as f:\n",
    "    f.write(str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3235d190-92e7-4dd6-83b6-cd0fd0643c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('data/tokenized.txt', 'w') as f:\n",
    "    f.write(str(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7b321-c671-40d2-9c3d-4cefabf733ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b0523fd-73ca-44c1-b21f-6d3bd3efb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 100\n",
    "\n",
    "sent_split = zip(*[iter(tokens)] * max_sentence_len)\n",
    "sentences = [list(s) for s in sent_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cf58e52-9300-4dfc-95f3-2104bea4a4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjust word embedding size according to paper findings\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(sentences, vector_size=100, min_count=1, window=5, epochs=1)\n",
    "\n",
    "# save model\n",
    "w2v_model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9eb00ef0-6b75-4b3f-8aa2-dbf6a63d8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9567 100\n"
     ]
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "print(vocab_size,emdedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd9a4657-8906-4a92-9072-2fc27db77353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05340158  0.02590642  0.01503839  0.02531578  0.00643208 -0.06506889\n",
      "  0.03091099  0.06639995 -0.0208962  -0.02296234 -0.02905319 -0.0500361\n",
      "  0.01018123  0.02076851  0.01678655 -0.04894024  0.02409277 -0.04112661\n",
      " -0.01375142 -0.0844128   0.02889113  0.01567878  0.0188634  -0.02133239\n",
      "  0.01525009  0.00229912 -0.02622324 -0.02251646 -0.04431868  0.00872653\n",
      "  0.0675933  -0.0094258   0.02700188 -0.0586458  -0.037376    0.04670507\n",
      "  0.00821736 -0.06266262 -0.0414429  -0.06861643 -0.01216607 -0.03952619\n",
      " -0.0149381   0.02402886  0.01373445  0.00380556 -0.0202361   0.00073795\n",
      "  0.00867628  0.00627567  0.0277653  -0.05044303 -0.01215924 -0.02200374\n",
      " -0.03629857  0.0133618   0.02861061 -0.02262054 -0.04472381  0.00937136\n",
      "  0.0161874   0.00357403 -0.00137439  0.02344876 -0.05799637  0.02631601\n",
      "  0.03597176  0.05481634 -0.08044028  0.03455045  0.00136511  0.05876053\n",
      "  0.05654617  0.01024706  0.0549088   0.02575218 -0.01714257  0.01101278\n",
      " -0.03688185  0.01040825 -0.02903974 -0.01382768 -0.03402722  0.049214\n",
      " -0.01230526 -0.01735845  0.0278324   0.0520982   0.03166028  0.00638829\n",
      "  0.07376548  0.02713485 -0.00926879  0.02340498  0.09148     0.05162678\n",
      "  0.02435894 -0.04396647  0.0183727  -0.01620358]\n"
     ]
    }
   ],
   "source": [
    "example_vector = w2v_model.wv['computer']\n",
    "print(example_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "892e42ae-9743-49c1-99ed-62398ed7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 0.9998010396957397), ('of', 0.9997996091842651), ('and', 0.9997888207435608), ('an', 0.9997869729995728), ('the', 0.9997761845588684), ('a', 0.9997751116752625), ('from', 0.9997710585594177), ('with', 0.9997611045837402), ('to', 0.999756395816803), ('using', 0.9997522830963135)]\n"
     ]
    }
   ],
   "source": [
    "example_similar = w2v_model.wv.most_similar('language', topn=10) \n",
    "print(example_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c6d54-f4ae-4c3b-9841-961af26d8edb",
   "metadata": {},
   "source": [
    "## Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "102e1829-5f93-44ba-9a41-6770308f5b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1485, 100)\n",
      "y shape: (1485,)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        x[i, t] = word_to_id(word)\n",
    "    y[i] = word_to_id(sentence[-1])\n",
    "\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb43fb17-104d-468b-884a-2f132d115495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "np.save('data/x.npy', x)\n",
    "np.save('data/y.npy', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
