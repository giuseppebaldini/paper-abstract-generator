{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1441eca-c957-478f-82a2-42681ed2bbd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5284f-ef84-4e81-bd64-0c35762de00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ed4bbb-eeb4-48cd-8746-af30d44e031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from pybtex.database import parse_file\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ad0465-7ed6-4893-b9bf-448f1e77f3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bib_data = parse_file('data/anthology+abstracts.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba9e1ea-6fba-4a08-8930-32a26ef0a604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lieberman-etal-1965-automatic'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the last entry\n",
    "list(bib_data.entries.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a333e9-2641-4878-a3aa-78d28de781cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of entries in the anthology\n",
    "len(list(bib_data.entries.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ada274-efff-416b-85b3-a234e374d2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create raw .txt datasets for each of the past 5 years (2016-2021)\n",
    "for k in bib_data.entries.keys():\n",
    "    try:\n",
    "        year = bib_data.entries[k].fields['year']\n",
    "        abstract = bib_data.entries[k].fields['abstract']\n",
    "        \n",
    "        if year > '2015':\n",
    "            a = open('data/datasets/abstracts.txt', 'a')\n",
    "            a.write(abstract + '\\n')\n",
    "            a.close()\n",
    "    \n",
    "    # corrupted entries / entries without abstracts are skipped\n",
    "    except (KeyError, UnicodeEncodeError): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315924a3-292d-4185-948a-5a11fa7e15c4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39f81be3-5459-41ff-a257-7d11b83f44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of abstracts\n",
    "with open('data/datasets/abstracts.txt') as f:\n",
    "    text = f.read()  \n",
    "    abstracts = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d855b69a-7be3-4aaf-89bd-2171ae4ea4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21943"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of abstract entry\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26d36897-cf78-4117-8ca6-bbf0e825b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_text):\n",
    "    \n",
    "    # makes text lowercase\n",
    "    input_lower = input_text.lower()\n",
    "    \n",
    "    # only letters (remove numerical and special characters)\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = tokenizer.tokenize(input_lower)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4d262b5-1dba-464f-a94a-6bd9fb369dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenize(a) for a in abstracts[1::20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f717bae1-93fa-4311-8609-82f5d0e9f1a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'new',\n",
       " 'dataset',\n",
       " 'for',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'in',\n",
       " 'conversational',\n",
       " 'context',\n",
       " 'qrecc',\n",
       " 'which',\n",
       " 'contains',\n",
       " 'k',\n",
       " 'conversations',\n",
       " 'with',\n",
       " 'k',\n",
       " 'question',\n",
       " 'answer',\n",
       " 'pairs',\n",
       " 'the',\n",
       " 'task',\n",
       " 'in',\n",
       " 'qrecc',\n",
       " 'is',\n",
       " 'to',\n",
       " 'find',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'conversational',\n",
       " 'questions',\n",
       " 'within',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'm',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'split',\n",
       " 'into',\n",
       " 'm',\n",
       " 'passages',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'questions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'conversation',\n",
       " 'may',\n",
       " 'be',\n",
       " 'distributed',\n",
       " 'across',\n",
       " 'several',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'qrecc',\n",
       " 'provides',\n",
       " 'annotations',\n",
       " 'that',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'train',\n",
       " 'and',\n",
       " 'evaluate',\n",
       " 'individual',\n",
       " 'subtasks',\n",
       " 'of',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'passage',\n",
       " 'retrieval',\n",
       " 'and',\n",
       " 'reading',\n",
       " 'comprehension',\n",
       " 'required',\n",
       " 'for',\n",
       " 'the',\n",
       " 'end',\n",
       " 'to',\n",
       " 'end',\n",
       " 'conversational',\n",
       " 'question',\n",
       " 'answering',\n",
       " 'qa',\n",
       " 'task',\n",
       " 'we',\n",
       " 'report',\n",
       " 'the',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'baseline',\n",
       " 'approach',\n",
       " 'that',\n",
       " 'combines',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'model',\n",
       " 'for',\n",
       " 'question',\n",
       " 'rewriting',\n",
       " 'and',\n",
       " 'competitive',\n",
       " 'models',\n",
       " 'for',\n",
       " 'open',\n",
       " 'domain',\n",
       " 'qa',\n",
       " 'our',\n",
       " 'results',\n",
       " 'set',\n",
       " 'the',\n",
       " 'first',\n",
       " 'baseline',\n",
       " 'for',\n",
       " 'the',\n",
       " 'qrecc',\n",
       " 'dataset',\n",
       " 'with',\n",
       " 'f',\n",
       " 'of',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'human',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'of',\n",
       " 'indicating',\n",
       " 'the',\n",
       " 'difficulty',\n",
       " 'of',\n",
       " 'the',\n",
       " 'setup',\n",
       " 'and',\n",
       " 'a',\n",
       " 'large',\n",
       " 'room',\n",
       " 'for',\n",
       " 'improvement']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tokenized abstract\n",
    "tokenized[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "700294ac-cc97-4dbb-bc1e-654f5bfc8e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokenized abstracts\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "118b8552-886a-42aa-b67f-458bfa87624e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'describes'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single token\n",
    "tokenized[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f529f641-f451-4b7e-bb7b-b6900cc123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single list of tokens\n",
    "tokens = [word for abstract in tokenized for word in abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91183a05-fb8c-4677-8276-f4bce411808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check single token\n",
    "tokens[2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a75dd80b-83ba-4f07-9bb2-851bfa07881b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148567"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c6a2206-e9cc-419a-8464-9cc05099c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file \n",
    "with open('data/tokens.txt', 'w') as f:\n",
    "    f.write(str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3235d190-92e7-4dd6-83b6-cd0fd0643c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('data/tokenized.txt', 'w') as f:\n",
    "    f.write(str(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c6d54-f4ae-4c3b-9841-961af26d8edb",
   "metadata": {},
   "source": [
    "## Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8b0523fd-73ca-44c1-b21f-6d3bd3efb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust seq_len to avg of abstract length\n",
    "seq_len = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6d447cfa-06a1-4878-8d20-6d456df7aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(tokens, seq_len):\n",
    "    for i in range(0, len(tokens), seq_len): \n",
    "        yield tokens[i:i + seq_len]\n",
    "        \n",
    "seqs = list(create_seq(tokens, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "568c5bed-a925-40af-addb-cfe1fc3f76e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 1471)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "    x.append(\" \".join(s[:-1]))\n",
    "    y.append(\" \".join(s[1:]))\n",
    "    \n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "eda295d8-ae50-4d7b-9708-163ec6a20d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 'neural')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]\n",
    "\n",
    "word_to_id('nlp'), id_to_word(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "091e7b1e-245e-4fb7-b561-d35d92c95a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 1471)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_integer_seq(seq):\n",
    "    return [word_to_id(w) for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x = [get_integer_seq(i) for i in x]\n",
    "y = [get_integer_seq(i) for i in y]\n",
    "\n",
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "df6ea852-2c18-436b-8d70-44adfa8c4683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470, 1470)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of last sequence, remove if not == seq_len\n",
    "def check_len(seq):\n",
    "    if len(seq[-1]) != seq_len-1:\n",
    "        del seq[-1]\n",
    "    return seq\n",
    "    \n",
    "x = check_len(x_all)\n",
    "y = check_len(y_all)\n",
    "\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "197d16c6-4c4e-463b-9776-6fae884cebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "07e7931d-f9db-42cc-b142-303f7785e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470, 100)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fb43fb17-104d-468b-884a-2f132d115495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "np.save('data/x.npy', x)\n",
    "np.save('data/y.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7b321-c671-40d2-9c3d-4cefabf733ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6cf58e52-9300-4dfc-95f3-2104bea4a4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: adjust word embedding size according to paper findings\n",
    "w2v_model = gensim.models.Word2Vec(seqs, vector_size=100, min_count=1, window=5, epochs=1)\n",
    "\n",
    "# save model\n",
    "w2v_model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9eb00ef0-6b75-4b3f-8aa2-dbf6a63d8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9569 100\n"
     ]
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "print(vocab_size,emdedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cd9a4657-8906-4a92-9072-2fc27db77353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02636507  0.02693588  0.01410158  0.00957402  0.00407673 -0.08141849\n",
      "  0.02094897  0.05846748 -0.03385695 -0.00771476 -0.02724805 -0.04349714\n",
      "  0.00375009  0.02974946  0.00262331 -0.04681301  0.0263958  -0.03452348\n",
      " -0.02680098 -0.06978898  0.02214226  0.01627002  0.02066958 -0.02097843\n",
      " -0.00957535  0.00463436 -0.01632814 -0.01811325 -0.03827393  0.00270969\n",
      "  0.04532893 -0.00361596  0.02638591 -0.04051479 -0.01313958  0.02572526\n",
      "  0.00330814 -0.03875148 -0.01706738 -0.06009709  0.00505941 -0.02165239\n",
      " -0.01808534  0.01643691  0.01767226 -0.00580908 -0.0133606   0.00676859\n",
      "  0.00964711  0.00359484  0.02938396 -0.04573927 -0.00615245 -0.01930001\n",
      " -0.02863558  0.00734415  0.01987244 -0.00836672 -0.05088564  0.01861229\n",
      " -0.00103728 -0.00074192  0.00289908  0.03447132 -0.06189742  0.04086243\n",
      "  0.00753767  0.04772544 -0.05605566  0.02583542  0.00143682  0.04337582\n",
      "  0.04133814 -0.01416804  0.04785234  0.01296554 -0.01561027  0.0119024\n",
      " -0.03500389 -0.00702313 -0.02259577 -0.00241676 -0.05181037  0.04787846\n",
      " -0.00513266 -0.00581565  0.02493202  0.04481506  0.0125085   0.00872439\n",
      "  0.05582775  0.01231227  0.00813391  0.00530083  0.07186023  0.03682295\n",
      "  0.01411687 -0.03260786  0.00473181 -0.00417848]\n"
     ]
    }
   ],
   "source": [
    "example_vector = w2v_model.wv['computer']\n",
    "print(example_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "892e42ae-9743-49c1-99ed-62398ed7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 0.9997780323028564), ('an', 0.999767005443573), ('of', 0.9997662901878357), ('the', 0.9997624158859253), ('and', 0.999758780002594), ('a', 0.999756932258606), ('from', 0.9997407793998718), ('with', 0.9997388124465942), ('to', 0.9997334480285645), ('in', 0.9997265338897705)]\n"
     ]
    }
   ],
   "source": [
    "example_similar = w2v_model.wv.most_similar('language', topn=10) \n",
    "print(example_similar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
