{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pybtex.database import parse_file\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_data = parse_file('data/test_dataset.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wassa-2021-approaches',\n",
       " 'xiang-etal-2021-toxccin',\n",
       " 'kerz-etal-2021-language',\n",
       " 'lindow-etal-2021-partisanship',\n",
       " 'akula-garibay-2021-explainable',\n",
       " 'troiano-etal-2021-emotion',\n",
       " 'dayanik-pado-2021-disentangling',\n",
       " 'lamprinidis-etal-2021-universal',\n",
       " 'bianchi-etal-2021-feel']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bib_data.entries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiang-etal-2021-toxccin\n",
      "kerz-etal-2021-language\n",
      "lindow-etal-2021-partisanship\n",
      "akula-garibay-2021-explainable\n",
      "troiano-etal-2021-emotion\n",
      "dayanik-pado-2021-disentangling\n",
      "lamprinidis-etal-2021-universal\n",
      "bianchi-etal-2021-feel\n"
     ]
    }
   ],
   "source": [
    "for k in bib_data.entries.keys():\n",
    "    try:\n",
    "        f = open('data.txt', 'a')\n",
    "        f.write(bib_data.entries[k].fields['abstract'])\n",
    "        f.close()\n",
    "        print(k)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #1: char-level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate stop words\n",
    "def tokenize_input(input):\n",
    "    # lowercase\n",
    "    input = input.lower()\n",
    "\n",
    "    # use tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # end result in final\n",
    "    final = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = tokenize_input(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 5647\n",
      "Total vocab: 36\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through inputs\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    \n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 5547\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "filepath = \"lstm_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5547 samples\n",
      "Epoch 1/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 3.0834\n",
      "Epoch 00001: loss improved from inf to 3.07730, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 113s 20ms/sample - loss: 3.0773\n",
      "Epoch 2/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9703\n",
      "Epoch 00002: loss improved from 3.07730 to 2.96878, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 123s 22ms/sample - loss: 2.9688\n",
      "Epoch 3/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9581\n",
      "Epoch 00003: loss improved from 2.96878 to 2.95555, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 123s 22ms/sample - loss: 2.9556\n",
      "Epoch 4/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9471\n",
      "Epoch 00004: loss improved from 2.95555 to 2.94841, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 119s 21ms/sample - loss: 2.9484\n",
      "Epoch 5/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9401\n",
      "Epoch 00005: loss improved from 2.94841 to 2.93708, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 111s 20ms/sample - loss: 2.9371\n",
      "Epoch 6/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9375\n",
      "Epoch 00006: loss improved from 2.93708 to 2.93647, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 107s 19ms/sample - loss: 2.9365\n",
      "Epoch 7/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9295\n",
      "Epoch 00007: loss improved from 2.93647 to 2.92930, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 108s 20ms/sample - loss: 2.9293\n",
      "Epoch 8/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9275\n",
      "Epoch 00008: loss improved from 2.92930 to 2.92780, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 108s 19ms/sample - loss: 2.9278\n",
      "Epoch 9/10\n",
      "5376/5547 [============================>.] - ETA: 3s - loss: 2.9277\n",
      "Epoch 00009: loss did not improve from 2.92780\n",
      "5547/5547 [==============================] - 109s 20ms/sample - loss: 2.9280\n",
      "Epoch 10/10\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9232\n",
      "Epoch 00010: loss improved from 2.92780 to 2.92664, saving model to model_weights_saved.hdf5\n",
      "5547/5547 [==============================] - 88s 16ms/sample - loss: 2.9266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16f33c57fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=10, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lstm_weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Abstract: \n",
      "\n",
      "\" easures liwc style measures show recurrent neural network classifier trained exclusively within text \"\n"
     ]
    }
   ],
   "source": [
    "# random seed initialization\n",
    "start = np.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Generated Abstract: \\n\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #2: char-level GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the unique characters present in the text\n",
    "vocabulary = sorted(list(set(text)))\n",
    "\n",
    "# Creating dictionaries to map each character to an index\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "steps = 5\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - max_length, steps):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "      \n",
    "# Hot encoding each character into a boolean vector\n",
    "  \n",
    "# Initializing a matrix of boolean vectors with each column representing\n",
    "# the hot encoded representation of the character\n",
    "X = np.zeros((len(sentences), max_length, len(vocabulary)), dtype = np.bool)\n",
    "y = np.zeros((len(sentences), len(vocabulary)), dtype = np.bool)\n",
    "  \n",
    "# Placing the value 1 at the appropriate position for each vector\n",
    "# to complete the hot-encoding process\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_indices[char]] = 1\n",
    "    y[i, char_to_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "  \n",
    "# Defining the cell type\n",
    "model.add(GRU(128, input_shape =(max_length, len(vocabulary))))\n",
    "  \n",
    "# Defining the densely connected Neural Network layer\n",
    "model.add(Dense(len(vocabulary)))\n",
    "  \n",
    "# Defining the activation function for the cell\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Defining the optimizing function\n",
    "optimizer = RMSprop(lr = 0.01)\n",
    "  \n",
    "# Configuring the model for training\n",
    "model.compile(loss ='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to sample an index from a probability array\n",
    "def sample_index(preds, temperature = 1.0):\n",
    "# temperature determines the freedom the function has when generating text\n",
    "  \n",
    "    # Converting the predictions vector into a numpy array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "  \n",
    "    # Normalizing the predicitons array\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "  \n",
    "    # The main sampling step. Creates an array of probablities signifying\n",
    "    # the probability of each character to be the next character in the \n",
    "    # generated text\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "  \n",
    "    # Returning the character with maximum probability to be the next character\n",
    "    # in the generated text\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch\n",
    "# in which the loss decreases\n",
    "filepath = \"gru_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor ='loss', \n",
    "                             save_best_only = True, \n",
    "                             mode ='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to reduce the learning rate each time the learning plateaus\n",
    "reduce_alpha = ReduceLROnPlateau(monitor ='loss', factor = 0.2,\n",
    "\t\t\t\t\t\t\tpatience = 1, min_lr = 0.001)\n",
    "callbacks = [checkpoint, reduce_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1454 samples\n",
      "Epoch 1/50\n",
      "1454/1454 [==============================] - 4s 3ms/sample - loss: 3.7912\n",
      "Epoch 2/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.1156\n",
      "Epoch 3/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0862\n",
      "Epoch 4/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0686\n",
      "Epoch 5/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0500\n",
      "Epoch 6/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0284\n",
      "Epoch 7/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9982\n",
      "Epoch 8/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9807\n",
      "Epoch 9/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9415\n",
      "Epoch 10/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9108\n",
      "Epoch 11/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.8776\n",
      "Epoch 12/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.8598\n",
      "Epoch 13/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.8114\n",
      "Epoch 14/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7755\n",
      "Epoch 15/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7403\n",
      "Epoch 16/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7057\n",
      "Epoch 17/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6675\n",
      "Epoch 18/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6405\n",
      "Epoch 19/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6057\n",
      "Epoch 20/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5720\n",
      "Epoch 21/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5515\n",
      "Epoch 22/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5221\n",
      "Epoch 23/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4908\n",
      "Epoch 24/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4639\n",
      "Epoch 25/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4362\n",
      "Epoch 26/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4086\n",
      "Epoch 27/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3869\n",
      "Epoch 28/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3597\n",
      "Epoch 29/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3427\n",
      "Epoch 30/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3113\n",
      "Epoch 31/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2945\n",
      "Epoch 32/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2676\n",
      "Epoch 33/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2427\n",
      "Epoch 34/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2231\n",
      "Epoch 35/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1958\n",
      "Epoch 36/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1754\n",
      "Epoch 37/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1599\n",
      "Epoch 38/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1302\n",
      "Epoch 39/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1086\n",
      "Epoch 40/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0873\n",
      "Epoch 41/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0660\n",
      "Epoch 42/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0409\n",
      "Epoch 43/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0254\n",
      "Epoch 44/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9988\n",
      "Epoch 45/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9777\n",
      "Epoch 46/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9540\n",
      "Epoch 47/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9345\n",
      "Epoch 48/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9136\n",
      "Epoch 49/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.8886\n",
      "Epoch 50/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.8680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2509a43f898>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the GRU model\n",
    "model.fit(X, y, batch_size = 128, epochs = 50, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ble for TED talks in a multi-label classification task and (2) to determine what types of features derficas an tal angess on the pres an the ereress on the pres ans angess on the pres ans andeal ange ses anne the pres ange seresich of the pres anne pres anntantent tont the ereress on the pres ans ange seres an that ans ange pred ans angesich ored anntict on the pres ange pres anne pred at onthe feress to turessal se furess the preas on the pres ans ange serfire ine pred the ereress on theress on the pres an enthe ferress on the pres angeress on the pres ans andeltat on the pres ange ing ares a\n"
     ]
    }
   ],
   "source": [
    "def generate_text(length, diversity):\n",
    "\t# Get random starting text\n",
    "\tstart_index = random.randint(0, len(text) - max_length - 1)\n",
    "\n",
    "\t# Defining the generated text\n",
    "\tgenerated = ''\n",
    "\tsentence = text[start_index: start_index + max_length]\n",
    "\tgenerated += sentence\n",
    "\n",
    "\t# Generating new text of given length\n",
    "\tfor i in range(length):\n",
    "\n",
    "\t\t\t# Initializing the predicition vector\n",
    "\t\t\tx_pred = np.zeros((1, max_length, len(vocabulary)))\n",
    "\t\t\tfor t, char in enumerate(sentence):\n",
    "\t\t\t\tx_pred[0, t, char_to_indices[char]] = 1.\n",
    "\n",
    "\t\t\t# Making the predicitons\n",
    "\t\t\tpreds = model.predict(x_pred, verbose = 0)[0]\n",
    "\n",
    "\t\t\t# Getting the index of the next most probable index\n",
    "\t\t\tnext_index = sample_index(preds, diversity)\n",
    "\n",
    "\t\t\t# Getting the most probable next character using the mapping built\n",
    "\t\t\tnext_char = indices_to_char[next_index]\n",
    "\n",
    "\t\t\t# Generating new text\n",
    "\t\t\tgenerated += next_char\n",
    "\t\t\tsentence = sentence[1:] + next_char\n",
    "\treturn generated\n",
    "\n",
    "print(generate_text(500, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #3: Standard GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode('In this paper we present', \n",
    "                          max_length=1024, \n",
    "                          truncation=True,\n",
    "                          return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In this paper we present a system of neural networks to analyze the role of specific brain regions in various tasks, including perceptual recognition and task performance. We conclude that the use of neural networks may have unexpected evolutionary potential for early human ancestors.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs, \n",
    "                         max_length=200, \n",
    "                         do_sample=True)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
