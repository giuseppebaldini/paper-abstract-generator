{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import GRU, LSTM, Activation, Dense, Dropout, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/x.npy')\n",
    "y = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline #1: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = Sequential()\n",
    "\n",
    "gru.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "gru.add(GRU(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru.add(Dropout(0.2))\n",
    "gru.add(GRU(256, return_sequences=True))\n",
    "gru.add(Dropout(0.2))\n",
    "gru.add(GRU(128))\n",
    "gru.add(Dropout(0.2))\n",
    "gru.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/gru.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1485 samples\n",
      "Epoch 1/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 9.0987\n",
      "Epoch 00001: loss improved from inf to 9.08099, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 54s 36ms/sample - loss: 9.0810\n",
      "Epoch 2/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 7.7193\n",
      "Epoch 00002: loss improved from 9.08099 to 7.67719, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 45s 30ms/sample - loss: 7.6772\n",
      "Epoch 3/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.4682\n",
      "Epoch 00003: loss improved from 7.67719 to 6.44648, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 46s 31ms/sample - loss: 6.4465\n",
      "Epoch 4/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.1372\n",
      "Epoch 00004: loss improved from 6.44648 to 6.12681, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 46s 31ms/sample - loss: 6.1268\n",
      "Epoch 5/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0315\n",
      "Epoch 00005: loss improved from 6.12681 to 6.04947, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 46s 31ms/sample - loss: 6.0495\n",
      "Epoch 6/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0084\n",
      "Epoch 00006: loss improved from 6.04947 to 6.01415, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 47s 32ms/sample - loss: 6.0142\n",
      "Epoch 7/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9871\n",
      "Epoch 00007: loss improved from 6.01415 to 5.99797, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 47s 32ms/sample - loss: 5.9980\n",
      "Epoch 8/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9853\n",
      "Epoch 00008: loss improved from 5.99797 to 5.98792, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 5.9879\n",
      "Epoch 9/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9813\n",
      "Epoch 00009: loss improved from 5.98792 to 5.98631, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 5.9863\n",
      "Epoch 10/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9750\n",
      "Epoch 00010: loss improved from 5.98631 to 5.98503, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 5.9850\n",
      "Epoch 11/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9723\n",
      "Epoch 00011: loss improved from 5.98503 to 5.97461, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 5.9746\n",
      "Epoch 12/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9673\n",
      "Epoch 00012: loss did not improve from 5.97461\n",
      "1485/1485 [==============================] - 47s 31ms/sample - loss: 5.9752\n",
      "Epoch 13/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9808\n",
      "Epoch 00013: loss did not improve from 5.97461\n",
      "1485/1485 [==============================] - 48s 33ms/sample - loss: 5.9786\n",
      "Epoch 14/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9766\n",
      "Epoch 00014: loss did not improve from 5.97461\n",
      "1485/1485 [==============================] - 48s 33ms/sample - loss: 5.9762\n",
      "Epoch 15/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9700\n",
      "Epoch 00015: loss did not improve from 5.97461\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9760\n",
      "Epoch 16/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9607\n",
      "Epoch 00016: loss improved from 5.97461 to 5.97374, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9737\n",
      "Epoch 17/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9747\n",
      "Epoch 00017: loss did not improve from 5.97374\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9743\n",
      "Epoch 18/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9658\n",
      "Epoch 00018: loss improved from 5.97374 to 5.96499, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9650\n",
      "Epoch 19/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9709\n",
      "Epoch 00019: loss did not improve from 5.96499\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9714\n",
      "Epoch 20/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9539\n",
      "Epoch 00020: loss improved from 5.96499 to 5.96326, saving model to weights/gru.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217d95e0d68>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.fit(x, y, batch_size=128, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline #2: GRU + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gru_w2v = Sequential()\n",
    "\n",
    "gru_w2v.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[w2v_model.wv.vectors]))\n",
    "gru_w2v.add(GRU(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru_w2v.add(Dropout(0.2))\n",
    "gru_w2v.add(GRU(256, return_sequences=True))\n",
    "gru_w2v.add(Dropout(0.2))\n",
    "gru_w2v.add(GRU(128))\n",
    "gru_w2v.add(Dropout(0.2))\n",
    "gru_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = \"weights/gru_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1485 samples\n",
      "Epoch 1/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.3025\n",
      "Epoch 00001: loss improved from 7.27410 to 6.30135, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 6.3014\n",
      "Epoch 2/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0884\n",
      "Epoch 00002: loss improved from 6.30135 to 6.09080, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 6.0908\n",
      "Epoch 3/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0185\n",
      "Epoch 00003: loss improved from 6.09080 to 6.03003, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 6.0300\n",
      "Epoch 4/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0073\n",
      "Epoch 00004: loss improved from 6.03003 to 6.00779, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 6.0078\n",
      "Epoch 5/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9851\n",
      "Epoch 00005: loss improved from 6.00779 to 5.99053, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9905\n",
      "Epoch 6/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9748\n",
      "Epoch 00006: loss did not improve from 5.99053\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9906\n",
      "Epoch 7/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9874\n",
      "Epoch 00007: loss improved from 5.99053 to 5.98847, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9885\n",
      "Epoch 8/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9769\n",
      "Epoch 00008: loss improved from 5.98847 to 5.98231, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 48s 33ms/sample - loss: 5.9823\n",
      "Epoch 9/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9775\n",
      "Epoch 00009: loss improved from 5.98231 to 5.97804, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9780\n",
      "Epoch 10/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9753\n",
      "Epoch 00010: loss improved from 5.97804 to 5.97348, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9735\n",
      "Epoch 11/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9702\n",
      "Epoch 00011: loss did not improve from 5.97348\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9751\n",
      "Epoch 12/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9687\n",
      "Epoch 00012: loss did not improve from 5.97348\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9763\n",
      "Epoch 13/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9727\n",
      "Epoch 00013: loss did not improve from 5.97348\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9740\n",
      "Epoch 14/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9619\n",
      "Epoch 00014: loss improved from 5.97348 to 5.96428, saving model to weights/gru_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9643\n",
      "Epoch 15/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9757\n",
      "Epoch 00015: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9729\n",
      "Epoch 16/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9512\n",
      "Epoch 00016: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9696\n",
      "Epoch 17/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9793\n",
      "Epoch 00017: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9706\n",
      "Epoch 18/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9570\n",
      "Epoch 00018: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9698\n",
      "Epoch 19/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9560\n",
      "Epoch 00019: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9670\n",
      "Epoch 20/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9704\n",
      "Epoch 00020: loss did not improve from 5.96428\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21780016da0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_w2v.fit(x, y, batch_size=128, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline #3: LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "lstm.add(GRU(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(GRU(256, return_sequences=True))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(GRU(128))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1485 samples\n",
      "Epoch 1/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 7.6394\n",
      "Epoch 00001: loss improved from inf to 7.61531, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 7.6153\n",
      "Epoch 2/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.4372\n",
      "Epoch 00002: loss improved from 7.61531 to 6.42914, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 6.4291\n",
      "Epoch 3/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.1358\n",
      "Epoch 00003: loss improved from 6.42914 to 6.12401, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 6.1240\n",
      "Epoch 4/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0411\n",
      "Epoch 00004: loss improved from 6.12401 to 6.04974, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 6.0497\n",
      "Epoch 5/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0189\n",
      "Epoch 00005: loss improved from 6.04974 to 6.01366, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 48s 32ms/sample - loss: 6.0137\n",
      "Epoch 6/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0145\n",
      "Epoch 00006: loss improved from 6.01366 to 6.00278, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 6.0028\n",
      "Epoch 7/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9912\n",
      "Epoch 00007: loss improved from 6.00278 to 5.99018, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9902\n",
      "Epoch 8/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0017\n",
      "Epoch 00008: loss improved from 5.99018 to 5.98621, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9862\n",
      "Epoch 9/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9822\n",
      "Epoch 00009: loss improved from 5.98621 to 5.98172, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9817\n",
      "Epoch 10/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9844\n",
      "Epoch 00010: loss did not improve from 5.98172\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9834\n",
      "Epoch 11/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9790\n",
      "Epoch 00011: loss improved from 5.98172 to 5.97360, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9736\n",
      "Epoch 12/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9757\n",
      "Epoch 00012: loss did not improve from 5.97360\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9755\n",
      "Epoch 13/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9708\n",
      "Epoch 00013: loss improved from 5.97360 to 5.97074, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9707\n",
      "Epoch 14/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9721\n",
      "Epoch 00014: loss improved from 5.97074 to 5.96756, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9676\n",
      "Epoch 15/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9725\n",
      "Epoch 00015: loss did not improve from 5.96756\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9733\n",
      "Epoch 16/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9667\n",
      "Epoch 00016: loss did not improve from 5.96756\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9755\n",
      "Epoch 17/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9727\n",
      "Epoch 00017: loss did not improve from 5.96756\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9743\n",
      "Epoch 18/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9624\n",
      "Epoch 00018: loss did not improve from 5.96756\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9707\n",
      "Epoch 19/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9667\n",
      "Epoch 00019: loss improved from 5.96756 to 5.96475, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9647\n",
      "Epoch 20/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9434\n",
      "Epoch 00020: loss improved from 5.96475 to 5.95874, saving model to weights/lstm.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21780052f98>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(x, y, batch_size=128, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline #4: LSTM + Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v = Sequential()\n",
    "\n",
    "lstm_w2v.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[w2v_model.wv.vectors]))\n",
    "lstm_w2v.add(LSTM(256, input_shape=(vocab_size, 100), return_sequences=True))\n",
    "lstm_w2v.add(Dropout(0.2))\n",
    "lstm_w2v.add(LSTM(256, return_sequences=True))\n",
    "lstm_w2v.add(Dropout(0.2))\n",
    "lstm_w2v.add(LSTM(128))\n",
    "lstm_w2v.add(Dropout(0.2))\n",
    "lstm_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1485 samples\n",
      "Epoch 1/20\n",
      "1408/1485 [===========================>..] - ETA: 1s - loss: 7.3875\n",
      "Epoch 00001: loss improved from 8.92470 to 7.33414, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 21s 14ms/sample - loss: 7.3341\n",
      "Epoch 2/20\n",
      "1408/1485 [===========================>..] - ETA: 1s - loss: 6.3100\n",
      "Epoch 00002: loss improved from 7.33414 to 6.29197, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 22s 15ms/sample - loss: 6.2920\n",
      "Epoch 3/20\n",
      "1408/1485 [===========================>..] - ETA: 1s - loss: 6.0830\n",
      "Epoch 00003: loss improved from 6.29197 to 6.08482, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 24s 16ms/sample - loss: 6.0848\n",
      "Epoch 4/20\n",
      "1408/1485 [===========================>..] - ETA: 1s - loss: 6.0126\n",
      "Epoch 00004: loss improved from 6.08482 to 6.02693, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 40s 27ms/sample - loss: 6.0269\n",
      "Epoch 5/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 6.0140\n",
      "Epoch 00005: loss improved from 6.02693 to 6.00411, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 6.0041\n",
      "Epoch 6/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9993\n",
      "Epoch 00006: loss improved from 6.00411 to 5.99680, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9968\n",
      "Epoch 7/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9908\n",
      "Epoch 00007: loss improved from 5.99680 to 5.98746, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9875\n",
      "Epoch 8/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9742\n",
      "Epoch 00008: loss improved from 5.98746 to 5.98054, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9805\n",
      "Epoch 9/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9759\n",
      "Epoch 00009: loss improved from 5.98054 to 5.97674, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 49s 33ms/sample - loss: 5.9767\n",
      "Epoch 10/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9890\n",
      "Epoch 00010: loss did not improve from 5.97674\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9798\n",
      "Epoch 11/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9761\n",
      "Epoch 00011: loss did not improve from 5.97674\n",
      "1485/1485 [==============================] - 50s 33ms/sample - loss: 5.9772\n",
      "Epoch 12/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9634\n",
      "Epoch 00012: loss improved from 5.97674 to 5.97425, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9742\n",
      "Epoch 13/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9760\n",
      "Epoch 00013: loss did not improve from 5.97425\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9760\n",
      "Epoch 14/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9618\n",
      "Epoch 00014: loss improved from 5.97425 to 5.97398, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 51s 34ms/sample - loss: 5.9740\n",
      "Epoch 15/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9592\n",
      "Epoch 00015: loss improved from 5.97398 to 5.96986, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9699\n",
      "Epoch 16/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9551\n",
      "Epoch 00016: loss improved from 5.96986 to 5.96297, saving model to weights/lstm_w2v.hdf5\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9630\n",
      "Epoch 17/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9545\n",
      "Epoch 00017: loss did not improve from 5.96297\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9654\n",
      "Epoch 18/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9528\n",
      "Epoch 00018: loss did not improve from 5.96297\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9652\n",
      "Epoch 19/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9675\n",
      "Epoch 00019: loss did not improve from 5.96297\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9694\n",
      "Epoch 20/20\n",
      "1408/1485 [===========================>..] - ETA: 2s - loss: 5.9586\n",
      "Epoch 00020: loss did not improve from 5.96297\n",
      "1485/1485 [==============================] - 50s 34ms/sample - loss: 5.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217de5de240>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_w2v.fit(x, y, batch_size=128, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "  \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "  \n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=lstm, prompt=\"In this paper\", words=20, temperature=0.2):\n",
    "    word_ids = [word_to_id(word) for word in prompt.lower().split()]\n",
    "    \n",
    "    for i in range(words):\n",
    "        prediction = model.predict(x=np.array(word_ids))\n",
    "        id = sample(prediction[-1], temperature)\n",
    "        word_ids.append(id)\n",
    "    \n",
    "    return ' '.join(id_to_word(id) for id in word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper experimented technical ignorant literary agent moderation mos falls gaokao readily parlaclarin constellations mass sufficient jupyter opus porting point synonym pgn'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_w2v, prompt=\"In this paper\", words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
