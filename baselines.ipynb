{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import GRU, LSTM, Activation, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33831, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/x.npy')\n",
    "y = np.load('data/y.npy')[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17628, 133), (17628,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate history to save losses\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #1: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = Sequential()\n",
    "\n",
    "gru.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "gru.add(GRU(128, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru.add(Dropout(0.3))\n",
    "gru.add(GRU(128))\n",
    "gru.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/gru.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.8340\n",
      "Epoch 00001: val_loss improved from inf to 7.33138, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 160s 11ms/sample - loss: 7.8342 - val_loss: 7.3314\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.5751\n",
      "Epoch 00002: val_loss improved from 7.33138 to 6.92182, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 158s 11ms/sample - loss: 6.5752 - val_loss: 6.9218\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.0885\n",
      "Epoch 00003: val_loss improved from 6.92182 to 6.73195, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 160s 11ms/sample - loss: 6.0878 - val_loss: 6.7319\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.7693\n",
      "Epoch 00004: val_loss improved from 6.73195 to 6.46204, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 162s 11ms/sample - loss: 5.7680 - val_loss: 6.4620\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.4927\n",
      "Epoch 00005: val_loss improved from 6.46204 to 6.31441, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 160s 11ms/sample - loss: 5.4934 - val_loss: 6.3144\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.1814\n",
      "Epoch 00006: val_loss improved from 6.31441 to 6.15739, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 159s 11ms/sample - loss: 5.1826 - val_loss: 6.1574\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.8704\n",
      "Epoch 00007: val_loss improved from 6.15739 to 5.94442, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 4.8692 - val_loss: 5.9444\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.5647\n",
      "Epoch 00008: val_loss improved from 5.94442 to 5.62560, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 160s 11ms/sample - loss: 4.5640 - val_loss: 5.6256\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.3119\n",
      "Epoch 00009: val_loss improved from 5.62560 to 5.58116, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 162s 11ms/sample - loss: 4.3122 - val_loss: 5.5812\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.1580\n",
      "Epoch 00010: val_loss improved from 5.58116 to 5.41713, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 161s 11ms/sample - loss: 4.1572 - val_loss: 5.4171\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.8644\n",
      "Epoch 00011: val_loss improved from 5.41713 to 5.27120, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 161s 11ms/sample - loss: 3.8634 - val_loss: 5.2712\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.6717\n",
      "Epoch 00012: val_loss did not improve from 5.27120\n",
      "14102/14102 [==============================] - 159s 11ms/sample - loss: 3.6721 - val_loss: 5.2936\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.5298\n",
      "Epoch 00013: val_loss did not improve from 5.27120\n",
      "14102/14102 [==============================] - 161s 11ms/sample - loss: 3.5311 - val_loss: 5.5380\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.4095\n",
      "Epoch 00014: val_loss improved from 5.27120 to 5.04288, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 3.4099 - val_loss: 5.0429\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.1493\n",
      "Epoch 00015: val_loss improved from 5.04288 to 4.99557, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 163s 12ms/sample - loss: 3.1488 - val_loss: 4.9956\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.9774\n",
      "Epoch 00016: val_loss did not improve from 4.99557\n",
      "14102/14102 [==============================] - 165s 12ms/sample - loss: 2.9773 - val_loss: 5.0203\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.7765\n",
      "Epoch 00017: val_loss improved from 4.99557 to 4.88822, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 163s 12ms/sample - loss: 2.7766 - val_loss: 4.8882\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.5447\n",
      "Epoch 00018: val_loss did not improve from 4.88822\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 3.5449 - val_loss: 5.2499\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.8838\n",
      "Epoch 00019: val_loss did not improve from 4.88822\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 2.8842 - val_loss: 4.9728\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.5585\n",
      "Epoch 00020: val_loss improved from 4.88822 to 4.88585, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 2.5578 - val_loss: 4.8858\n"
     ]
    }
   ],
   "source": [
    "gru_loss = gru.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #2: GRU + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gru_w2v = Sequential()\n",
    "\n",
    "gru_w2v.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[w2v_model.wv.vectors]))\n",
    "gru_w2v.add(GRU(128, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru_w2v.add(Dropout(0.3))\n",
    "gru_w2v.add(GRU(128))\n",
    "gru_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = \"weights/gru_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.8468\n",
      "Epoch 00001: val_loss improved from inf to 7.47606, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 7.8454 - val_loss: 7.4761\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.8072\n",
      "Epoch 00002: val_loss improved from 7.47606 to 7.46903, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 163s 12ms/sample - loss: 6.8069 - val_loss: 7.4690\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.4827\n",
      "Epoch 00003: val_loss improved from 7.46903 to 6.94594, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 165s 12ms/sample - loss: 6.4825 - val_loss: 6.9459\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.9877\n",
      "Epoch 00004: val_loss improved from 6.94594 to 6.44232, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 5.9868 - val_loss: 6.4423\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.5137\n",
      "Epoch 00005: val_loss improved from 6.44232 to 6.12578, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 166s 12ms/sample - loss: 5.5138 - val_loss: 6.1258\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.1342\n",
      "Epoch 00006: val_loss improved from 6.12578 to 5.88748, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 167s 12ms/sample - loss: 5.1340 - val_loss: 5.8875\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.7295\n",
      "Epoch 00007: val_loss improved from 5.88748 to 5.58913, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 167s 12ms/sample - loss: 4.7279 - val_loss: 5.5891\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.3491\n",
      "Epoch 00008: val_loss improved from 5.58913 to 5.51689, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 170s 12ms/sample - loss: 4.3483 - val_loss: 5.5169\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.0034\n",
      "Epoch 00009: val_loss improved from 5.51689 to 5.18818, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 167s 12ms/sample - loss: 4.0030 - val_loss: 5.1882\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.6945\n",
      "Epoch 00010: val_loss improved from 5.18818 to 5.07204, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 169s 12ms/sample - loss: 3.6937 - val_loss: 5.0720\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.4193\n",
      "Epoch 00011: val_loss improved from 5.07204 to 4.99528, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 3.4204 - val_loss: 4.9953\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.1308\n",
      "Epoch 00012: val_loss improved from 4.99528 to 4.89771, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 3.1314 - val_loss: 4.8977\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.8865\n",
      "Epoch 00013: val_loss improved from 4.89771 to 4.82547, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 169s 12ms/sample - loss: 2.8878 - val_loss: 4.8255\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.6249\n",
      "Epoch 00014: val_loss improved from 4.82547 to 4.79091, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 2.6254 - val_loss: 4.7909\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.4134\n",
      "Epoch 00015: val_loss improved from 4.79091 to 4.70538, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 172s 12ms/sample - loss: 2.4123 - val_loss: 4.7054\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.1816\n",
      "Epoch 00016: val_loss improved from 4.70538 to 4.69409, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 2.1814 - val_loss: 4.6941\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.9929\n",
      "Epoch 00017: val_loss improved from 4.69409 to 4.65358, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 169s 12ms/sample - loss: 1.9921 - val_loss: 4.6536\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.8032\n",
      "Epoch 00018: val_loss did not improve from 4.65358\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 1.8038 - val_loss: 4.6782\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.6039\n",
      "Epoch 00019: val_loss improved from 4.65358 to 4.62693, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 168s 12ms/sample - loss: 1.6041 - val_loss: 4.6269\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00020: val_loss improved from 4.62693 to 4.61792, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 173s 12ms/sample - loss: 1.4525 - val_loss: 4.6179\n"
     ]
    }
   ],
   "source": [
    "gru_w2v_loss = gru_w2v.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #3: LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "lstm.add(LSTM(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "lstm.add(Dropout(0.3))\n",
    "lstm.add(LSTM(256, return_sequences=True))\n",
    "lstm.add(Dropout(0.3))\n",
    "lstm.add(LSTM(128))\n",
    "lstm.add(Dropout(0.3))\n",
    "lstm.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.8955\n",
      "Epoch 00001: val_loss improved from inf to 7.38688, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 528s 37ms/sample - loss: 7.8935 - val_loss: 7.3869\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.9568\n",
      "Epoch 00002: val_loss improved from 7.38688 to 7.37773, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 573s 41ms/sample - loss: 6.9567 - val_loss: 7.3777\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.8096\n",
      "Epoch 00003: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 596s 42ms/sample - loss: 6.8092 - val_loss: 7.4085\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7498\n",
      "Epoch 00004: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 593s 42ms/sample - loss: 6.7507 - val_loss: 7.4348\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7249\n",
      "Epoch 00005: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 599s 42ms/sample - loss: 6.7247 - val_loss: 7.4861\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7005\n",
      "Epoch 00006: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 602s 43ms/sample - loss: 6.7007 - val_loss: 7.5028\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6859\n",
      "Epoch 00007: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 610s 43ms/sample - loss: 6.6848 - val_loss: 7.5372\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6741\n",
      "Epoch 00008: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 602s 43ms/sample - loss: 6.6743 - val_loss: 7.5583\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7321\n",
      "Epoch 00009: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 615s 44ms/sample - loss: 6.7318 - val_loss: 7.5778\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6684\n",
      "Epoch 00010: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 617s 44ms/sample - loss: 6.6692 - val_loss: 7.5843\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6471\n",
      "Epoch 00011: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 617s 44ms/sample - loss: 6.6475 - val_loss: 7.7074\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6182\n",
      "Epoch 00012: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 614s 44ms/sample - loss: 6.6199 - val_loss: 7.7740\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.5884\n",
      "Epoch 00013: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 612s 43ms/sample - loss: 6.5886 - val_loss: 7.7744\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.5319\n",
      "Epoch 00014: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 617s 44ms/sample - loss: 6.5326 - val_loss: 7.7610\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.3865\n",
      "Epoch 00015: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 613s 43ms/sample - loss: 6.3868 - val_loss: 7.6350\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.1868\n",
      "Epoch 00016: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 616s 44ms/sample - loss: 6.1875 - val_loss: 7.4666\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.9945\n",
      "Epoch 00017: val_loss did not improve from 7.37773\n",
      "14102/14102 [==============================] - 608s 43ms/sample - loss: 5.9946 - val_loss: 7.4060\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.8375\n",
      "Epoch 00018: val_loss improved from 7.37773 to 7.29852, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 615s 44ms/sample - loss: 5.8387 - val_loss: 7.2985\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.6426\n",
      "Epoch 00019: val_loss improved from 7.29852 to 7.07393, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 621s 44ms/sample - loss: 5.6410 - val_loss: 7.0739\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.4523\n",
      "Epoch 00020: val_loss improved from 7.07393 to 7.05773, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 491s 35ms/sample - loss: 5.4523 - val_loss: 7.0577\n"
     ]
    }
   ],
   "source": [
    "lstm_loss = lstm.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #4: LSTM + Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v = Sequential()\n",
    "\n",
    "lstm_w2v.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[w2v_model.wv.vectors]))\n",
    "lstm_w2v.add(LSTM(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "lstm_w2v.add(Dropout(0.3))\n",
    "lstm_w2v.add(LSTM(256, return_sequences=True))\n",
    "lstm_w2v.add(Dropout(0.3))\n",
    "lstm_w2v.add(LSTM(128))\n",
    "lstm_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.8111\n",
      "Epoch 00001: val_loss improved from inf to 7.43378, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 486s 34ms/sample - loss: 7.8100 - val_loss: 7.4338\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7985\n",
      "Epoch 00002: val_loss did not improve from 7.43378\n",
      "14102/14102 [==============================] - 484s 34ms/sample - loss: 6.7979 - val_loss: 7.4602\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7215\n",
      "Epoch 00003: val_loss did not improve from 7.43378\n",
      "14102/14102 [==============================] - 492s 35ms/sample - loss: 6.7233 - val_loss: 7.5397\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.7046\n",
      "Epoch 00004: val_loss did not improve from 7.43378\n",
      "14102/14102 [==============================] - 495s 35ms/sample - loss: 6.7046 - val_loss: 7.5245\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6896\n",
      "Epoch 00005: val_loss did not improve from 7.43378\n",
      "14102/14102 [==============================] - 495s 35ms/sample - loss: 6.6895 - val_loss: 7.5870\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.6744\n",
      "Epoch 00006: val_loss did not improve from 7.43378\n",
      "14102/14102 [==============================] - 496s 35ms/sample - loss: 6.6738 - val_loss: 7.5816\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.4399\n",
      "Epoch 00007: val_loss improved from 7.43378 to 7.05544, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 498s 35ms/sample - loss: 6.4391 - val_loss: 7.0554\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.1614\n",
      "Epoch 00008: val_loss improved from 7.05544 to 6.95545, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 500s 35ms/sample - loss: 6.1623 - val_loss: 6.9555\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.9742\n",
      "Epoch 00009: val_loss improved from 6.95545 to 6.58428, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 500s 35ms/sample - loss: 5.9734 - val_loss: 6.5843\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.7552\n",
      "Epoch 00010: val_loss improved from 6.58428 to 6.46152, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 505s 36ms/sample - loss: 5.7561 - val_loss: 6.4615\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.5335\n",
      "Epoch 00011: val_loss improved from 6.46152 to 6.38016, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 506s 36ms/sample - loss: 5.5327 - val_loss: 6.3802\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.3950\n",
      "Epoch 00012: val_loss improved from 6.38016 to 6.27737, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 507s 36ms/sample - loss: 5.3948 - val_loss: 6.2774\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.2850\n",
      "Epoch 00013: val_loss improved from 6.27737 to 6.19511, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 505s 36ms/sample - loss: 5.2844 - val_loss: 6.1951\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.2204\n",
      "Epoch 00014: val_loss did not improve from 6.19511\n",
      "14102/14102 [==============================] - 508s 36ms/sample - loss: 5.2201 - val_loss: 6.2061\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.0850\n",
      "Epoch 00015: val_loss improved from 6.19511 to 6.12172, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 508s 36ms/sample - loss: 5.0837 - val_loss: 6.1217\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.9751\n",
      "Epoch 00016: val_loss improved from 6.12172 to 6.07856, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 507s 36ms/sample - loss: 4.9750 - val_loss: 6.0786\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.9328\n",
      "Epoch 00017: val_loss improved from 6.07856 to 6.04744, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 503s 36ms/sample - loss: 4.9327 - val_loss: 6.0474\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.7357\n",
      "Epoch 00018: val_loss improved from 6.04744 to 5.87321, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 502s 36ms/sample - loss: 4.7352 - val_loss: 5.8732\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.5653\n",
      "Epoch 00019: val_loss improved from 5.87321 to 5.78828, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 502s 36ms/sample - loss: 4.5645 - val_loss: 5.7883\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.4796\n",
      "Epoch 00020: val_loss improved from 5.78828 to 5.72500, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 500s 35ms/sample - loss: 4.4782 - val_loss: 5.7250\n"
     ]
    }
   ],
   "source": [
    "lstm_w2v_loss = lstm_w2v.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.8341645916511515,\n",
       "  6.575212576358543,\n",
       "  6.087811384889491,\n",
       "  5.767957742629094,\n",
       "  5.493398336430269,\n",
       "  5.182574231917766,\n",
       "  4.869159506290724,\n",
       "  4.563995022314556,\n",
       "  4.3122422595410255,\n",
       "  4.157215563663606,\n",
       "  3.863440603453155,\n",
       "  3.672141636640294,\n",
       "  3.531058174179828,\n",
       "  3.409918043228299,\n",
       "  3.1487538343523434,\n",
       "  2.9773093582535277,\n",
       "  2.7766302244288923,\n",
       "  3.5449167590938617,\n",
       "  2.8842256669371946,\n",
       "  2.55776538708213],\n",
       " [7.331378291428123,\n",
       "  6.921815133270592,\n",
       "  6.731945537939304,\n",
       "  6.462044506808674,\n",
       "  6.314413201585793,\n",
       "  6.157391627674676,\n",
       "  5.94441581894847,\n",
       "  5.62559708559033,\n",
       "  5.581162776449358,\n",
       "  5.4171294860385455,\n",
       "  5.271198965917578,\n",
       "  5.2936142744560915,\n",
       "  5.537974470390627,\n",
       "  5.0428843839021145,\n",
       "  4.995565172899736,\n",
       "  5.020266127464653,\n",
       "  4.888221364229563,\n",
       "  5.249857886569174,\n",
       "  4.97281373338272,\n",
       "  4.885848040210207])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_loss.history['loss'], gru_loss.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.845446772853325,\n",
       "  6.806934561463658,\n",
       "  6.482452941302661,\n",
       "  5.986814778065144,\n",
       "  5.513763695672089,\n",
       "  5.133950507010218,\n",
       "  4.727881537077529,\n",
       "  4.348273793275839,\n",
       "  4.00301337935302,\n",
       "  3.6937359691663696,\n",
       "  3.4204171263500402,\n",
       "  3.1314307973834916,\n",
       "  2.887846521309396,\n",
       "  2.625431431217272,\n",
       "  2.4122874984976552,\n",
       "  2.181352887656093,\n",
       "  1.9920630986226262,\n",
       "  1.8038288756746415,\n",
       "  1.6040568960556627,\n",
       "  1.4524924337838563],\n",
       " [7.476056483004219,\n",
       "  7.469031755314317,\n",
       "  6.945937311115146,\n",
       "  6.442320265150584,\n",
       "  6.12578002688429,\n",
       "  5.887483444527612,\n",
       "  5.589130960130178,\n",
       "  5.516890495254854,\n",
       "  5.188178329392042,\n",
       "  5.072040264402272,\n",
       "  4.9952848808236645,\n",
       "  4.897714528588497,\n",
       "  4.825465497954353,\n",
       "  4.79090562843154,\n",
       "  4.705375311933183,\n",
       "  4.694088408437157,\n",
       "  4.653583645888235,\n",
       "  4.678163722098313,\n",
       "  4.626926575475164,\n",
       "  4.61792118956403])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_w2v_loss.history['loss'], gru_w2v_loss.history['val_loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.89347024235112,\n",
       "  6.956729783418076,\n",
       "  6.8092470493676425,\n",
       "  6.7506687555595795,\n",
       "  6.724674624460772,\n",
       "  6.700719599520259,\n",
       "  6.6847503182905275,\n",
       "  6.674327410631798,\n",
       "  6.731778434955758,\n",
       "  6.669151969301567,\n",
       "  6.647520798315411,\n",
       "  6.619873300813949,\n",
       "  6.58855637954898,\n",
       "  6.532592772558351,\n",
       "  6.38683921261859,\n",
       "  6.187477502226407,\n",
       "  5.994619360241953,\n",
       "  5.838692889824233,\n",
       "  5.640957064703667,\n",
       "  5.452253699285767],\n",
       " [7.386884759112102,\n",
       "  7.377732946057247,\n",
       "  7.408487534698815,\n",
       "  7.434756687400156,\n",
       "  7.486120041702798,\n",
       "  7.502830002839364,\n",
       "  7.537222785430406,\n",
       "  7.558284343539891,\n",
       "  7.57775523834315,\n",
       "  7.584296000930172,\n",
       "  7.707392449684598,\n",
       "  7.774013583658774,\n",
       "  7.774364192852295,\n",
       "  7.760990731363194,\n",
       "  7.634970129989997,\n",
       "  7.4665791175612926,\n",
       "  7.406004113766935,\n",
       "  7.298523410639165,\n",
       "  7.073926996522104,\n",
       "  7.057730688808049])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_loss.history['loss'], lstm_loss.history['val_loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.810007195357075,\n",
       "  6.79793516691146,\n",
       "  6.723323032883377,\n",
       "  6.704629453695035,\n",
       "  6.689473740785042,\n",
       "  6.673809045359626,\n",
       "  6.4391152050931,\n",
       "  6.162315641121059,\n",
       "  5.97338697714529,\n",
       "  5.756078058498969,\n",
       "  5.5327315499572585,\n",
       "  5.39480724038511,\n",
       "  5.284380430398199,\n",
       "  5.220097092698946,\n",
       "  5.083741643989767,\n",
       "  4.975039355451545,\n",
       "  4.9327438987750325,\n",
       "  4.73517405187098,\n",
       "  4.564463304401442,\n",
       "  4.478178289693698],\n",
       " [7.433782300013274,\n",
       "  7.460221233248913,\n",
       "  7.539689917085662,\n",
       "  7.524469950627821,\n",
       "  7.586973894609353,\n",
       "  7.581624762956486,\n",
       "  7.055442507133116,\n",
       "  6.955451033295249,\n",
       "  6.584280345092358,\n",
       "  6.461518313894201,\n",
       "  6.380163464029607,\n",
       "  6.277374869101574,\n",
       "  6.195105610824212,\n",
       "  6.20614134474228,\n",
       "  6.121717579582504,\n",
       "  6.078564567858025,\n",
       "  6.047441920704011,\n",
       "  5.873205572248384,\n",
       "  5.788283335912383,\n",
       "  5.724998967727341])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_w2v_loss.history['loss'], lstm_w2v_loss.history['val_loss'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using top k sampling\n",
    "def sample(preds, top_k):\n",
    "    \n",
    "    top_ids = preds.argsort()[-top_k:][::-1]\n",
    "    next_id = top_ids[random.sample(range(top_k),1)[0]]\n",
    "    \n",
    "    return next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=gru, prompt='In this paper', n=20, top_k=10):\n",
    "    \n",
    "    word_ids = [word_to_id(word) for word in prompt.lower().split()]\n",
    "    \n",
    "    for i in range(n):\n",
    "        prediction = model.predict(x=np.array(word_ids))\n",
    "        id = sample(prediction[-1], top_k)\n",
    "        word_ids.append(id)\n",
    "        \n",
    "    words = [id_to_word(w) for w in word_ids]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper docre xie copula cmn cpg e correctthe docre xie copula docre docre rerun slovenia buffered correctthe accommodated de correctthe cpg'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper backpropagation also anonymous including freeform ethical approach new new of and workload extracted minutes cubic also explore sql contributions cubic'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=gru_w2v, n=20, top_k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here we propose paper existing performance existing by are model based have we results word existing can level classification we task language our we on models are classification a experiments of two are models experiments of our paper words neural that approach of learning as level of based different be level as to'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm, prompt='here we propose', n=50, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here we propose abstraites cbt adam terrorists syntaxe dragon simplequestions in corpora datasets of a speech approach that pair datasets language an domain architecture on an then of corpora state and models on language data the how pair embeddings speech as screen first approach networks we machine machine architecture to with to experimental'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_w2v, prompt='here we propose', n=50, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we present a novel approach a task language multilingual language then that was as headache our on speech four networks an network our finally on'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_w2v, prompt='In this paper we present a novel approach', n=20, top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {gru_loss: 'GRU', gru_w2v_loss: 'GRU + Word2Vec', lstm_loss: 'LSTM', lstm_w2v_loss: 'LSTM + Word2Vec'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimimum validation loss within a set num of epochs\n",
    "def min_val_loss(model, max_epochs=20):\n",
    "    return min(model.history['val_loss'][:max_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss for GRU: 4.88585\n",
      "Perplexity for model GRU: 132.40\n",
      "\n",
      "Minimum validation loss for GRU + Word2Vec: 4.61792\n",
      "Perplexity for model GRU + Word2Vec: 101.28\n",
      "\n",
      "Minimum validation loss for LSTM: 7.05773\n",
      "Perplexity for model LSTM: 1161.81\n",
      "\n",
      "Minimum validation loss for LSTM + Word2Vec: 5.72500\n",
      "Perplexity for model LSTM + Word2Vec: 306.43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models.keys():\n",
    "    print(\"Minimum validation loss for {}: {:.5f}\".format(models[m], min_val_loss(m)))\n",
    "    print(\"Perplexity for model {}: {:.2f}\\n\".format(models[m], math.exp(min_val_loss(m))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
