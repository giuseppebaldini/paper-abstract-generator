{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Activation, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from pybtex.database import parse_file\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_data = parse_file('data/test_dataset.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wassa-2021-approaches',\n",
       " 'xiang-etal-2021-toxccin',\n",
       " 'kerz-etal-2021-language',\n",
       " 'lindow-etal-2021-partisanship',\n",
       " 'akula-garibay-2021-explainable',\n",
       " 'troiano-etal-2021-emotion',\n",
       " 'dayanik-pado-2021-disentangling',\n",
       " 'lamprinidis-etal-2021-universal',\n",
       " 'bianchi-etal-2021-feel']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bib_data.entries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiang-etal-2021-toxccin\n",
      "kerz-etal-2021-language\n",
      "lindow-etal-2021-partisanship\n",
      "akula-garibay-2021-explainable\n",
      "troiano-etal-2021-emotion\n",
      "dayanik-pado-2021-disentangling\n",
      "lamprinidis-etal-2021-universal\n",
      "bianchi-etal-2021-feel\n"
     ]
    }
   ],
   "source": [
    "for k in bib_data.entries.keys():\n",
    "    try:\n",
    "        f = open('data.txt', 'a')\n",
    "        f.write(bib_data.entries[k].fields['abstract'])\n",
    "        f.close()\n",
    "        print(k)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = open(\"data.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #1: char-level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate stop words\n",
    "def tokenize_input(input):\n",
    "    # lowercase\n",
    "    input = input.lower()\n",
    "\n",
    "    # use tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # end result in final\n",
    "    final = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = tokenize_input(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'despite recent successes transformer based models terms effectiveness variety tasks decisions often remain opaque humans explanations particularly important tasks like offensive language toxicity detection social media manual appeal process often place dispute automatically flagged content work propose technique improve interpretability models based simple powerful assumption post least toxic toxic span incorporate assumption transformer models scoring post based maximum toxicity spans augmenting training process identify correct spans find approach effective produce explanations exceed quality provided logistic regression analysis often regarded highly interpretable model according human study aim paper twofold 1 automatically predict ratings assigned viewers 14 categories available ted talks multi label classification task 2 determine types features drive classification accuracy categories focus features language usage five groups pertaining syntactic complexity lexical richness register based n gram measures information theoretic measures liwc style measures show recurrent neural network classifier trained exclusively within text distributions features reach relatively high levels overall accuracy 69 across 14 categories find features two groups strong predictors affective ratings across categories distinct patterns language usage rating category ideological differences large impact individual community response covid 19 pandemic united states early behavioral research pandemic showed conservatives less likely adhere health directives contradicts body work suggesting conservative ideology emphasizes rule abiding loss aversion prevention focus reconcile contradiction analyzing semantic content local press releases federal press releases localized tweets first month government response covid 19 united states controlling factors covid 19 confirmed cases deaths local economic indicators find online expressions fear conservative areas lead increase adherence public health recommendations concerning covid 19 expressions fear government press releases significant predictor expressed fear twitter sarcasm linguistic expression often used communicate opposite said usually something unpleasant intention insult ridicule inherent ambiguity sarcastic expressions makes sarcasm detection difficult work focus detecting sarcasm textual conversations written english various social networking platforms online media end develop interpretable deep learning model using multi head self attention gated recurrent units show effectiveness interpretability approach achieving state art results datasets social networking platforms online discussion forums political dialogues humans judge affective content texts also implicitly assess correctness judgment confidence hypothesize people confidence performed well annotation task leads dis agreements among true confidence may serve diagnostic tool systematic differences annotations probe assumption conduct study subset corpus contemporary american english ask raters distinguish neutral sentences emotion bearing ones scoring confidence answers confidence turns approximate inter annotator disagreements find confidence correlated emotion intensity perceiving stronger affect text prompts annotators certain classification performances insight relevant modelling studies intensity opens question wether automatic regressors classifiers actually predict intensity rather human self perceived confidence text classification central tool nlp however target classes strongly correlated textual attributes text classification models pick wrong features leading bad generalization biases social media analysis problem surfaces demographic user classes language topic gender influence generate text substantial extent adversarial training claimed mitigate problem thorough evaluation missing paper experiment text classification correlated attributes document topic author gender using novel multilingual parallel corpus ted talk transcripts findings individual classifiers topic author gender indeed biased b debiasing adversarial training works topic breaks author gender c gender debiasing results differ across languages interpret result terms feature space overlap highlighting role linguistic surface realization target classes emotions universal aspects human psychology expressed differently across different languages cultures introduce new data set 530k anonymized public facebook posts across 18 languages labeled five different emotions using multilingual bert embeddings show emotions reliably inferred within across languages zero shot learning produces promising results low resource languages following established theories basic emotions provide detailed analysis possibilities limits cross lingual emotion classification find structural typological similarity languages facilitates cross lingual learning well linguistic diversity training data results suggest commonalities underlying expression emotion different languages publicly release anonymized data future research sentiment analysis popular task understand people reactions online often need nuanced information post negative user angry sad abundance approaches introduced tackling tasks also italian treat one tasks introduce feel novel benchmark corpus italian twitter posts annotated four basic emotions textit anger textit fear textit joy textit sadness collapsing also sentiment analysis evaluate corpus benchmark datasets emotion sentiment classification obtaining competitive results release open source python library researchers use model trained feel inferring sentiments emotions italian text'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 5647\n",
      "Total vocab: 36\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total chars:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through inputs\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    \n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 5547\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "filepath = \"lstm_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5547 samples\n",
      "Epoch 1/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 3.1050\n",
      "Epoch 00001: loss improved from inf to 3.10250, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 66s 12ms/sample - loss: 3.1025\n",
      "Epoch 2/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9731\n",
      "Epoch 00002: loss improved from 3.10250 to 2.97373, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 76s 14ms/sample - loss: 2.9737\n",
      "Epoch 3/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9597\n",
      "Epoch 00003: loss improved from 2.97373 to 2.95766, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 76s 14ms/sample - loss: 2.9577\n",
      "Epoch 4/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9418\n",
      "Epoch 00004: loss improved from 2.95766 to 2.94320, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 71s 13ms/sample - loss: 2.9432\n",
      "Epoch 5/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9394\n",
      "Epoch 00005: loss improved from 2.94320 to 2.94045, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 70s 13ms/sample - loss: 2.9404\n",
      "Epoch 6/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9379\n",
      "Epoch 00006: loss improved from 2.94045 to 2.93801, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 70s 13ms/sample - loss: 2.9380\n",
      "Epoch 7/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9342\n",
      "Epoch 00007: loss improved from 2.93801 to 2.93517, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 73s 13ms/sample - loss: 2.9352\n",
      "Epoch 8/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9311\n",
      "Epoch 00008: loss improved from 2.93517 to 2.93384, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 70s 13ms/sample - loss: 2.9338\n",
      "Epoch 9/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9311\n",
      "Epoch 00009: loss improved from 2.93384 to 2.93119, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 72s 13ms/sample - loss: 2.9312\n",
      "Epoch 10/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9249\n",
      "Epoch 00010: loss improved from 2.93119 to 2.92339, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 76s 14ms/sample - loss: 2.9234\n",
      "Epoch 11/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9299\n",
      "Epoch 00011: loss did not improve from 2.92339\n",
      "5547/5547 [==============================] - 74s 13ms/sample - loss: 2.9299\n",
      "Epoch 12/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9235\n",
      "Epoch 00012: loss did not improve from 2.92339\n",
      "5547/5547 [==============================] - 73s 13ms/sample - loss: 2.9250\n",
      "Epoch 13/20\n",
      "5376/5547 [============================>.] - ETA: 1s - loss: 2.9267\n",
      "Epoch 00013: loss did not improve from 2.92339\n",
      "5547/5547 [==============================] - 64s 12ms/sample - loss: 2.9256\n",
      "Epoch 14/20\n",
      "5376/5547 [============================>.] - ETA: 1s - loss: 2.9148\n",
      "Epoch 00014: loss improved from 2.92339 to 2.91809, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 64s 11ms/sample - loss: 2.9181\n",
      "Epoch 15/20\n",
      "5376/5547 [============================>.] - ETA: 1s - loss: 2.9205\n",
      "Epoch 00015: loss did not improve from 2.91809\n",
      "5547/5547 [==============================] - 65s 12ms/sample - loss: 2.9230\n",
      "Epoch 16/20\n",
      "5376/5547 [============================>.] - ETA: 1s - loss: 2.9177\n",
      "Epoch 00016: loss did not improve from 2.91809\n",
      "5547/5547 [==============================] - 64s 12ms/sample - loss: 2.9183\n",
      "Epoch 17/20\n",
      "5376/5547 [============================>.] - ETA: 1s - loss: 2.9197\n",
      "Epoch 00017: loss did not improve from 2.91809\n",
      "5547/5547 [==============================] - 64s 12ms/sample - loss: 2.9185\n",
      "Epoch 18/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9162\n",
      "Epoch 00018: loss improved from 2.91809 to 2.91685, saving model to lstm_weights.hdf5\n",
      "5547/5547 [==============================] - 67s 12ms/sample - loss: 2.9169\n",
      "Epoch 19/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9175\n",
      "Epoch 00019: loss did not improve from 2.91685\n",
      "5547/5547 [==============================] - 68s 12ms/sample - loss: 2.9185\n",
      "Epoch 20/20\n",
      "5376/5547 [============================>.] - ETA: 2s - loss: 2.9199\n",
      "Epoch 00020: loss did not improve from 2.91685\n",
      "5547/5547 [==============================] - 67s 12ms/sample - loss: 2.9179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c40bf22f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lstm_weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Abstract: \n",
      "\n",
      "\" im paper twofold 1 automatically predict ratings assigned viewers 14 categories available ted talks  \"\n"
     ]
    }
   ],
   "source": [
    "# random seed initialization\n",
    "start = np.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Generated Abstract: \\n\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #2: char-level GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the unique characters present in the text\n",
    "vocabulary = sorted(list(set(text)))\n",
    "\n",
    "# Creating dictionaries to map each character to an index\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "steps = 5\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - max_length, steps):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "      \n",
    "# Hot encoding each character into a boolean vector\n",
    "  \n",
    "# Initializing a matrix of boolean vectors with each column representing\n",
    "# the hot encoded representation of the character\n",
    "X = np.zeros((len(sentences), max_length, len(vocabulary)), dtype = np.bool)\n",
    "y = np.zeros((len(sentences), len(vocabulary)), dtype = np.bool)\n",
    "  \n",
    "# Placing the value 1 at the appropriate position for each vector\n",
    "# to complete the hot-encoding process\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_indices[char]] = 1\n",
    "    y[i, char_to_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "  \n",
    "# Defining the cell type\n",
    "model.add(GRU(128, input_shape =(max_length, len(vocabulary))))\n",
    "  \n",
    "# Defining the densely connected Neural Network layer\n",
    "model.add(Dense(len(vocabulary)))\n",
    "  \n",
    "# Defining the activation function for the cell\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Defining the optimizing function\n",
    "optimizer = RMSprop(lr = 0.01)\n",
    "  \n",
    "# Configuring the model for training\n",
    "model.compile(loss ='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to sample an index from a probability array\n",
    "def sample_index(preds, temperature = 1.0):\n",
    "# temperature determines the freedom the function has when generating text\n",
    "  \n",
    "    # Converting the predictions vector into a numpy array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "  \n",
    "    # Normalizing the predicitons array\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "  \n",
    "    # The main sampling step. Creates an array of probablities signifying\n",
    "    # the probability of each character to be the next character in the \n",
    "    # generated text\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "  \n",
    "    # Returning the character with maximum probability to be the next character\n",
    "    # in the generated text\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch\n",
    "# in which the loss decreases\n",
    "filepath = \"gru_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor ='loss', \n",
    "                             save_best_only = True, \n",
    "                             mode ='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to reduce the learning rate each time the learning plateaus\n",
    "reduce_alpha = ReduceLROnPlateau(monitor ='loss', factor = 0.2,\n",
    "\t\t\t\t\t\t\tpatience = 1, min_lr = 0.001)\n",
    "callbacks = [checkpoint, reduce_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1454 samples\n",
      "Epoch 1/50\n",
      "1454/1454 [==============================] - 4s 3ms/sample - loss: 3.7443\n",
      "Epoch 2/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.1109\n",
      "Epoch 3/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0854\n",
      "Epoch 4/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0650\n",
      "Epoch 5/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0350\n",
      "Epoch 6/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 3.0174\n",
      "Epoch 7/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9836\n",
      "Epoch 8/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9648\n",
      "Epoch 9/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9223\n",
      "Epoch 10/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.9016\n",
      "Epoch 11/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.8640\n",
      "Epoch 12/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.8372\n",
      "Epoch 13/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7955\n",
      "Epoch 14/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7537\n",
      "Epoch 15/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.7336\n",
      "Epoch 16/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6890\n",
      "Epoch 17/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6517\n",
      "Epoch 18/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.6182\n",
      "Epoch 19/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5877\n",
      "Epoch 20/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5550\n",
      "Epoch 21/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.5257\n",
      "Epoch 22/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4972\n",
      "Epoch 23/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4656\n",
      "Epoch 24/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4454\n",
      "Epoch 25/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.4157\n",
      "Epoch 26/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3899\n",
      "Epoch 27/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3717\n",
      "Epoch 28/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3537\n",
      "Epoch 29/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.3230\n",
      "Epoch 30/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2985\n",
      "Epoch 31/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2774\n",
      "Epoch 32/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2547\n",
      "Epoch 33/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2330\n",
      "Epoch 34/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.2190\n",
      "Epoch 35/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1872\n",
      "Epoch 36/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1652\n",
      "Epoch 37/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1501\n",
      "Epoch 38/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.1200\n",
      "Epoch 39/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0999\n",
      "Epoch 40/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0754\n",
      "Epoch 41/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0554\n",
      "Epoch 42/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0340\n",
      "Epoch 43/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 2.0130\n",
      "Epoch 44/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9889\n",
      "Epoch 45/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9688\n",
      "Epoch 46/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9398\n",
      "Epoch 47/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.9254\n",
      "Epoch 48/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.8947\n",
      "Epoch 49/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.8758\n",
      "Epoch 50/50\n",
      "1454/1454 [==============================] - 3s 2ms/sample - loss: 1.8474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c40d1a6898>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the GRU model\n",
    "model.fit(X, y, batch_size = 128, epochs = 50, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    generated += next_char\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def generate_text(length, diversity):\n",
    "\t# Get random starting text\n",
    "\tstart_index = random.randint(0, len(text) - max_length - 1)\n",
    "\n",
    "\t# Defining the generated text\n",
    "\tgenerated = ''\n",
    "\tsentence = text[start_index: start_index + max_length]\n",
    "\tgenerated += sentence\n",
    "\n",
    "\t# Generating new text of given length\n",
    "\tfor i in range(length):\n",
    "\n",
    "\t\t\t# Initializing the predicition vector\n",
    "\t\t\tx_pred = np.zeros((1, max_length, len(vocabulary)))\n",
    "\t\t\tfor t, char in enumerate(sentence):\n",
    "\t\t\t\tx_pred[0, t, char_to_indices[char]] = 1.\n",
    "\n",
    "\t\t\t# Making the predicitons\n",
    "\t\t\tpreds = model.predict(x_pred, verbose = 0)[0]\n",
    "\n",
    "\t\t\t# Getting the index of the next most probable index\n",
    "\t\t\tnext_index = sample_index(preds, diversity)\n",
    "\n",
    "\t\t\t# Getting the most probable next character using the mapping built\n",
    "\t\t\tnext_char = indices_to_char[next_index]\n",
    "\n",
    "\t\t\t# Generating new text\n",
    "            generated += next_char\n",
    "\t\t\tsentence = sentence[1:] + next_char\n",
    "\treturn generated\n",
    "\n",
    "print(generate_text(500, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #3: Standard GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode('In this paper we present', \n",
    "                          max_length=1024, \n",
    "                          truncation=True,\n",
    "                          return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, \n",
    "                         max_length=200, \n",
    "                         do_sample=True)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level LSTM + pre-trained word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"data/arxiv_abstracts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 40\n",
    "\n",
    "with open(text_path) as file_:\n",
    "    docs = file_.readlines()\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_abs_model = gensim.models.Word2Vec(sentences, vector_size=100, min_count=1, window=5, epochs=5)\n",
    "w2v_abs_model.save(\"word2vec_arxiv_abstracts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = w2v_abs_model.wv.vectors.shape\n",
    "print(vocab_size,emdedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vector = w2v_abs_model.wv['computer']\n",
    "print(example_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_similar = w2v_abs_model.wv.most_similar('computer', topn=10) \n",
    "print(example_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_abs_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_abs_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_x[i, t] = word_to_id(word)\n",
    "    train_y[i] = word_to_id(sentence[-1])\n",
    "\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[w2v_abs_model.wv.vectors]))\n",
    "model.add(LSTM(256, input_shape=(vocab_size, 100), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "  \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "  \n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(text, num_generated=20):\n",
    "    word_ids = [word_to_id(word) for word in text.lower().split()]\n",
    "    \n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_ids))\n",
    "        id = sample(prediction[-1], temperature=0.5)\n",
    "        word_ids.append(id)\n",
    "    \n",
    "    return ' '.join(id_to_word(id) for id in word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(w2v_abs_model.wv.index_to_key)\n",
    "\n",
    "generate_next(\"Here we present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + custom-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + temporal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compass_path = \"data/twec_test/compass.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp import TWEC\n",
    "\n",
    "# siter is the number of iterations of the compass, diter is the number of iterations of each slice\n",
    "aligner = TWEC(vector_size=30, siter=10, diter=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 40\n",
    "\n",
    "with open(compass_path) as file_:\n",
    "    docs = file_.readlines()\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.train_compass(compass_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_one = aligner.train_slice(\"data/twec_test/arxiv_14.txt\", save=True) \n",
    "slice_two = aligner.train_slice(\"data/twec_test/arxiv_9.txt\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec.load(\"model/arxiv_14.model\")\n",
    "model2 = gensim.models.Word2Vec.load(\"model/arxiv_9.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = model2.wv.vectors.shape\n",
    "print(vocab_size,emdedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vector = model2.wv['computer']\n",
    "print(example_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_similar = model2.wv.most_similar('computer', topn=10) \n",
    "print(example_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return model2.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return model2.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_x[i, t] = word_to_id(word)\n",
    "    train_y[i] = word_to_id(sentence[-1])\n",
    "\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, weights=[model2.wv.vectors]))\n",
    "model.add(LSTM(256, input_shape=(vocab_size, 30), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(model2.wv.index_to_key)\n",
    "\n",
    "generate_next(\"This paper\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
