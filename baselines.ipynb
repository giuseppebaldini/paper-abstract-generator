{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import GRU, LSTM, Activation, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33831, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/x.npy')\n",
    "y = np.load('data/y.npy')[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17628, 133), (17628,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate history to save losses\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #1: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = Sequential()\n",
    "\n",
    "gru.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "gru.add(GRU(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru.add(Dropout(0.3))\n",
    "gru.add(GRU(128))\n",
    "gru.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/gru.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.7702\n",
      "Epoch 00001: val_loss improved from inf to 7.35866, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 354s 25ms/sample - loss: 7.7688 - val_loss: 7.3587\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.5069\n",
      "Epoch 00002: val_loss improved from 7.35866 to 6.63899, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 366s 26ms/sample - loss: 6.5061 - val_loss: 6.6390\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.9182\n",
      "Epoch 00003: val_loss improved from 6.63899 to 6.54964, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 367s 26ms/sample - loss: 5.9174 - val_loss: 6.5496\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.6636\n",
      "Epoch 00004: val_loss improved from 6.54964 to 6.49495, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 371s 26ms/sample - loss: 5.6633 - val_loss: 6.4950\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.4873\n",
      "Epoch 00005: val_loss improved from 6.49495 to 6.38037, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 371s 26ms/sample - loss: 5.4879 - val_loss: 6.3804\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.2333\n",
      "Epoch 00006: val_loss improved from 6.38037 to 6.20430, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 371s 26ms/sample - loss: 5.2330 - val_loss: 6.2043\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.0352\n",
      "Epoch 00007: val_loss improved from 6.20430 to 6.02909, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 364s 26ms/sample - loss: 5.0355 - val_loss: 6.0291\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.5956\n",
      "Epoch 00008: val_loss improved from 6.02909 to 5.77966, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 364s 26ms/sample - loss: 4.5953 - val_loss: 5.7797\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.4075\n",
      "Epoch 00009: val_loss improved from 5.77966 to 5.68209, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 360s 26ms/sample - loss: 4.4067 - val_loss: 5.6821\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.0061\n",
      "Epoch 00010: val_loss improved from 5.68209 to 5.44100, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 362s 26ms/sample - loss: 4.0065 - val_loss: 5.4410\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.7399\n",
      "Epoch 00011: val_loss improved from 5.44100 to 5.32221, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 367s 26ms/sample - loss: 3.7407 - val_loss: 5.3222\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.4418\n",
      "Epoch 00012: val_loss improved from 5.32221 to 5.14795, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 376s 27ms/sample - loss: 3.4422 - val_loss: 5.1479\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.1906\n",
      "Epoch 00013: val_loss improved from 5.14795 to 5.00885, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 365s 26ms/sample - loss: 3.1906 - val_loss: 5.0089\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.9533\n",
      "Epoch 00014: val_loss improved from 5.00885 to 4.87004, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 361s 26ms/sample - loss: 2.9537 - val_loss: 4.8700\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.7252\n",
      "Epoch 00015: val_loss improved from 4.87004 to 4.76029, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 368s 26ms/sample - loss: 2.7244 - val_loss: 4.7603\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.4963\n",
      "Epoch 00016: val_loss improved from 4.76029 to 4.63941, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 377s 27ms/sample - loss: 2.4946 - val_loss: 4.6394\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.3138\n",
      "Epoch 00017: val_loss improved from 4.63941 to 4.48813, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 371s 26ms/sample - loss: 2.3136 - val_loss: 4.4881\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.1726\n",
      "Epoch 00018: val_loss improved from 4.48813 to 4.44514, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 372s 26ms/sample - loss: 2.1723 - val_loss: 4.4451\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.9471\n",
      "Epoch 00019: val_loss improved from 4.44514 to 4.33850, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 365s 26ms/sample - loss: 1.9459 - val_loss: 4.3385\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.7865\n",
      "Epoch 00020: val_loss improved from 4.33850 to 4.25001, saving model to weights/gru.hdf5\n",
      "14102/14102 [==============================] - 369s 26ms/sample - loss: 1.7868 - val_loss: 4.2500\n"
     ]
    }
   ],
   "source": [
    "gru_loss = gru.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #2: GRU + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gru_w2v = Sequential()\n",
    "\n",
    "gru_w2v.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[w2v_model.wv.vectors]))\n",
    "gru_w2v.add(GRU(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "gru_w2v.add(Dropout(0.3))\n",
    "gru_w2v.add(GRU(128))\n",
    "gru_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = \"weights/gru_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 7.6924\n",
      "Epoch 00001: val_loss improved from inf to 7.06587, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 663s 47ms/sample - loss: 7.6911 - val_loss: 7.0659\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 6.3650\n",
      "Epoch 00002: val_loss improved from 7.06587 to 6.64127, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 594s 42ms/sample - loss: 6.3641 - val_loss: 6.6413\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 5.4405\n",
      "Epoch 00003: val_loss improved from 6.64127 to 5.66575, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 614s 44ms/sample - loss: 5.4405 - val_loss: 5.6657\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 4.3939\n",
      "Epoch 00004: val_loss improved from 5.66575 to 4.91596, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 615s 44ms/sample - loss: 4.3933 - val_loss: 4.9160\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 3.5811\n",
      "Epoch 00005: val_loss improved from 4.91596 to 4.31876, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 621s 44ms/sample - loss: 3.5819 - val_loss: 4.3188\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.9012\n",
      "Epoch 00006: val_loss improved from 4.31876 to 3.83396, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 625s 44ms/sample - loss: 2.9025 - val_loss: 3.8340\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 2.3117\n",
      "Epoch 00007: val_loss improved from 3.83396 to 3.46584, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 624s 44ms/sample - loss: 2.3136 - val_loss: 3.4658\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.8468\n",
      "Epoch 00008: val_loss improved from 3.46584 to 3.22089, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 621s 44ms/sample - loss: 1.8469 - val_loss: 3.2209\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.4716\n",
      "Epoch 00009: val_loss improved from 3.22089 to 3.05204, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 625s 44ms/sample - loss: 1.4705 - val_loss: 3.0520\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 1.1548\n",
      "Epoch 00010: val_loss improved from 3.05204 to 2.87980, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 623s 44ms/sample - loss: 1.1550 - val_loss: 2.8798\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.8954\n",
      "Epoch 00011: val_loss improved from 2.87980 to 2.74885, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 617s 44ms/sample - loss: 0.8949 - val_loss: 2.7488\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.6959\n",
      "Epoch 00012: val_loss improved from 2.74885 to 2.70643, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 622s 44ms/sample - loss: 0.6959 - val_loss: 2.7064\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.5212\n",
      "Epoch 00013: val_loss improved from 2.70643 to 2.66512, saving model to weights/gru_w2v.hdf5\n",
      "14102/14102 [==============================] - 623s 44ms/sample - loss: 0.5219 - val_loss: 2.6651\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.3886\n",
      "Epoch 00014: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 622s 44ms/sample - loss: 0.3887 - val_loss: 2.7131\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00015: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 616s 44ms/sample - loss: 0.2795 - val_loss: 2.6980\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00016: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 623s 44ms/sample - loss: 0.2059 - val_loss: 2.6986\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00017: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 626s 44ms/sample - loss: 0.1480 - val_loss: 2.6890\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00018: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 625s 44ms/sample - loss: 0.1084 - val_loss: 2.6702\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.0856\n",
      "Epoch 00019: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 625s 44ms/sample - loss: 0.0856 - val_loss: 2.7214\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 0s - loss: 0.0666\n",
      "Epoch 00020: val_loss did not improve from 2.66512\n",
      "14102/14102 [==============================] - 625s 44ms/sample - loss: 0.0667 - val_loss: 2.6753\n"
     ]
    }
   ],
   "source": [
    "gru_w2v_loss = gru_w2v.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #3: LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size))\n",
    "lstm.add(LSTM(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(256, return_sequences=True))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(256))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 7.9065\n",
      "Epoch 00001: val_loss improved from inf to 7.57240, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 714s 51ms/sample - loss: 7.9064 - val_loss: 7.5724\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.9839\n",
      "Epoch 00002: val_loss improved from 7.57240 to 7.41155, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 779s 55ms/sample - loss: 6.9842 - val_loss: 7.4116\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.7796\n",
      "Epoch 00003: val_loss did not improve from 7.41155\n",
      "14102/14102 [==============================] - 788s 56ms/sample - loss: 6.7801 - val_loss: 7.5079\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.7325\n",
      "Epoch 00004: val_loss did not improve from 7.41155\n",
      "14102/14102 [==============================] - 788s 56ms/sample - loss: 6.7324 - val_loss: 7.5367\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.8767\n",
      "Epoch 00005: val_loss did not improve from 7.41155\n",
      "14102/14102 [==============================] - 795s 56ms/sample - loss: 6.8762 - val_loss: 7.5986\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.6839\n",
      "Epoch 00006: val_loss did not improve from 7.41155\n",
      "14102/14102 [==============================] - 799s 57ms/sample - loss: 6.6834 - val_loss: 7.5720\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.6686\n",
      "Epoch 00007: val_loss did not improve from 7.41155\n",
      "14102/14102 [==============================] - 798s 57ms/sample - loss: 6.6684 - val_loss: 7.5752\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.6026\n",
      "Epoch 00008: val_loss improved from 7.41155 to 7.27573, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 797s 57ms/sample - loss: 6.6018 - val_loss: 7.2757\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.1395\n",
      "Epoch 00009: val_loss improved from 7.27573 to 6.97503, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 814s 58ms/sample - loss: 6.1402 - val_loss: 6.9750\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.8109\n",
      "Epoch 00010: val_loss improved from 6.97503 to 6.71988, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 808s 57ms/sample - loss: 5.8103 - val_loss: 6.7199\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.5927\n",
      "Epoch 00011: val_loss improved from 6.71988 to 6.58038, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 808s 57ms/sample - loss: 5.5936 - val_loss: 6.5804\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.4172\n",
      "Epoch 00012: val_loss did not improve from 6.58038\n",
      "14102/14102 [==============================] - 815s 58ms/sample - loss: 5.4177 - val_loss: 6.6475\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3785\n",
      "Epoch 00013: val_loss did not improve from 6.58038\n",
      "14102/14102 [==============================] - 815s 58ms/sample - loss: 5.3792 - val_loss: 7.3355\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3041\n",
      "Epoch 00014: val_loss did not improve from 6.58038\n",
      "14102/14102 [==============================] - 818s 58ms/sample - loss: 5.3031 - val_loss: 6.6573\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.0079\n",
      "Epoch 00015: val_loss did not improve from 6.58038\n",
      "14102/14102 [==============================] - 807s 57ms/sample - loss: 5.0075 - val_loss: 6.5996\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.1256\n",
      "Epoch 00016: val_loss did not improve from 6.58038\n",
      "14102/14102 [==============================] - 820s 58ms/sample - loss: 5.1249 - val_loss: 6.6965\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 4.8262\n",
      "Epoch 00017: val_loss improved from 6.58038 to 6.53958, saving model to weights/lstm.hdf5\n",
      "14102/14102 [==============================] - 826s 59ms/sample - loss: 4.8257 - val_loss: 6.5396\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 4.6835\n",
      "Epoch 00018: val_loss did not improve from 6.53958\n",
      "14102/14102 [==============================] - 826s 59ms/sample - loss: 4.6827 - val_loss: 6.6734\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 4.5819\n",
      "Epoch 00019: val_loss did not improve from 6.53958\n",
      "14102/14102 [==============================] - 808s 57ms/sample - loss: 4.5822 - val_loss: 6.5990\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 4.5624\n",
      "Epoch 00020: val_loss did not improve from 6.53958\n",
      "14102/14102 [==============================] - 826s 59ms/sample - loss: 4.5631 - val_loss: 6.6195\n"
     ]
    }
   ],
   "source": [
    "lstm_loss = lstm.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline #4: LSTM + Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v = Sequential()\n",
    "\n",
    "lstm_w2v.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[w2v_model.wv.vectors]))\n",
    "lstm_w2v.add(LSTM(256, input_shape=(vocab_size, emdedding_size), return_sequences=True))\n",
    "lstm_w2v.add(LSTM(256, return_sequences=True))\n",
    "lstm_w2v.add(LSTM(256))\n",
    "lstm_w2v.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights/lstm_w2v.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14102 samples, validate on 3526 samples\n",
      "Epoch 1/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 7.8536\n",
      "Epoch 00001: val_loss improved from inf to 7.38322, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 899s 64ms/sample - loss: 7.8520 - val_loss: 7.3832\n",
      "Epoch 2/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.9489\n",
      "Epoch 00002: val_loss did not improve from 7.38322\n",
      "14102/14102 [==============================] - 953s 68ms/sample - loss: 6.9484 - val_loss: 7.4103\n",
      "Epoch 3/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.7978\n",
      "Epoch 00003: val_loss improved from 7.38322 to 7.37524, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 1005s 71ms/sample - loss: 6.7973 - val_loss: 7.3752\n",
      "Epoch 4/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.4619\n",
      "Epoch 00004: val_loss improved from 7.37524 to 6.81413, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 986s 70ms/sample - loss: 6.4624 - val_loss: 6.8141\n",
      "Epoch 5/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 6.1001\n",
      "Epoch 00005: val_loss improved from 6.81413 to 6.60395, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 970s 69ms/sample - loss: 6.0999 - val_loss: 6.6039\n",
      "Epoch 6/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.9393\n",
      "Epoch 00006: val_loss did not improve from 6.60395\n",
      "14102/14102 [==============================] - 935s 66ms/sample - loss: 5.9397 - val_loss: 7.1162\n",
      "Epoch 7/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.9470\n",
      "Epoch 00007: val_loss did not improve from 6.60395\n",
      "14102/14102 [==============================] - 726s 51ms/sample - loss: 5.9477 - val_loss: 6.6466\n",
      "Epoch 8/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.7714\n",
      "Epoch 00008: val_loss improved from 6.60395 to 6.50708, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 727s 52ms/sample - loss: 5.7709 - val_loss: 6.5071\n",
      "Epoch 9/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.7004\n",
      "Epoch 00009: val_loss did not improve from 6.50708\n",
      "14102/14102 [==============================] - 723s 51ms/sample - loss: 5.7013 - val_loss: 6.5641\n",
      "Epoch 10/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.6438\n",
      "Epoch 00010: val_loss improved from 6.50708 to 6.49947, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 724s 51ms/sample - loss: 5.6443 - val_loss: 6.4995\n",
      "Epoch 11/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.6073\n",
      "Epoch 00011: val_loss did not improve from 6.49947\n",
      "14102/14102 [==============================] - 726s 51ms/sample - loss: 5.6074 - val_loss: 6.5884\n",
      "Epoch 12/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.6504\n",
      "Epoch 00012: val_loss did not improve from 6.49947\n",
      "14102/14102 [==============================] - 728s 52ms/sample - loss: 5.6507 - val_loss: 6.5286\n",
      "Epoch 13/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.5132\n",
      "Epoch 00013: val_loss did not improve from 6.49947\n",
      "14102/14102 [==============================] - 726s 52ms/sample - loss: 5.5129 - val_loss: 6.5537\n",
      "Epoch 14/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.4352\n",
      "Epoch 00014: val_loss improved from 6.49947 to 6.45779, saving model to weights/lstm_w2v.hdf5\n",
      "14102/14102 [==============================] - 727s 52ms/sample - loss: 5.4363 - val_loss: 6.4578\n",
      "Epoch 15/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3946\n",
      "Epoch 00015: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 720s 51ms/sample - loss: 5.3937 - val_loss: 6.5470\n",
      "Epoch 16/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3637\n",
      "Epoch 00016: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 724s 51ms/sample - loss: 5.3635 - val_loss: 6.4681\n",
      "Epoch 17/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3942\n",
      "Epoch 00017: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 725s 51ms/sample - loss: 5.3945 - val_loss: 6.5377\n",
      "Epoch 18/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3056\n",
      "Epoch 00018: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 722s 51ms/sample - loss: 5.3049 - val_loss: 6.5073\n",
      "Epoch 19/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.3621\n",
      "Epoch 00019: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 719s 51ms/sample - loss: 5.3623 - val_loss: 6.5104\n",
      "Epoch 20/20\n",
      "14080/14102 [============================>.] - ETA: 1s - loss: 5.2779\n",
      "Epoch 00020: val_loss did not improve from 6.45779\n",
      "14102/14102 [==============================] - 723s 51ms/sample - loss: 5.2779 - val_loss: 6.5250\n"
     ]
    }
   ],
   "source": [
    "lstm_w2v_loss = lstm_w2v.fit(x, y, validation_split=0.2, batch_size=64, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using top k sampling\n",
    "def sample(preds, top_k):\n",
    "    \n",
    "    top_ids = preds.argsort()[-top_k:][::-1]\n",
    "    next_id = top_ids[random.sample(range(top_k),1)[0]]\n",
    "    \n",
    "    return next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=gru, prompt='In this paper', n=20, top_k=10):\n",
    "    \n",
    "    word_ids = [word_to_id(word) for word in prompt.lower().split()]\n",
    "    \n",
    "    for i in range(n):\n",
    "        prediction = model.predict(x=np.array(word_ids))\n",
    "        id = sample(prediction[-1], top_k)\n",
    "        word_ids.append(id)\n",
    "        \n",
    "    words = [id_to_word(w) for w in word_ids]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper umuteam harness oupoco succession coce raisonnement vis refuse videmment sentes srivastava airbus verypluming edit neuralclassifier originating bistparser subsective overestimated natifs'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper de that to the the we a and a as a that of we we the the that of a internal a of a to we of the a the describe learning order ner has used l order events e general correlations s it human a of the a we'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we present a novel approach efforts f annotated four approaches present mechanism apply bert all accuracy identification real generate century long weather unlike and detection'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_w2v, prompt='In this paper we present a novel approach', n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {gru_loss: 'GRU', gru_w2v_loss: 'GRU + Word2Vec', lstm_loss: 'LSTM', lstm_w2v_loss: 'LSTM + Word2Vec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimimum validation loss within a set num of epochs\n",
    "def min_val_loss(model, max_epochs=50):\n",
    "    return min(model.history['val_loss'][:max_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss for GRU: 4.25001\n",
      "Perplexity for model GRU: 70.11\n",
      "\n",
      "Minimum validation loss for GRU + Word2Vec: 2.66512\n",
      "Perplexity for model GRU + Word2Vec: 14.37\n",
      "\n",
      "Minimum validation loss for LSTM: 6.53958\n",
      "Perplexity for model LSTM: 692.00\n",
      "\n",
      "Minimum validation loss for LSTM + Word2Vec: 6.45779\n",
      "Perplexity for model LSTM + Word2Vec: 637.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models.keys():\n",
    "    print(\"Minimum validation loss for {}: {:.5f}\".format(models[m], min_val_loss(m)))\n",
    "    print(\"Perplexity for model {}: {:.2f}\\n\".format(models[m], math.exp(min_val_loss(m))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
