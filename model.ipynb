{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized.txt','r') as f:\n",
    "    tokenized = eval(f.read())\n",
    "    \n",
    "with open('data/tokens.txt','r') as f:\n",
    "    tokens = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f210f4c1-b171-4e41-be01-ee4dd311b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.load('data/x.npy')\n",
    "y_arr = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        x = arr_x[pos:n]\n",
    "        y = arr_y[pos:n]\n",
    "        pos = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4ae5c6-cf7a-4286-ab02-9700f0eb6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17862, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fa4c61-c375-42d9-8c23-a6a39478fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = w2v_model.wv.vectors.shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbd194-1eee-4b01-abdd-5de52fc08690",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5485 unique tokens: ['across', 'all', 'annotation', 'arabic', 'baselines']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1a2d-98a7-45c5-8dd6-7851aa323e51",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b2143a-22ba-4535-801d-37fb06ec47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "batch_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffbc817-9458-491f-a8d1-ee7d9fabb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef742f30-a219-408b-a39a-bc77dc2e8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.862*\"e\" + 0.293*\"de\" + 0.163*\"d\" + 0.125*\"les\" + 0.122*\"la\" + 0.121*\"des\" + 0.108*\"l\" + 0.095*\"et\" + 0.087*\"s\" + 0.081*\"le\"'),\n",
       " (1,\n",
       "  '0.177*\"word\" + 0.151*\"text\" + 0.137*\"learning\" + 0.135*\"using\" + 0.134*\"information\" + 0.132*\"it\" + 0.130*\"performance\" + 0.126*\"tasks\" + 0.122*\"training\" + 0.121*\"or\"'),\n",
       " (2,\n",
       "  '0.777*\"word\" + 0.237*\"embeddings\" + 0.202*\"words\" + -0.142*\"text\" + 0.107*\"languages\" + -0.100*\"domain\" + -0.096*\"knowledge\" + -0.094*\"dataset\" + -0.091*\"question\" + -0.083*\"training\"'),\n",
       " (3,\n",
       "  '-0.663*\"translation\" + -0.271*\"english\" + -0.270*\"machine\" + -0.168*\"nmt\" + -0.158*\"languages\" + 0.126*\"information\" + -0.118*\"parallel\" + -0.117*\"mt\" + 0.110*\"knowledge\" + -0.096*\"source\"'),\n",
       " (4,\n",
       "  '0.305*\"corpus\" + -0.185*\"neural\" + -0.183*\"training\" + 0.180*\"languages\" + -0.162*\"translation\" + 0.154*\"speech\" + -0.152*\"tasks\" + -0.144*\"propose\" + 0.135*\"system\" + -0.134*\"sentence\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b70180a-f67f-4092-9569-40997bf0b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('translation', -0.663295596485134),\n",
       " ('english', -0.2709134713390895),\n",
       " ('machine', -0.27049498939311634),\n",
       " ('nmt', -0.16822649912604723),\n",
       " ('languages', -0.15800620384957956),\n",
       " ('information', 0.12646583717818807),\n",
       " ('parallel', -0.11846985826794525),\n",
       " ('mt', -0.11745536996348017),\n",
       " ('knowledge', 0.1103081521205139),\n",
       " ('source', -0.0955311876535538)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d2d4cc-f09c-4096-9c53-1e6d7d05eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(lsi.projection.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62faf-ac59-4f61-87a7-bf2de09c82cb",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fd01de-ac01-43ef-a370-6bd234108e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics = PCA(n_components=n_layers*batch_size, svd_solver='full').fit_transform(trans_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d50757e-d8ca-482a-88b7-99636cf4f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1570d-87e9-4581-9f83-3c9b54956822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab6422-e377-4938-8384-5dca63116fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385453f3-8dc1-46e1-b9c2-d37088ab4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionedLSTM(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm = ConditionedLSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e0f1-f4e5-46cf-afed-4f9941b79994",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Condtiioned LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b0329-f90a-4121-9308-931b5c2b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondLSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "cond_lstm_w2v = CondLSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e7eba-ecc4-4c76-b121-7b59b496bbdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4497a4-357f-47eb-9466-5e3001770e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        batch = 0\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_arr, y_arr, batch_size):\n",
    "            batch += 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()    \n",
    "            \n",
    "            \n",
    "            # show epoch - batch - loss every n batches\n",
    "            if batch % print_every == 0:\n",
    "                \n",
    "                tot_batches = int(x_arr.shape[0] / batch_size)\n",
    "            \n",
    "                print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                      \"Batch: {}/{} -\".format(batch, tot_batches),\n",
    "                      \"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "path = 'weights/cond_lstm.pt'\n",
    "loss = 0.2\n",
    "\n",
    "optimizer = torch.optim.Adam(cond_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': cond_lstm.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83fc79af-27cf-4951-9d4d-6f47b15a8e3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 5/91 - Loss: 6.9388556480407715\n",
      "Epoch: 1/20 - Batch: 10/91 - Loss: 6.793819427490234\n",
      "Epoch: 1/20 - Batch: 15/91 - Loss: 6.919086933135986\n",
      "Epoch: 1/20 - Batch: 20/91 - Loss: 6.97359561920166\n",
      "Epoch: 1/20 - Batch: 25/91 - Loss: 6.977308750152588\n",
      "Epoch: 1/20 - Batch: 30/91 - Loss: 6.9340291023254395\n",
      "Epoch: 1/20 - Batch: 35/91 - Loss: 6.987171649932861\n",
      "Epoch: 1/20 - Batch: 40/91 - Loss: 6.788027763366699\n",
      "Epoch: 1/20 - Batch: 45/91 - Loss: 6.8932647705078125\n",
      "Epoch: 1/20 - Batch: 50/91 - Loss: 6.909572601318359\n",
      "Epoch: 1/20 - Batch: 55/91 - Loss: 6.885960578918457\n",
      "Epoch: 1/20 - Batch: 60/91 - Loss: 6.820892810821533\n",
      "Epoch: 1/20 - Batch: 65/91 - Loss: 6.857855319976807\n",
      "Epoch: 1/20 - Batch: 70/91 - Loss: 6.941005706787109\n",
      "Epoch: 1/20 - Batch: 75/91 - Loss: 7.204423427581787\n",
      "Epoch: 1/20 - Batch: 80/91 - Loss: 6.836722373962402\n",
      "Epoch: 1/20 - Batch: 85/91 - Loss: 6.921016216278076\n",
      "Epoch: 1/20 - Batch: 90/91 - Loss: 6.750144004821777\n",
      "Epoch: 1/20 - Loss: 8.968854904174805\n",
      "Epoch: 2/20 - Batch: 5/91 - Loss: 6.916810512542725\n",
      "Epoch: 2/20 - Batch: 10/91 - Loss: 6.7298665046691895\n",
      "Epoch: 2/20 - Batch: 15/91 - Loss: 6.804832935333252\n",
      "Epoch: 2/20 - Batch: 20/91 - Loss: 6.83640193939209\n",
      "Epoch: 2/20 - Batch: 25/91 - Loss: 6.8094801902771\n",
      "Epoch: 2/20 - Batch: 30/91 - Loss: 6.760684013366699\n",
      "Epoch: 2/20 - Batch: 35/91 - Loss: 6.803859710693359\n",
      "Epoch: 2/20 - Batch: 40/91 - Loss: 6.6032209396362305\n",
      "Epoch: 2/20 - Batch: 45/91 - Loss: 6.6917548179626465\n",
      "Epoch: 2/20 - Batch: 50/91 - Loss: 6.688358306884766\n",
      "Epoch: 2/20 - Batch: 55/91 - Loss: 6.658660888671875\n",
      "Epoch: 2/20 - Batch: 60/91 - Loss: 6.584736347198486\n",
      "Epoch: 2/20 - Batch: 65/91 - Loss: 6.597686767578125\n",
      "Epoch: 2/20 - Batch: 70/91 - Loss: 6.665835380554199\n",
      "Epoch: 2/20 - Batch: 75/91 - Loss: 6.871377468109131\n",
      "Epoch: 2/20 - Batch: 80/91 - Loss: 6.525944232940674\n",
      "Epoch: 2/20 - Batch: 85/91 - Loss: 6.650882720947266\n",
      "Epoch: 2/20 - Batch: 90/91 - Loss: 6.4057841300964355\n",
      "Epoch: 2/20 - Loss: 8.056407928466797\n",
      "Epoch: 3/20 - Batch: 5/91 - Loss: 6.697854995727539\n",
      "Epoch: 3/20 - Batch: 10/91 - Loss: 6.4802961349487305\n",
      "Epoch: 3/20 - Batch: 15/91 - Loss: 6.574042797088623\n",
      "Epoch: 3/20 - Batch: 20/91 - Loss: 6.577566623687744\n",
      "Epoch: 3/20 - Batch: 25/91 - Loss: 6.553137302398682\n",
      "Epoch: 3/20 - Batch: 30/91 - Loss: 6.480745792388916\n",
      "Epoch: 3/20 - Batch: 35/91 - Loss: 6.529702186584473\n",
      "Epoch: 3/20 - Batch: 40/91 - Loss: 6.297967910766602\n",
      "Epoch: 3/20 - Batch: 45/91 - Loss: 6.431939601898193\n",
      "Epoch: 3/20 - Batch: 50/91 - Loss: 6.413480758666992\n",
      "Epoch: 3/20 - Batch: 55/91 - Loss: 6.384863376617432\n",
      "Epoch: 3/20 - Batch: 60/91 - Loss: 6.303399085998535\n",
      "Epoch: 3/20 - Batch: 65/91 - Loss: 6.334060192108154\n",
      "Epoch: 3/20 - Batch: 70/91 - Loss: 6.402451038360596\n",
      "Epoch: 3/20 - Batch: 75/91 - Loss: 6.455230236053467\n",
      "Epoch: 3/20 - Batch: 80/91 - Loss: 6.271562099456787\n",
      "Epoch: 3/20 - Batch: 85/91 - Loss: 6.426577568054199\n",
      "Epoch: 3/20 - Batch: 90/91 - Loss: 6.166189193725586\n",
      "Epoch: 3/20 - Loss: 6.805059432983398\n",
      "Epoch: 4/20 - Batch: 5/91 - Loss: 6.498696327209473\n",
      "Epoch: 4/20 - Batch: 10/91 - Loss: 6.301069736480713\n",
      "Epoch: 4/20 - Batch: 15/91 - Loss: 6.4066996574401855\n",
      "Epoch: 4/20 - Batch: 20/91 - Loss: 6.411835193634033\n",
      "Epoch: 4/20 - Batch: 25/91 - Loss: 6.391860485076904\n",
      "Epoch: 4/20 - Batch: 30/91 - Loss: 6.290008544921875\n",
      "Epoch: 4/20 - Batch: 35/91 - Loss: 6.350968837738037\n",
      "Epoch: 4/20 - Batch: 40/91 - Loss: 6.09243106842041\n",
      "Epoch: 4/20 - Batch: 45/91 - Loss: 6.251009941101074\n",
      "Epoch: 4/20 - Batch: 50/91 - Loss: 6.233267784118652\n",
      "Epoch: 4/20 - Batch: 55/91 - Loss: 6.204968452453613\n",
      "Epoch: 4/20 - Batch: 60/91 - Loss: 6.1202802658081055\n",
      "Epoch: 4/20 - Batch: 65/91 - Loss: 6.154543876647949\n",
      "Epoch: 4/20 - Batch: 70/91 - Loss: 6.21803092956543\n",
      "Epoch: 4/20 - Batch: 75/91 - Loss: 6.186966419219971\n",
      "Epoch: 4/20 - Batch: 80/91 - Loss: 6.0927019119262695\n",
      "Epoch: 4/20 - Batch: 85/91 - Loss: 6.2740159034729\n",
      "Epoch: 4/20 - Batch: 90/91 - Loss: 5.999598503112793\n",
      "Epoch: 4/20 - Loss: 6.3058085441589355\n",
      "Epoch: 5/20 - Batch: 5/91 - Loss: 6.3481855392456055\n",
      "Epoch: 5/20 - Batch: 10/91 - Loss: 6.163417816162109\n",
      "Epoch: 5/20 - Batch: 15/91 - Loss: 6.2717204093933105\n",
      "Epoch: 5/20 - Batch: 20/91 - Loss: 6.282658100128174\n",
      "Epoch: 5/20 - Batch: 25/91 - Loss: 6.259768009185791\n",
      "Epoch: 5/20 - Batch: 30/91 - Loss: 6.138242244720459\n",
      "Epoch: 5/20 - Batch: 35/91 - Loss: 6.1925435066223145\n",
      "Epoch: 5/20 - Batch: 40/91 - Loss: 5.925911903381348\n",
      "Epoch: 5/20 - Batch: 45/91 - Loss: 6.113986015319824\n",
      "Epoch: 5/20 - Batch: 50/91 - Loss: 6.0861430168151855\n",
      "Epoch: 5/20 - Batch: 55/91 - Loss: 6.04770040512085\n",
      "Epoch: 5/20 - Batch: 60/91 - Loss: 5.974649429321289\n",
      "Epoch: 5/20 - Batch: 65/91 - Loss: 5.991557598114014\n",
      "Epoch: 5/20 - Batch: 70/91 - Loss: 6.065046787261963\n",
      "Epoch: 5/20 - Batch: 75/91 - Loss: 6.022614002227783\n",
      "Epoch: 5/20 - Batch: 80/91 - Loss: 5.9326605796813965\n",
      "Epoch: 5/20 - Batch: 85/91 - Loss: 6.138369083404541\n",
      "Epoch: 5/20 - Batch: 90/91 - Loss: 5.852585315704346\n",
      "Epoch: 5/20 - Loss: 6.128159046173096\n",
      "Epoch: 6/20 - Batch: 5/91 - Loss: 6.211091995239258\n",
      "Epoch: 6/20 - Batch: 10/91 - Loss: 6.029568672180176\n",
      "Epoch: 6/20 - Batch: 15/91 - Loss: 6.141303539276123\n",
      "Epoch: 6/20 - Batch: 20/91 - Loss: 6.15155029296875\n",
      "Epoch: 6/20 - Batch: 25/91 - Loss: 6.1262006759643555\n",
      "Epoch: 6/20 - Batch: 30/91 - Loss: 6.008187770843506\n",
      "Epoch: 6/20 - Batch: 35/91 - Loss: 6.056237697601318\n",
      "Epoch: 6/20 - Batch: 40/91 - Loss: 5.79050350189209\n",
      "Epoch: 6/20 - Batch: 45/91 - Loss: 5.975820541381836\n",
      "Epoch: 6/20 - Batch: 50/91 - Loss: 5.9477386474609375\n",
      "Epoch: 6/20 - Batch: 55/91 - Loss: 5.923235893249512\n",
      "Epoch: 6/20 - Batch: 60/91 - Loss: 5.837225437164307\n",
      "Epoch: 6/20 - Batch: 65/91 - Loss: 5.859252452850342\n",
      "Epoch: 6/20 - Batch: 70/91 - Loss: 5.933681488037109\n",
      "Epoch: 6/20 - Batch: 75/91 - Loss: 5.900040149688721\n",
      "Epoch: 6/20 - Batch: 80/91 - Loss: 5.8152689933776855\n",
      "Epoch: 6/20 - Batch: 85/91 - Loss: 6.0081305503845215\n",
      "Epoch: 6/20 - Batch: 90/91 - Loss: 5.723804473876953\n",
      "Epoch: 6/20 - Loss: 6.029361724853516\n",
      "Epoch: 7/20 - Batch: 5/91 - Loss: 6.086528778076172\n",
      "Epoch: 7/20 - Batch: 10/91 - Loss: 5.908094882965088\n",
      "Epoch: 7/20 - Batch: 15/91 - Loss: 6.0359272956848145\n",
      "Epoch: 7/20 - Batch: 20/91 - Loss: 6.047126293182373\n",
      "Epoch: 7/20 - Batch: 25/91 - Loss: 6.018872261047363\n",
      "Epoch: 7/20 - Batch: 30/91 - Loss: 5.893703460693359\n",
      "Epoch: 7/20 - Batch: 35/91 - Loss: 5.94539737701416\n",
      "Epoch: 7/20 - Batch: 40/91 - Loss: 5.676285266876221\n",
      "Epoch: 7/20 - Batch: 45/91 - Loss: 5.877707004547119\n",
      "Epoch: 7/20 - Batch: 50/91 - Loss: 5.841914653778076\n",
      "Epoch: 7/20 - Batch: 55/91 - Loss: 5.8118109703063965\n",
      "Epoch: 7/20 - Batch: 60/91 - Loss: 5.708868503570557\n",
      "Epoch: 7/20 - Batch: 65/91 - Loss: 5.744129180908203\n",
      "Epoch: 7/20 - Batch: 70/91 - Loss: 5.827404022216797\n",
      "Epoch: 7/20 - Batch: 75/91 - Loss: 5.802996635437012\n",
      "Epoch: 7/20 - Batch: 80/91 - Loss: 5.7108869552612305\n",
      "Epoch: 7/20 - Batch: 85/91 - Loss: 5.926377773284912\n",
      "Epoch: 7/20 - Batch: 90/91 - Loss: 5.615504741668701\n",
      "Epoch: 7/20 - Loss: 5.936750411987305\n",
      "Epoch: 8/20 - Batch: 5/91 - Loss: 5.978621006011963\n",
      "Epoch: 8/20 - Batch: 10/91 - Loss: 5.7987380027771\n",
      "Epoch: 8/20 - Batch: 15/91 - Loss: 5.941072940826416\n",
      "Epoch: 8/20 - Batch: 20/91 - Loss: 5.942163467407227\n",
      "Epoch: 8/20 - Batch: 25/91 - Loss: 5.918395519256592\n",
      "Epoch: 8/20 - Batch: 30/91 - Loss: 5.795186996459961\n",
      "Epoch: 8/20 - Batch: 35/91 - Loss: 5.849644660949707\n",
      "Epoch: 8/20 - Batch: 40/91 - Loss: 5.580722808837891\n",
      "Epoch: 8/20 - Batch: 45/91 - Loss: 5.7763671875\n",
      "Epoch: 8/20 - Batch: 50/91 - Loss: 5.746387481689453\n",
      "Epoch: 8/20 - Batch: 55/91 - Loss: 5.712108135223389\n",
      "Epoch: 8/20 - Batch: 60/91 - Loss: 5.6157307624816895\n",
      "Epoch: 8/20 - Batch: 65/91 - Loss: 5.652227401733398\n",
      "Epoch: 8/20 - Batch: 70/91 - Loss: 5.725949287414551\n",
      "Epoch: 8/20 - Batch: 75/91 - Loss: 5.6903977394104\n",
      "Epoch: 8/20 - Batch: 80/91 - Loss: 5.620977401733398\n",
      "Epoch: 8/20 - Batch: 85/91 - Loss: 5.8415093421936035\n",
      "Epoch: 8/20 - Batch: 90/91 - Loss: 5.519176006317139\n",
      "Epoch: 8/20 - Loss: 5.84958028793335\n",
      "Epoch: 9/20 - Batch: 5/91 - Loss: 5.889289379119873\n",
      "Epoch: 9/20 - Batch: 10/91 - Loss: 5.700403213500977\n",
      "Epoch: 9/20 - Batch: 15/91 - Loss: 5.8501811027526855\n",
      "Epoch: 9/20 - Batch: 20/91 - Loss: 5.851848602294922\n",
      "Epoch: 9/20 - Batch: 25/91 - Loss: 5.841577529907227\n",
      "Epoch: 9/20 - Batch: 30/91 - Loss: 5.704185962677002\n",
      "Epoch: 9/20 - Batch: 35/91 - Loss: 5.76877498626709\n",
      "Epoch: 9/20 - Batch: 40/91 - Loss: 5.499429702758789\n",
      "Epoch: 9/20 - Batch: 45/91 - Loss: 5.689318656921387\n",
      "Epoch: 9/20 - Batch: 50/91 - Loss: 5.66310977935791\n",
      "Epoch: 9/20 - Batch: 55/91 - Loss: 5.6359710693359375\n",
      "Epoch: 9/20 - Batch: 60/91 - Loss: 5.526986598968506\n",
      "Epoch: 9/20 - Batch: 65/91 - Loss: 5.567836284637451\n",
      "Epoch: 9/20 - Batch: 70/91 - Loss: 5.646745204925537\n",
      "Epoch: 9/20 - Batch: 75/91 - Loss: 5.615022659301758\n",
      "Epoch: 9/20 - Batch: 80/91 - Loss: 5.5501708984375\n",
      "Epoch: 9/20 - Batch: 85/91 - Loss: 5.751831531524658\n",
      "Epoch: 9/20 - Batch: 90/91 - Loss: 5.4485554695129395\n",
      "Epoch: 9/20 - Loss: 5.74801778793335\n",
      "Epoch: 10/20 - Batch: 5/91 - Loss: 5.7953033447265625\n",
      "Epoch: 10/20 - Batch: 10/91 - Loss: 5.6241455078125\n",
      "Epoch: 10/20 - Batch: 15/91 - Loss: 5.784963607788086\n",
      "Epoch: 10/20 - Batch: 20/91 - Loss: 5.778547763824463\n",
      "Epoch: 10/20 - Batch: 25/91 - Loss: 5.769864082336426\n",
      "Epoch: 10/20 - Batch: 30/91 - Loss: 5.626748561859131\n",
      "Epoch: 10/20 - Batch: 35/91 - Loss: 5.686695575714111\n",
      "Epoch: 10/20 - Batch: 40/91 - Loss: 5.430247783660889\n",
      "Epoch: 10/20 - Batch: 45/91 - Loss: 5.622986316680908\n",
      "Epoch: 10/20 - Batch: 50/91 - Loss: 5.592957973480225\n",
      "Epoch: 10/20 - Batch: 55/91 - Loss: 5.567940711975098\n",
      "Epoch: 10/20 - Batch: 60/91 - Loss: 5.456051826477051\n",
      "Epoch: 10/20 - Batch: 65/91 - Loss: 5.502327919006348\n",
      "Epoch: 10/20 - Batch: 70/91 - Loss: 5.5666093826293945\n",
      "Epoch: 10/20 - Batch: 75/91 - Loss: 5.55241584777832\n",
      "Epoch: 10/20 - Batch: 80/91 - Loss: 5.470061779022217\n",
      "Epoch: 10/20 - Batch: 85/91 - Loss: 5.6906256675720215\n",
      "Epoch: 10/20 - Batch: 90/91 - Loss: 5.388742446899414\n",
      "Epoch: 10/20 - Loss: 5.664587020874023\n",
      "Epoch: 11/20 - Batch: 5/91 - Loss: 5.7225799560546875\n",
      "Epoch: 11/20 - Batch: 10/91 - Loss: 5.546524524688721\n",
      "Epoch: 11/20 - Batch: 15/91 - Loss: 5.727779388427734\n",
      "Epoch: 11/20 - Batch: 20/91 - Loss: 5.712298393249512\n",
      "Epoch: 11/20 - Batch: 25/91 - Loss: 5.711348056793213\n",
      "Epoch: 11/20 - Batch: 30/91 - Loss: 5.575890064239502\n",
      "Epoch: 11/20 - Batch: 35/91 - Loss: 5.6213698387146\n",
      "Epoch: 11/20 - Batch: 40/91 - Loss: 5.361062526702881\n",
      "Epoch: 11/20 - Batch: 45/91 - Loss: 5.5459089279174805\n",
      "Epoch: 11/20 - Batch: 50/91 - Loss: 5.532517910003662\n",
      "Epoch: 11/20 - Batch: 55/91 - Loss: 5.491576671600342\n",
      "Epoch: 11/20 - Batch: 60/91 - Loss: 5.387521266937256\n",
      "Epoch: 11/20 - Batch: 65/91 - Loss: 5.43339729309082\n",
      "Epoch: 11/20 - Batch: 70/91 - Loss: 5.495549201965332\n",
      "Epoch: 11/20 - Batch: 75/91 - Loss: 5.489816665649414\n",
      "Epoch: 11/20 - Batch: 80/91 - Loss: 5.420867919921875\n",
      "Epoch: 11/20 - Batch: 85/91 - Loss: 5.629724502563477\n",
      "Epoch: 11/20 - Batch: 90/91 - Loss: 5.325002670288086\n",
      "Epoch: 11/20 - Loss: 5.587871074676514\n",
      "Epoch: 12/20 - Batch: 5/91 - Loss: 5.668208122253418\n",
      "Epoch: 12/20 - Batch: 10/91 - Loss: 5.480096817016602\n",
      "Epoch: 12/20 - Batch: 15/91 - Loss: 5.666726112365723\n",
      "Epoch: 12/20 - Batch: 20/91 - Loss: 5.6403656005859375\n",
      "Epoch: 12/20 - Batch: 25/91 - Loss: 5.644280433654785\n",
      "Epoch: 12/20 - Batch: 30/91 - Loss: 5.504700183868408\n",
      "Epoch: 12/20 - Batch: 35/91 - Loss: 5.5596723556518555\n",
      "Epoch: 12/20 - Batch: 40/91 - Loss: 5.311052322387695\n",
      "Epoch: 12/20 - Batch: 45/91 - Loss: 5.477169990539551\n",
      "Epoch: 12/20 - Batch: 50/91 - Loss: 5.480009078979492\n",
      "Epoch: 12/20 - Batch: 55/91 - Loss: 5.440153121948242\n",
      "Epoch: 12/20 - Batch: 60/91 - Loss: 5.3228678703308105\n",
      "Epoch: 12/20 - Batch: 65/91 - Loss: 5.375935077667236\n",
      "Epoch: 12/20 - Batch: 70/91 - Loss: 5.4437737464904785\n",
      "Epoch: 12/20 - Batch: 75/91 - Loss: 5.431304454803467\n",
      "Epoch: 12/20 - Batch: 80/91 - Loss: 5.355606555938721\n",
      "Epoch: 12/20 - Batch: 85/91 - Loss: 5.576082706451416\n",
      "Epoch: 12/20 - Batch: 90/91 - Loss: 5.282935619354248\n",
      "Epoch: 12/20 - Loss: 5.504929065704346\n",
      "Epoch: 13/20 - Batch: 5/91 - Loss: 5.616561412811279\n",
      "Epoch: 13/20 - Batch: 10/91 - Loss: 5.412291049957275\n",
      "Epoch: 13/20 - Batch: 15/91 - Loss: 5.582907676696777\n",
      "Epoch: 13/20 - Batch: 20/91 - Loss: 5.596775531768799\n",
      "Epoch: 13/20 - Batch: 25/91 - Loss: 5.581961631774902\n",
      "Epoch: 13/20 - Batch: 30/91 - Loss: 5.443780422210693\n",
      "Epoch: 13/20 - Batch: 35/91 - Loss: 5.492527008056641\n",
      "Epoch: 13/20 - Batch: 40/91 - Loss: 5.250143527984619\n",
      "Epoch: 13/20 - Batch: 45/91 - Loss: 5.421543598175049\n",
      "Epoch: 13/20 - Batch: 50/91 - Loss: 5.429968357086182\n",
      "Epoch: 13/20 - Batch: 55/91 - Loss: 5.37785530090332\n",
      "Epoch: 13/20 - Batch: 60/91 - Loss: 5.261600494384766\n",
      "Epoch: 13/20 - Batch: 65/91 - Loss: 5.320216178894043\n",
      "Epoch: 13/20 - Batch: 70/91 - Loss: 5.390079498291016\n",
      "Epoch: 13/20 - Batch: 75/91 - Loss: 5.366877555847168\n",
      "Epoch: 13/20 - Batch: 80/91 - Loss: 5.295268535614014\n",
      "Epoch: 13/20 - Batch: 85/91 - Loss: 5.523900032043457\n",
      "Epoch: 13/20 - Batch: 90/91 - Loss: 5.223871231079102\n",
      "Epoch: 13/20 - Loss: 5.4195356369018555\n",
      "Epoch: 14/20 - Batch: 5/91 - Loss: 5.538790225982666\n",
      "Epoch: 14/20 - Batch: 10/91 - Loss: 5.3537163734436035\n",
      "Epoch: 14/20 - Batch: 15/91 - Loss: 5.524409770965576\n",
      "Epoch: 14/20 - Batch: 20/91 - Loss: 5.544532299041748\n",
      "Epoch: 14/20 - Batch: 25/91 - Loss: 5.532651424407959\n",
      "Epoch: 14/20 - Batch: 30/91 - Loss: 5.399326801300049\n",
      "Epoch: 14/20 - Batch: 35/91 - Loss: 5.449724197387695\n",
      "Epoch: 14/20 - Batch: 40/91 - Loss: 5.214615345001221\n",
      "Epoch: 14/20 - Batch: 45/91 - Loss: 5.379187107086182\n",
      "Epoch: 14/20 - Batch: 50/91 - Loss: 5.378187656402588\n",
      "Epoch: 14/20 - Batch: 55/91 - Loss: 5.321611404418945\n",
      "Epoch: 14/20 - Batch: 60/91 - Loss: 5.216937065124512\n",
      "Epoch: 14/20 - Batch: 65/91 - Loss: 5.279654502868652\n",
      "Epoch: 14/20 - Batch: 70/91 - Loss: 5.326684474945068\n",
      "Epoch: 14/20 - Batch: 75/91 - Loss: 5.352461338043213\n",
      "Epoch: 14/20 - Batch: 80/91 - Loss: 5.243910312652588\n",
      "Epoch: 14/20 - Batch: 85/91 - Loss: 5.4718194007873535\n",
      "Epoch: 14/20 - Batch: 90/91 - Loss: 5.166255474090576\n",
      "Epoch: 14/20 - Loss: 5.370457172393799\n",
      "Epoch: 15/20 - Batch: 5/91 - Loss: 5.482362270355225\n",
      "Epoch: 15/20 - Batch: 10/91 - Loss: 5.3111252784729\n",
      "Epoch: 15/20 - Batch: 15/91 - Loss: 5.493406295776367\n",
      "Epoch: 15/20 - Batch: 20/91 - Loss: 5.4987382888793945\n",
      "Epoch: 15/20 - Batch: 25/91 - Loss: 5.485843658447266\n",
      "Epoch: 15/20 - Batch: 30/91 - Loss: 5.367259502410889\n",
      "Epoch: 15/20 - Batch: 35/91 - Loss: 5.407419204711914\n",
      "Epoch: 15/20 - Batch: 40/91 - Loss: 5.167928695678711\n",
      "Epoch: 15/20 - Batch: 45/91 - Loss: 5.324265956878662\n",
      "Epoch: 15/20 - Batch: 50/91 - Loss: 5.3281683921813965\n",
      "Epoch: 15/20 - Batch: 55/91 - Loss: 5.2908220291137695\n",
      "Epoch: 15/20 - Batch: 60/91 - Loss: 5.165217876434326\n",
      "Epoch: 15/20 - Batch: 65/91 - Loss: 5.217023849487305\n",
      "Epoch: 15/20 - Batch: 70/91 - Loss: 5.3036980628967285\n",
      "Epoch: 15/20 - Batch: 75/91 - Loss: 5.2765374183654785\n",
      "Epoch: 15/20 - Batch: 80/91 - Loss: 5.202096939086914\n",
      "Epoch: 15/20 - Batch: 85/91 - Loss: 5.409142971038818\n",
      "Epoch: 15/20 - Batch: 90/91 - Loss: 5.113702297210693\n",
      "Epoch: 15/20 - Loss: 5.267731189727783\n",
      "Epoch: 16/20 - Batch: 5/91 - Loss: 5.434053421020508\n",
      "Epoch: 16/20 - Batch: 10/91 - Loss: 5.261103630065918\n",
      "Epoch: 16/20 - Batch: 15/91 - Loss: 5.437353134155273\n",
      "Epoch: 16/20 - Batch: 20/91 - Loss: 5.4493632316589355\n",
      "Epoch: 16/20 - Batch: 25/91 - Loss: 5.428070068359375\n",
      "Epoch: 16/20 - Batch: 30/91 - Loss: 5.32395076751709\n",
      "Epoch: 16/20 - Batch: 35/91 - Loss: 5.35568904876709\n",
      "Epoch: 16/20 - Batch: 40/91 - Loss: 5.110265254974365\n",
      "Epoch: 16/20 - Batch: 45/91 - Loss: 5.2889628410339355\n",
      "Epoch: 16/20 - Batch: 50/91 - Loss: 5.293436050415039\n",
      "Epoch: 16/20 - Batch: 55/91 - Loss: 5.232367515563965\n",
      "Epoch: 16/20 - Batch: 60/91 - Loss: 5.118844032287598\n",
      "Epoch: 16/20 - Batch: 65/91 - Loss: 5.168831825256348\n",
      "Epoch: 16/20 - Batch: 70/91 - Loss: 5.248486518859863\n",
      "Epoch: 16/20 - Batch: 75/91 - Loss: 5.219595909118652\n",
      "Epoch: 16/20 - Batch: 80/91 - Loss: 5.1455159187316895\n",
      "Epoch: 16/20 - Batch: 85/91 - Loss: 5.3705735206604\n",
      "Epoch: 16/20 - Batch: 90/91 - Loss: 5.079145908355713\n",
      "Epoch: 16/20 - Loss: 5.189324378967285\n",
      "Epoch: 17/20 - Batch: 5/91 - Loss: 5.389629364013672\n",
      "Epoch: 17/20 - Batch: 10/91 - Loss: 5.211549758911133\n",
      "Epoch: 17/20 - Batch: 15/91 - Loss: 5.3799896240234375\n",
      "Epoch: 17/20 - Batch: 20/91 - Loss: 5.407630443572998\n",
      "Epoch: 17/20 - Batch: 25/91 - Loss: 5.383383274078369\n",
      "Epoch: 17/20 - Batch: 30/91 - Loss: 5.282402992248535\n",
      "Epoch: 17/20 - Batch: 35/91 - Loss: 5.309507846832275\n",
      "Epoch: 17/20 - Batch: 40/91 - Loss: 5.0616607666015625\n",
      "Epoch: 17/20 - Batch: 45/91 - Loss: 5.2219133377075195\n",
      "Epoch: 17/20 - Batch: 50/91 - Loss: 5.240207672119141\n",
      "Epoch: 17/20 - Batch: 55/91 - Loss: 5.185606002807617\n",
      "Epoch: 17/20 - Batch: 60/91 - Loss: 5.0667033195495605\n",
      "Epoch: 17/20 - Batch: 65/91 - Loss: 5.1337199211120605\n",
      "Epoch: 17/20 - Batch: 70/91 - Loss: 5.205355644226074\n",
      "Epoch: 17/20 - Batch: 75/91 - Loss: 5.177430629730225\n",
      "Epoch: 17/20 - Batch: 80/91 - Loss: 5.118963241577148\n",
      "Epoch: 17/20 - Batch: 85/91 - Loss: 5.319431781768799\n",
      "Epoch: 17/20 - Batch: 90/91 - Loss: 5.031219959259033\n",
      "Epoch: 17/20 - Loss: 5.104238510131836\n",
      "Epoch: 18/20 - Batch: 5/91 - Loss: 5.351059436798096\n",
      "Epoch: 18/20 - Batch: 10/91 - Loss: 5.1533026695251465\n",
      "Epoch: 18/20 - Batch: 15/91 - Loss: 5.341886043548584\n",
      "Epoch: 18/20 - Batch: 20/91 - Loss: 5.3469061851501465\n",
      "Epoch: 18/20 - Batch: 25/91 - Loss: 5.344160079956055\n",
      "Epoch: 18/20 - Batch: 30/91 - Loss: 5.238080024719238\n",
      "Epoch: 18/20 - Batch: 35/91 - Loss: 5.2601141929626465\n",
      "Epoch: 18/20 - Batch: 40/91 - Loss: 5.032272815704346\n",
      "Epoch: 18/20 - Batch: 45/91 - Loss: 5.173086166381836\n",
      "Epoch: 18/20 - Batch: 50/91 - Loss: 5.195109844207764\n",
      "Epoch: 18/20 - Batch: 55/91 - Loss: 5.146890163421631\n",
      "Epoch: 18/20 - Batch: 60/91 - Loss: 5.031773090362549\n",
      "Epoch: 18/20 - Batch: 65/91 - Loss: 5.092729091644287\n",
      "Epoch: 18/20 - Batch: 70/91 - Loss: 5.164979457855225\n",
      "Epoch: 18/20 - Batch: 75/91 - Loss: 5.146651744842529\n",
      "Epoch: 18/20 - Batch: 80/91 - Loss: 5.063547134399414\n",
      "Epoch: 18/20 - Batch: 85/91 - Loss: 5.292351245880127\n",
      "Epoch: 18/20 - Batch: 90/91 - Loss: 4.987931251525879\n",
      "Epoch: 18/20 - Loss: 5.054783821105957\n",
      "Epoch: 19/20 - Batch: 5/91 - Loss: 5.298576831817627\n",
      "Epoch: 19/20 - Batch: 10/91 - Loss: 5.108013153076172\n",
      "Epoch: 19/20 - Batch: 15/91 - Loss: 5.302079677581787\n",
      "Epoch: 19/20 - Batch: 20/91 - Loss: 5.304325580596924\n",
      "Epoch: 19/20 - Batch: 25/91 - Loss: 5.298227310180664\n",
      "Epoch: 19/20 - Batch: 30/91 - Loss: 5.19659423828125\n",
      "Epoch: 19/20 - Batch: 35/91 - Loss: 5.220881938934326\n",
      "Epoch: 19/20 - Batch: 40/91 - Loss: 4.988711357116699\n",
      "Epoch: 19/20 - Batch: 45/91 - Loss: 5.1311235427856445\n",
      "Epoch: 19/20 - Batch: 50/91 - Loss: 5.14985466003418\n",
      "Epoch: 19/20 - Batch: 55/91 - Loss: 5.109355449676514\n",
      "Epoch: 19/20 - Batch: 60/91 - Loss: 4.999523162841797\n",
      "Epoch: 19/20 - Batch: 65/91 - Loss: 5.0585832595825195\n",
      "Epoch: 19/20 - Batch: 70/91 - Loss: 5.1229352951049805\n",
      "Epoch: 19/20 - Batch: 75/91 - Loss: 5.087787628173828\n",
      "Epoch: 19/20 - Batch: 80/91 - Loss: 5.022121906280518\n",
      "Epoch: 19/20 - Batch: 85/91 - Loss: 5.242668628692627\n",
      "Epoch: 19/20 - Batch: 90/91 - Loss: 4.963929653167725\n",
      "Epoch: 19/20 - Loss: 4.963405132293701\n",
      "Epoch: 20/20 - Batch: 5/91 - Loss: 5.259457588195801\n",
      "Epoch: 20/20 - Batch: 10/91 - Loss: 5.069798469543457\n",
      "Epoch: 20/20 - Batch: 15/91 - Loss: 5.2561564445495605\n",
      "Epoch: 20/20 - Batch: 20/91 - Loss: 5.2787885665893555\n",
      "Epoch: 20/20 - Batch: 25/91 - Loss: 5.252690315246582\n",
      "Epoch: 20/20 - Batch: 30/91 - Loss: 5.163015842437744\n",
      "Epoch: 20/20 - Batch: 35/91 - Loss: 5.189592361450195\n",
      "Epoch: 20/20 - Batch: 40/91 - Loss: 4.94987678527832\n",
      "Epoch: 20/20 - Batch: 45/91 - Loss: 5.086320400238037\n",
      "Epoch: 20/20 - Batch: 50/91 - Loss: 5.135249614715576\n",
      "Epoch: 20/20 - Batch: 55/91 - Loss: 5.066809177398682\n",
      "Epoch: 20/20 - Batch: 60/91 - Loss: 4.957340240478516\n",
      "Epoch: 20/20 - Batch: 65/91 - Loss: 5.014559745788574\n",
      "Epoch: 20/20 - Batch: 70/91 - Loss: 5.1004438400268555\n",
      "Epoch: 20/20 - Batch: 75/91 - Loss: 5.0475850105285645\n",
      "Epoch: 20/20 - Batch: 80/91 - Loss: 4.998193264007568\n",
      "Epoch: 20/20 - Batch: 85/91 - Loss: 5.200498104095459\n",
      "Epoch: 20/20 - Batch: 90/91 - Loss: 4.9247612953186035\n",
      "Epoch: 20/20 - Loss: 4.899267196655273\n"
     ]
    }
   ],
   "source": [
    "train(cond_lstm, batch_size=batch_size, epochs=epochs, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2cfe-fa23-43f0-ac9a-873c02886fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fec25ee-1384-4327-bf8f-2b574865057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(model, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_id(t)]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top n values\n",
    "    top_ids = p.argsort()[-5:][::-1]\n",
    "\n",
    "    # sample id of next word from top n values\n",
    "    next_id = top_ids[random.sample([0,1,2,3,4],1)[0]]\n",
    "\n",
    "    # return the value of the predicted word and the hidden state\n",
    "    return id_to_word(next_id), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for generation batch size\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model=cond_lstm, n=10, prompt='in this paper'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.ones(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    words = prompt.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, t, h)\n",
    "    \n",
    "    words.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(n-1):\n",
    "        token, h = predict(model, words[-1], h)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "645a8dd9-4a9c-4059-8c23-28e753ca0cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper for both languages we present a novel approach in a large number in this study to end trainable and sequence learning we also present two novel approaches for the task of generating an grained and the corpus and a corpus based approach to the best process of our approach is'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9350a124-6cd4-4cef-8261-8ca8aeea2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we use embeddings on three benchmark data we present our results on two benchmark datasets for both language generation tasks and show that our approach can improve competitive results over state in both a small set on natural learning tasks and the lack of a variety to the task and a variety in the first task to predict the most complex information in order we present our system to predict the quality in this work the first study of generating word representations in this study is the first study to predict a corpus for a given corpus for this paper presents our approach'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(n=100, prompt='we use embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec48bd6-4fd3-4d53-9d9a-ed9f4d4c048b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
