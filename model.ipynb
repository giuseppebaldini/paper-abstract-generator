{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized.txt','r') as f:\n",
    "    tokenized = eval(f.read())\n",
    "    \n",
    "with open('data/tokens.txt','r') as f:\n",
    "    tokens = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f210f4c1-b171-4e41-be01-ee4dd311b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.load('data/x.npy')\n",
    "y_arr = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        x = arr_x[pos:n]\n",
    "        y = arr_y[pos:n]\n",
    "        pos = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4ae5c6-cf7a-4286-ab02-9700f0eb6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17862, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fa4c61-c375-42d9-8c23-a6a39478fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = w2v_model.wv.vectors.shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbd194-1eee-4b01-abdd-5de52fc08690",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5485 unique tokens: ['across', 'all', 'annotation', 'arabic', 'baselines']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1a2d-98a7-45c5-8dd6-7851aa323e51",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b2143a-22ba-4535-801d-37fb06ec47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "batch_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffbc817-9458-491f-a8d1-ee7d9fabb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef742f30-a219-408b-a39a-bc77dc2e8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.862*\"e\" + 0.293*\"de\" + 0.163*\"d\" + 0.125*\"les\" + 0.122*\"la\" + 0.121*\"des\" + 0.108*\"l\" + 0.095*\"et\" + 0.087*\"s\" + 0.081*\"le\"'),\n",
       " (1,\n",
       "  '0.177*\"word\" + 0.151*\"text\" + 0.137*\"learning\" + 0.135*\"using\" + 0.134*\"information\" + 0.132*\"it\" + 0.130*\"performance\" + 0.126*\"tasks\" + 0.122*\"training\" + 0.121*\"or\"'),\n",
       " (2,\n",
       "  '-0.777*\"word\" + -0.237*\"embeddings\" + -0.202*\"words\" + 0.142*\"text\" + -0.107*\"languages\" + 0.100*\"domain\" + 0.096*\"knowledge\" + 0.094*\"dataset\" + 0.091*\"question\" + 0.083*\"training\"'),\n",
       " (3,\n",
       "  '0.663*\"translation\" + 0.271*\"english\" + 0.271*\"machine\" + 0.168*\"nmt\" + 0.158*\"languages\" + -0.126*\"information\" + 0.118*\"parallel\" + 0.117*\"mt\" + -0.110*\"knowledge\" + 0.095*\"source\"'),\n",
       " (4,\n",
       "  '-0.305*\"corpus\" + 0.185*\"neural\" + 0.183*\"training\" + -0.180*\"languages\" + 0.162*\"translation\" + -0.154*\"speech\" + 0.152*\"tasks\" + 0.144*\"propose\" + -0.135*\"system\" + 0.134*\"sentence\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b70180a-f67f-4092-9569-40997bf0b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('translation', 0.6633340577028972),\n",
       " ('english', 0.2709217211537034),\n",
       " ('machine', 0.2705053184612935),\n",
       " ('nmt', 0.16817154915592414),\n",
       " ('languages', 0.15796281011927574),\n",
       " ('information', -0.12648445062567976),\n",
       " ('parallel', 0.11846450616842508),\n",
       " ('mt', 0.11744000289966029),\n",
       " ('knowledge', -0.11030224023449138),\n",
       " ('source', 0.09548638796378722)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d2d4cc-f09c-4096-9c53-1e6d7d05eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(lsi.projection.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62faf-ac59-4f61-87a7-bf2de09c82cb",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fd01de-ac01-43ef-a370-6bd234108e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics = PCA(n_components=n_layers*batch_size, svd_solver='full').fit_transform(trans_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d50757e-d8ca-482a-88b7-99636cf4f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1570d-87e9-4581-9f83-3c9b54956822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab6422-e377-4938-8384-5dca63116fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385453f3-8dc1-46e1-b9c2-d37088ab4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionedLSTM(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm = ConditionedLSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e0f1-f4e5-46cf-afed-4f9941b79994",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Condtiioned LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a9b0329-f90a-4121-9308-931b5c2b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondLSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CondLSTM_Word2Vec(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm_w2v = CondLSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e7eba-ecc4-4c76-b121-7b59b496bbdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcf8113d-a604-4d21-89fc-e3505f0a9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be4497a4-357f-47eb-9466-5e3001770e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # loss values\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        batch = 0\n",
    "        \n",
    "        epoch_loss= []\n",
    "        \n",
    "        for x, y in get_batches(x_arr, y_arr, batch_size):\n",
    "            \n",
    "            batch += 1\n",
    "            \n",
    "            # initialize hidden state\n",
    "            h = model.init_hidden(batch_size)\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "            \n",
    "            # add current batch loss to epoch loss list\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()    \n",
    "            \n",
    "            \n",
    "            # show epoch - batch - loss every n batches\n",
    "            if batch % print_every == 0:\n",
    "                \n",
    "                tot_batches = int(x_arr.shape[0] / batch_size)\n",
    "            \n",
    "                print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                      \"Batch: {}/{} -\".format(batch, tot_batches),\n",
    "                      \"Loss: {}\".format(loss))\n",
    "        \n",
    "        # print loss at the end of each epoch\n",
    "        print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "              \"Loss: {}\".format(loss))\n",
    "        \n",
    "        # save average epoch loss\n",
    "        losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "        \n",
    "    my_plot(np.linspace(1, epochs, epochs).astype(int), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "path = 'weights/cond_lstm_w2v.pt'\n",
    "loss = 0.2\n",
    "\n",
    "optimizer = torch.optim.Adam(cond_lstm_w2v.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save checkpoints\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': cond_lstm_w2v.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00300c8f-f9d0-4631-8783-84509fad9716",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 - Batch: 5/91 - Loss: 9.636506080627441\n",
      "Epoch: 1/30 - Batch: 10/91 - Loss: 7.962486743927002\n",
      "Epoch: 1/30 - Batch: 15/91 - Loss: 7.210313320159912\n",
      "Epoch: 1/30 - Batch: 20/91 - Loss: 7.115902900695801\n",
      "Epoch: 1/30 - Batch: 25/91 - Loss: 7.133000373840332\n",
      "Epoch: 1/30 - Batch: 30/91 - Loss: 7.069431781768799\n",
      "Epoch: 1/30 - Batch: 35/91 - Loss: 7.065545082092285\n",
      "Epoch: 1/30 - Batch: 40/91 - Loss: 6.834764003753662\n",
      "Epoch: 1/30 - Batch: 45/91 - Loss: 6.977421283721924\n",
      "Epoch: 1/30 - Batch: 50/91 - Loss: 6.952078342437744\n",
      "Epoch: 1/30 - Batch: 55/91 - Loss: 6.928534030914307\n",
      "Epoch: 1/30 - Batch: 60/91 - Loss: 6.860357761383057\n",
      "Epoch: 1/30 - Batch: 65/91 - Loss: 6.889596462249756\n",
      "Epoch: 1/30 - Batch: 70/91 - Loss: 6.962825298309326\n",
      "Epoch: 1/30 - Batch: 75/91 - Loss: 7.319894790649414\n",
      "Epoch: 1/30 - Batch: 80/91 - Loss: 6.860445499420166\n",
      "Epoch: 1/30 - Batch: 85/91 - Loss: 6.941708564758301\n",
      "Epoch: 1/30 - Batch: 90/91 - Loss: 6.784283638000488\n",
      "Epoch: 1/30 - Loss: 8.893921852111816\n",
      "Epoch: 2/30 - Batch: 5/91 - Loss: 7.158723831176758\n",
      "Epoch: 2/30 - Batch: 10/91 - Loss: 6.914999008178711\n",
      "Epoch: 2/30 - Batch: 15/91 - Loss: 6.9895830154418945\n",
      "Epoch: 2/30 - Batch: 20/91 - Loss: 7.016523361206055\n",
      "Epoch: 2/30 - Batch: 25/91 - Loss: 6.987504959106445\n",
      "Epoch: 2/30 - Batch: 30/91 - Loss: 6.937319278717041\n",
      "Epoch: 2/30 - Batch: 35/91 - Loss: 6.96995735168457\n",
      "Epoch: 2/30 - Batch: 40/91 - Loss: 6.778225898742676\n",
      "Epoch: 2/30 - Batch: 45/91 - Loss: 6.901376247406006\n",
      "Epoch: 2/30 - Batch: 50/91 - Loss: 6.881591796875\n",
      "Epoch: 2/30 - Batch: 55/91 - Loss: 6.883155345916748\n",
      "Epoch: 2/30 - Batch: 60/91 - Loss: 6.822174072265625\n",
      "Epoch: 2/30 - Batch: 65/91 - Loss: 6.841267108917236\n",
      "Epoch: 2/30 - Batch: 70/91 - Loss: 6.9108686447143555\n",
      "Epoch: 2/30 - Batch: 75/91 - Loss: 7.195670127868652\n",
      "Epoch: 2/30 - Batch: 80/91 - Loss: 6.81186056137085\n",
      "Epoch: 2/30 - Batch: 85/91 - Loss: 6.914141654968262\n",
      "Epoch: 2/30 - Batch: 90/91 - Loss: 6.76984977722168\n",
      "Epoch: 2/30 - Loss: 8.543939590454102\n",
      "Epoch: 3/30 - Batch: 5/91 - Loss: 7.1386542320251465\n",
      "Epoch: 3/30 - Batch: 10/91 - Loss: 6.913114547729492\n",
      "Epoch: 3/30 - Batch: 15/91 - Loss: 6.989441394805908\n",
      "Epoch: 3/30 - Batch: 20/91 - Loss: 7.0183587074279785\n",
      "Epoch: 3/30 - Batch: 25/91 - Loss: 6.982641220092773\n",
      "Epoch: 3/30 - Batch: 30/91 - Loss: 6.921533584594727\n",
      "Epoch: 3/30 - Batch: 35/91 - Loss: 6.969137191772461\n",
      "Epoch: 3/30 - Batch: 40/91 - Loss: 6.774384021759033\n",
      "Epoch: 3/30 - Batch: 45/91 - Loss: 6.878522872924805\n",
      "Epoch: 3/30 - Batch: 50/91 - Loss: 6.881545543670654\n",
      "Epoch: 3/30 - Batch: 55/91 - Loss: 6.869897842407227\n",
      "Epoch: 3/30 - Batch: 60/91 - Loss: 6.810626029968262\n",
      "Epoch: 3/30 - Batch: 65/91 - Loss: 6.823603630065918\n",
      "Epoch: 3/30 - Batch: 70/91 - Loss: 6.901769638061523\n",
      "Epoch: 3/30 - Batch: 75/91 - Loss: 7.069299221038818\n",
      "Epoch: 3/30 - Batch: 80/91 - Loss: 6.7975029945373535\n",
      "Epoch: 3/30 - Batch: 85/91 - Loss: 6.882636547088623\n",
      "Epoch: 3/30 - Batch: 90/91 - Loss: 6.7527289390563965\n",
      "Epoch: 3/30 - Loss: 8.001843452453613\n",
      "Epoch: 4/30 - Batch: 5/91 - Loss: 7.10922384262085\n",
      "Epoch: 4/30 - Batch: 10/91 - Loss: 6.893441200256348\n",
      "Epoch: 4/30 - Batch: 15/91 - Loss: 6.962167263031006\n",
      "Epoch: 4/30 - Batch: 20/91 - Loss: 6.998679161071777\n",
      "Epoch: 4/30 - Batch: 25/91 - Loss: 6.956506729125977\n",
      "Epoch: 4/30 - Batch: 30/91 - Loss: 6.911906719207764\n",
      "Epoch: 4/30 - Batch: 35/91 - Loss: 6.9546799659729\n",
      "Epoch: 4/30 - Batch: 40/91 - Loss: 6.767871856689453\n",
      "Epoch: 4/30 - Batch: 45/91 - Loss: 6.858904838562012\n",
      "Epoch: 4/30 - Batch: 50/91 - Loss: 6.87159538269043\n",
      "Epoch: 4/30 - Batch: 55/91 - Loss: 6.857883453369141\n",
      "Epoch: 4/30 - Batch: 60/91 - Loss: 6.801946640014648\n",
      "Epoch: 4/30 - Batch: 65/91 - Loss: 6.8198699951171875\n",
      "Epoch: 4/30 - Batch: 70/91 - Loss: 6.8939995765686035\n",
      "Epoch: 4/30 - Batch: 75/91 - Loss: 6.88148307800293\n",
      "Epoch: 4/30 - Batch: 80/91 - Loss: 6.7883806228637695\n",
      "Epoch: 4/30 - Batch: 85/91 - Loss: 6.883452892303467\n",
      "Epoch: 4/30 - Batch: 90/91 - Loss: 6.737257480621338\n",
      "Epoch: 4/30 - Loss: 7.021652698516846\n",
      "Epoch: 5/30 - Batch: 5/91 - Loss: 7.0908942222595215\n",
      "Epoch: 5/30 - Batch: 10/91 - Loss: 6.877285003662109\n",
      "Epoch: 5/30 - Batch: 15/91 - Loss: 6.954005241394043\n",
      "Epoch: 5/30 - Batch: 20/91 - Loss: 7.008519172668457\n",
      "Epoch: 5/30 - Batch: 25/91 - Loss: 6.956843376159668\n",
      "Epoch: 5/30 - Batch: 30/91 - Loss: 6.902849197387695\n",
      "Epoch: 5/30 - Batch: 35/91 - Loss: 6.941228866577148\n",
      "Epoch: 5/30 - Batch: 40/91 - Loss: 6.756929397583008\n",
      "Epoch: 5/30 - Batch: 45/91 - Loss: 6.856828212738037\n",
      "Epoch: 5/30 - Batch: 50/91 - Loss: 6.857442855834961\n",
      "Epoch: 5/30 - Batch: 55/91 - Loss: 6.853729248046875\n",
      "Epoch: 5/30 - Batch: 60/91 - Loss: 6.7932586669921875\n",
      "Epoch: 5/30 - Batch: 65/91 - Loss: 6.805121421813965\n",
      "Epoch: 5/30 - Batch: 70/91 - Loss: 6.873600959777832\n",
      "Epoch: 5/30 - Batch: 75/91 - Loss: 6.746793270111084\n",
      "Epoch: 5/30 - Batch: 80/91 - Loss: 6.773432731628418\n",
      "Epoch: 5/30 - Batch: 85/91 - Loss: 6.8492255210876465\n",
      "Epoch: 5/30 - Batch: 90/91 - Loss: 6.702579498291016\n",
      "Epoch: 5/30 - Loss: 6.6497483253479\n",
      "Epoch: 6/30 - Batch: 5/91 - Loss: 7.0280656814575195\n",
      "Epoch: 6/30 - Batch: 10/91 - Loss: 6.817198276519775\n",
      "Epoch: 6/30 - Batch: 15/91 - Loss: 6.8774800300598145\n",
      "Epoch: 6/30 - Batch: 20/91 - Loss: 6.8802385330200195\n",
      "Epoch: 6/30 - Batch: 25/91 - Loss: 6.845879554748535\n",
      "Epoch: 6/30 - Batch: 30/91 - Loss: 6.788029670715332\n",
      "Epoch: 6/30 - Batch: 35/91 - Loss: 6.812812328338623\n",
      "Epoch: 6/30 - Batch: 40/91 - Loss: 6.630942344665527\n",
      "Epoch: 6/30 - Batch: 45/91 - Loss: 6.701855659484863\n",
      "Epoch: 6/30 - Batch: 50/91 - Loss: 6.6912150382995605\n",
      "Epoch: 6/30 - Batch: 55/91 - Loss: 6.683107852935791\n",
      "Epoch: 6/30 - Batch: 60/91 - Loss: 6.614732265472412\n",
      "Epoch: 6/30 - Batch: 65/91 - Loss: 6.602919101715088\n",
      "Epoch: 6/30 - Batch: 70/91 - Loss: 6.674985408782959\n",
      "Epoch: 6/30 - Batch: 75/91 - Loss: 6.527285099029541\n",
      "Epoch: 6/30 - Batch: 80/91 - Loss: 6.545553207397461\n",
      "Epoch: 6/30 - Batch: 85/91 - Loss: 6.629894256591797\n",
      "Epoch: 6/30 - Batch: 90/91 - Loss: 6.441752910614014\n",
      "Epoch: 6/30 - Loss: 6.450003623962402\n",
      "Epoch: 7/30 - Batch: 5/91 - Loss: 6.784228324890137\n",
      "Epoch: 7/30 - Batch: 10/91 - Loss: 6.587832450866699\n",
      "Epoch: 7/30 - Batch: 15/91 - Loss: 6.650776386260986\n",
      "Epoch: 7/30 - Batch: 20/91 - Loss: 6.632333755493164\n",
      "Epoch: 7/30 - Batch: 25/91 - Loss: 6.601095676422119\n",
      "Epoch: 7/30 - Batch: 30/91 - Loss: 6.532805919647217\n",
      "Epoch: 7/30 - Batch: 35/91 - Loss: 6.557894706726074\n",
      "Epoch: 7/30 - Batch: 40/91 - Loss: 6.349392890930176\n",
      "Epoch: 7/30 - Batch: 45/91 - Loss: 6.466919422149658\n",
      "Epoch: 7/30 - Batch: 50/91 - Loss: 6.434317588806152\n",
      "Epoch: 7/30 - Batch: 55/91 - Loss: 6.406628608703613\n",
      "Epoch: 7/30 - Batch: 60/91 - Loss: 6.332781791687012\n",
      "Epoch: 7/30 - Batch: 65/91 - Loss: 6.345805644989014\n",
      "Epoch: 7/30 - Batch: 70/91 - Loss: 6.409400463104248\n",
      "Epoch: 7/30 - Batch: 75/91 - Loss: 6.3171281814575195\n",
      "Epoch: 7/30 - Batch: 80/91 - Loss: 6.281306266784668\n",
      "Epoch: 7/30 - Batch: 85/91 - Loss: 6.4062418937683105\n",
      "Epoch: 7/30 - Batch: 90/91 - Loss: 6.176345348358154\n",
      "Epoch: 7/30 - Loss: 6.3200459480285645\n",
      "Epoch: 8/30 - Batch: 5/91 - Loss: 6.544343948364258\n",
      "Epoch: 8/30 - Batch: 10/91 - Loss: 6.345730781555176\n",
      "Epoch: 8/30 - Batch: 15/91 - Loss: 6.438876152038574\n",
      "Epoch: 8/30 - Batch: 20/91 - Loss: 6.430341720581055\n",
      "Epoch: 8/30 - Batch: 25/91 - Loss: 6.399956226348877\n",
      "Epoch: 8/30 - Batch: 30/91 - Loss: 6.298364162445068\n",
      "Epoch: 8/30 - Batch: 35/91 - Loss: 6.338848114013672\n",
      "Epoch: 8/30 - Batch: 40/91 - Loss: 6.105793476104736\n",
      "Epoch: 8/30 - Batch: 45/91 - Loss: 6.247718334197998\n",
      "Epoch: 8/30 - Batch: 50/91 - Loss: 6.222599506378174\n",
      "Epoch: 8/30 - Batch: 55/91 - Loss: 6.194411754608154\n",
      "Epoch: 8/30 - Batch: 60/91 - Loss: 6.114169120788574\n",
      "Epoch: 8/30 - Batch: 65/91 - Loss: 6.142655849456787\n",
      "Epoch: 8/30 - Batch: 70/91 - Loss: 6.202919960021973\n",
      "Epoch: 8/30 - Batch: 75/91 - Loss: 6.145188808441162\n",
      "Epoch: 8/30 - Batch: 80/91 - Loss: 6.0861616134643555\n",
      "Epoch: 8/30 - Batch: 85/91 - Loss: 6.231101036071777\n",
      "Epoch: 8/30 - Batch: 90/91 - Loss: 5.988325119018555\n",
      "Epoch: 8/30 - Loss: 6.2038679122924805\n",
      "Epoch: 9/30 - Batch: 5/91 - Loss: 6.353226184844971\n",
      "Epoch: 9/30 - Batch: 10/91 - Loss: 6.150550365447998\n",
      "Epoch: 9/30 - Batch: 15/91 - Loss: 6.26283073425293\n",
      "Epoch: 9/30 - Batch: 20/91 - Loss: 6.264869213104248\n",
      "Epoch: 9/30 - Batch: 25/91 - Loss: 6.2381391525268555\n",
      "Epoch: 9/30 - Batch: 30/91 - Loss: 6.119853973388672\n",
      "Epoch: 9/30 - Batch: 35/91 - Loss: 6.168781280517578\n",
      "Epoch: 9/30 - Batch: 40/91 - Loss: 5.922852039337158\n",
      "Epoch: 9/30 - Batch: 45/91 - Loss: 6.082447052001953\n",
      "Epoch: 9/30 - Batch: 50/91 - Loss: 6.048854827880859\n",
      "Epoch: 9/30 - Batch: 55/91 - Loss: 6.025081157684326\n",
      "Epoch: 9/30 - Batch: 60/91 - Loss: 5.93077278137207\n",
      "Epoch: 9/30 - Batch: 65/91 - Loss: 5.96450662612915\n",
      "Epoch: 9/30 - Batch: 70/91 - Loss: 6.021061420440674\n",
      "Epoch: 9/30 - Batch: 75/91 - Loss: 5.986189365386963\n",
      "Epoch: 9/30 - Batch: 80/91 - Loss: 5.901613235473633\n",
      "Epoch: 9/30 - Batch: 85/91 - Loss: 6.067768096923828\n",
      "Epoch: 9/30 - Batch: 90/91 - Loss: 5.820927619934082\n",
      "Epoch: 9/30 - Loss: 6.098634243011475\n",
      "Epoch: 10/30 - Batch: 5/91 - Loss: 6.171632289886475\n",
      "Epoch: 10/30 - Batch: 10/91 - Loss: 5.966683864593506\n",
      "Epoch: 10/30 - Batch: 15/91 - Loss: 6.098527908325195\n",
      "Epoch: 10/30 - Batch: 20/91 - Loss: 6.100941181182861\n",
      "Epoch: 10/30 - Batch: 25/91 - Loss: 6.0835089683532715\n",
      "Epoch: 10/30 - Batch: 30/91 - Loss: 5.9547648429870605\n",
      "Epoch: 10/30 - Batch: 35/91 - Loss: 5.991954326629639\n",
      "Epoch: 10/30 - Batch: 40/91 - Loss: 5.7464599609375\n",
      "Epoch: 10/30 - Batch: 45/91 - Loss: 5.913266658782959\n",
      "Epoch: 10/30 - Batch: 50/91 - Loss: 5.889632701873779\n",
      "Epoch: 10/30 - Batch: 55/91 - Loss: 5.861394882202148\n",
      "Epoch: 10/30 - Batch: 60/91 - Loss: 5.761603832244873\n",
      "Epoch: 10/30 - Batch: 65/91 - Loss: 5.808665752410889\n",
      "Epoch: 10/30 - Batch: 70/91 - Loss: 5.8640360832214355\n",
      "Epoch: 10/30 - Batch: 75/91 - Loss: 5.842531204223633\n",
      "Epoch: 10/30 - Batch: 80/91 - Loss: 5.738346099853516\n",
      "Epoch: 10/30 - Batch: 85/91 - Loss: 5.917490005493164\n",
      "Epoch: 10/30 - Batch: 90/91 - Loss: 5.645832538604736\n",
      "Epoch: 10/30 - Loss: 6.029538631439209\n",
      "Epoch: 11/30 - Batch: 5/91 - Loss: 6.01210880279541\n",
      "Epoch: 11/30 - Batch: 10/91 - Loss: 5.802322864532471\n",
      "Epoch: 11/30 - Batch: 15/91 - Loss: 5.952499866485596\n",
      "Epoch: 11/30 - Batch: 20/91 - Loss: 5.942554473876953\n",
      "Epoch: 11/30 - Batch: 25/91 - Loss: 5.9531965255737305\n",
      "Epoch: 11/30 - Batch: 30/91 - Loss: 5.8027143478393555\n",
      "Epoch: 11/30 - Batch: 35/91 - Loss: 5.8555474281311035\n",
      "Epoch: 11/30 - Batch: 40/91 - Loss: 5.595104217529297\n",
      "Epoch: 11/30 - Batch: 45/91 - Loss: 5.775668144226074\n",
      "Epoch: 11/30 - Batch: 50/91 - Loss: 5.742470741271973\n",
      "Epoch: 11/30 - Batch: 55/91 - Loss: 5.698368072509766\n",
      "Epoch: 11/30 - Batch: 60/91 - Loss: 5.609320640563965\n",
      "Epoch: 11/30 - Batch: 65/91 - Loss: 5.652719497680664\n",
      "Epoch: 11/30 - Batch: 70/91 - Loss: 5.714448928833008\n",
      "Epoch: 11/30 - Batch: 75/91 - Loss: 5.72126579284668\n",
      "Epoch: 11/30 - Batch: 80/91 - Loss: 5.605592727661133\n",
      "Epoch: 11/30 - Batch: 85/91 - Loss: 5.798468112945557\n",
      "Epoch: 11/30 - Batch: 90/91 - Loss: 5.508708477020264\n",
      "Epoch: 11/30 - Loss: 5.947956085205078\n",
      "Epoch: 12/30 - Batch: 5/91 - Loss: 5.871438503265381\n",
      "Epoch: 12/30 - Batch: 10/91 - Loss: 5.676655292510986\n",
      "Epoch: 12/30 - Batch: 15/91 - Loss: 5.819979190826416\n",
      "Epoch: 12/30 - Batch: 20/91 - Loss: 5.828922271728516\n",
      "Epoch: 12/30 - Batch: 25/91 - Loss: 5.83475399017334\n",
      "Epoch: 12/30 - Batch: 30/91 - Loss: 5.6838579177856445\n",
      "Epoch: 12/30 - Batch: 35/91 - Loss: 5.732668399810791\n",
      "Epoch: 12/30 - Batch: 40/91 - Loss: 5.468225002288818\n",
      "Epoch: 12/30 - Batch: 45/91 - Loss: 5.662636756896973\n",
      "Epoch: 12/30 - Batch: 50/91 - Loss: 5.636660099029541\n",
      "Epoch: 12/30 - Batch: 55/91 - Loss: 5.589153289794922\n",
      "Epoch: 12/30 - Batch: 60/91 - Loss: 5.497962474822998\n",
      "Epoch: 12/30 - Batch: 65/91 - Loss: 5.540119171142578\n",
      "Epoch: 12/30 - Batch: 70/91 - Loss: 5.6102399826049805\n",
      "Epoch: 12/30 - Batch: 75/91 - Loss: 5.619878768920898\n",
      "Epoch: 12/30 - Batch: 80/91 - Loss: 5.495489120483398\n",
      "Epoch: 12/30 - Batch: 85/91 - Loss: 5.69631290435791\n",
      "Epoch: 12/30 - Batch: 90/91 - Loss: 5.403131008148193\n",
      "Epoch: 12/30 - Loss: 5.873080253601074\n",
      "Epoch: 13/30 - Batch: 5/91 - Loss: 5.7692766189575195\n",
      "Epoch: 13/30 - Batch: 10/91 - Loss: 5.5596537590026855\n",
      "Epoch: 13/30 - Batch: 15/91 - Loss: 5.720510959625244\n",
      "Epoch: 13/30 - Batch: 20/91 - Loss: 5.729861259460449\n",
      "Epoch: 13/30 - Batch: 25/91 - Loss: 5.750057220458984\n",
      "Epoch: 13/30 - Batch: 30/91 - Loss: 5.5728983879089355\n",
      "Epoch: 13/30 - Batch: 35/91 - Loss: 5.632450103759766\n",
      "Epoch: 13/30 - Batch: 40/91 - Loss: 5.365689277648926\n",
      "Epoch: 13/30 - Batch: 45/91 - Loss: 5.560625076293945\n",
      "Epoch: 13/30 - Batch: 50/91 - Loss: 5.545833110809326\n",
      "Epoch: 13/30 - Batch: 55/91 - Loss: 5.478994846343994\n",
      "Epoch: 13/30 - Batch: 60/91 - Loss: 5.393495082855225\n",
      "Epoch: 13/30 - Batch: 65/91 - Loss: 5.452538967132568\n",
      "Epoch: 13/30 - Batch: 70/91 - Loss: 5.513700485229492\n",
      "Epoch: 13/30 - Batch: 75/91 - Loss: 5.534658908843994\n",
      "Epoch: 13/30 - Batch: 80/91 - Loss: 5.407208442687988\n",
      "Epoch: 13/30 - Batch: 85/91 - Loss: 5.609335899353027\n",
      "Epoch: 13/30 - Batch: 90/91 - Loss: 5.315562725067139\n",
      "Epoch: 13/30 - Loss: 5.806080341339111\n",
      "Epoch: 14/30 - Batch: 5/91 - Loss: 5.668394565582275\n",
      "Epoch: 14/30 - Batch: 10/91 - Loss: 5.464111328125\n",
      "Epoch: 14/30 - Batch: 15/91 - Loss: 5.629952907562256\n",
      "Epoch: 14/30 - Batch: 20/91 - Loss: 5.647139072418213\n",
      "Epoch: 14/30 - Batch: 25/91 - Loss: 5.663371562957764\n",
      "Epoch: 14/30 - Batch: 30/91 - Loss: 5.48471736907959\n",
      "Epoch: 14/30 - Batch: 35/91 - Loss: 5.542214393615723\n",
      "Epoch: 14/30 - Batch: 40/91 - Loss: 5.274547100067139\n",
      "Epoch: 14/30 - Batch: 45/91 - Loss: 5.462869644165039\n",
      "Epoch: 14/30 - Batch: 50/91 - Loss: 5.467528820037842\n",
      "Epoch: 14/30 - Batch: 55/91 - Loss: 5.408375263214111\n",
      "Epoch: 14/30 - Batch: 60/91 - Loss: 5.313253402709961\n",
      "Epoch: 14/30 - Batch: 65/91 - Loss: 5.361822128295898\n",
      "Epoch: 14/30 - Batch: 70/91 - Loss: 5.423267841339111\n",
      "Epoch: 14/30 - Batch: 75/91 - Loss: 5.471187591552734\n",
      "Epoch: 14/30 - Batch: 80/91 - Loss: 5.324595928192139\n",
      "Epoch: 14/30 - Batch: 85/91 - Loss: 5.537797927856445\n",
      "Epoch: 14/30 - Batch: 90/91 - Loss: 5.248199462890625\n",
      "Epoch: 14/30 - Loss: 5.747702598571777\n",
      "Epoch: 15/30 - Batch: 5/91 - Loss: 5.593867301940918\n",
      "Epoch: 15/30 - Batch: 10/91 - Loss: 5.381287097930908\n",
      "Epoch: 15/30 - Batch: 15/91 - Loss: 5.546352386474609\n",
      "Epoch: 15/30 - Batch: 20/91 - Loss: 5.563129425048828\n",
      "Epoch: 15/30 - Batch: 25/91 - Loss: 5.591238975524902\n",
      "Epoch: 15/30 - Batch: 30/91 - Loss: 5.416815280914307\n",
      "Epoch: 15/30 - Batch: 35/91 - Loss: 5.474318981170654\n",
      "Epoch: 15/30 - Batch: 40/91 - Loss: 5.203303813934326\n",
      "Epoch: 15/30 - Batch: 45/91 - Loss: 5.395452499389648\n",
      "Epoch: 15/30 - Batch: 50/91 - Loss: 5.414317607879639\n",
      "Epoch: 15/30 - Batch: 55/91 - Loss: 5.313075065612793\n",
      "Epoch: 15/30 - Batch: 60/91 - Loss: 5.223697662353516\n",
      "Epoch: 15/30 - Batch: 65/91 - Loss: 5.29296875\n",
      "Epoch: 15/30 - Batch: 70/91 - Loss: 5.359573841094971\n",
      "Epoch: 15/30 - Batch: 75/91 - Loss: 5.394635200500488\n",
      "Epoch: 15/30 - Batch: 80/91 - Loss: 5.248896598815918\n",
      "Epoch: 15/30 - Batch: 85/91 - Loss: 5.471340179443359\n",
      "Epoch: 15/30 - Batch: 90/91 - Loss: 5.171408653259277\n",
      "Epoch: 15/30 - Loss: 5.693943023681641\n",
      "Epoch: 16/30 - Batch: 5/91 - Loss: 5.5064311027526855\n",
      "Epoch: 16/30 - Batch: 10/91 - Loss: 5.312678813934326\n",
      "Epoch: 16/30 - Batch: 15/91 - Loss: 5.472323417663574\n",
      "Epoch: 16/30 - Batch: 20/91 - Loss: 5.5068769454956055\n",
      "Epoch: 16/30 - Batch: 25/91 - Loss: 5.5194292068481445\n",
      "Epoch: 16/30 - Batch: 30/91 - Loss: 5.356497287750244\n",
      "Epoch: 16/30 - Batch: 35/91 - Loss: 5.406710624694824\n",
      "Epoch: 16/30 - Batch: 40/91 - Loss: 5.141179084777832\n",
      "Epoch: 16/30 - Batch: 45/91 - Loss: 5.312791347503662\n",
      "Epoch: 16/30 - Batch: 50/91 - Loss: 5.347833156585693\n",
      "Epoch: 16/30 - Batch: 55/91 - Loss: 5.254510402679443\n",
      "Epoch: 16/30 - Batch: 60/91 - Loss: 5.1578874588012695\n",
      "Epoch: 16/30 - Batch: 65/91 - Loss: 5.2466535568237305\n",
      "Epoch: 16/30 - Batch: 70/91 - Loss: 5.286618709564209\n",
      "Epoch: 16/30 - Batch: 75/91 - Loss: 5.344064235687256\n",
      "Epoch: 16/30 - Batch: 80/91 - Loss: 5.189606189727783\n",
      "Epoch: 16/30 - Batch: 85/91 - Loss: 5.406169414520264\n",
      "Epoch: 16/30 - Batch: 90/91 - Loss: 5.11123514175415\n",
      "Epoch: 16/30 - Loss: 5.638383388519287\n",
      "Epoch: 17/30 - Batch: 5/91 - Loss: 5.449246406555176\n",
      "Epoch: 17/30 - Batch: 10/91 - Loss: 5.250910758972168\n",
      "Epoch: 17/30 - Batch: 15/91 - Loss: 5.411078929901123\n",
      "Epoch: 17/30 - Batch: 20/91 - Loss: 5.433850288391113\n",
      "Epoch: 17/30 - Batch: 25/91 - Loss: 5.464054584503174\n",
      "Epoch: 17/30 - Batch: 30/91 - Loss: 5.297635078430176\n",
      "Epoch: 17/30 - Batch: 35/91 - Loss: 5.348325252532959\n",
      "Epoch: 17/30 - Batch: 40/91 - Loss: 5.070743560791016\n",
      "Epoch: 17/30 - Batch: 45/91 - Loss: 5.24177360534668\n",
      "Epoch: 17/30 - Batch: 50/91 - Loss: 5.288990497589111\n",
      "Epoch: 17/30 - Batch: 55/91 - Loss: 5.1862711906433105\n",
      "Epoch: 17/30 - Batch: 60/91 - Loss: 5.108002185821533\n",
      "Epoch: 17/30 - Batch: 65/91 - Loss: 5.173352241516113\n",
      "Epoch: 17/30 - Batch: 70/91 - Loss: 5.222381591796875\n",
      "Epoch: 17/30 - Batch: 75/91 - Loss: 5.2834038734436035\n",
      "Epoch: 17/30 - Batch: 80/91 - Loss: 5.1311116218566895\n",
      "Epoch: 17/30 - Batch: 85/91 - Loss: 5.3590803146362305\n",
      "Epoch: 17/30 - Batch: 90/91 - Loss: 5.068635940551758\n",
      "Epoch: 17/30 - Loss: 5.577532768249512\n",
      "Epoch: 18/30 - Batch: 5/91 - Loss: 5.382355213165283\n",
      "Epoch: 18/30 - Batch: 10/91 - Loss: 5.1819868087768555\n",
      "Epoch: 18/30 - Batch: 15/91 - Loss: 5.34547233581543\n",
      "Epoch: 18/30 - Batch: 20/91 - Loss: 5.387477397918701\n",
      "Epoch: 18/30 - Batch: 25/91 - Loss: 5.404373645782471\n",
      "Epoch: 18/30 - Batch: 30/91 - Loss: 5.239706039428711\n",
      "Epoch: 18/30 - Batch: 35/91 - Loss: 5.281182289123535\n",
      "Epoch: 18/30 - Batch: 40/91 - Loss: 5.011883735656738\n",
      "Epoch: 18/30 - Batch: 45/91 - Loss: 5.1767401695251465\n",
      "Epoch: 18/30 - Batch: 50/91 - Loss: 5.2342963218688965\n",
      "Epoch: 18/30 - Batch: 55/91 - Loss: 5.123857498168945\n",
      "Epoch: 18/30 - Batch: 60/91 - Loss: 5.04482889175415\n",
      "Epoch: 18/30 - Batch: 65/91 - Loss: 5.119415760040283\n",
      "Epoch: 18/30 - Batch: 70/91 - Loss: 5.18325138092041\n",
      "Epoch: 18/30 - Batch: 75/91 - Loss: 5.227886199951172\n",
      "Epoch: 18/30 - Batch: 80/91 - Loss: 5.0797343254089355\n",
      "Epoch: 18/30 - Batch: 85/91 - Loss: 5.308990001678467\n",
      "Epoch: 18/30 - Batch: 90/91 - Loss: 5.021534442901611\n",
      "Epoch: 18/30 - Loss: 5.526782035827637\n",
      "Epoch: 19/30 - Batch: 5/91 - Loss: 5.335392951965332\n",
      "Epoch: 19/30 - Batch: 10/91 - Loss: 5.124818325042725\n",
      "Epoch: 19/30 - Batch: 15/91 - Loss: 5.2987380027771\n",
      "Epoch: 19/30 - Batch: 20/91 - Loss: 5.338487148284912\n",
      "Epoch: 19/30 - Batch: 25/91 - Loss: 5.3404035568237305\n",
      "Epoch: 19/30 - Batch: 30/91 - Loss: 5.190464496612549\n",
      "Epoch: 19/30 - Batch: 35/91 - Loss: 5.230730056762695\n",
      "Epoch: 19/30 - Batch: 40/91 - Loss: 4.970306396484375\n",
      "Epoch: 19/30 - Batch: 45/91 - Loss: 5.120753765106201\n",
      "Epoch: 19/30 - Batch: 50/91 - Loss: 5.189195156097412\n",
      "Epoch: 19/30 - Batch: 55/91 - Loss: 5.073747158050537\n",
      "Epoch: 19/30 - Batch: 60/91 - Loss: 4.993180751800537\n",
      "Epoch: 19/30 - Batch: 65/91 - Loss: 5.0691609382629395\n",
      "Epoch: 19/30 - Batch: 70/91 - Loss: 5.117142200469971\n",
      "Epoch: 19/30 - Batch: 75/91 - Loss: 5.181885242462158\n",
      "Epoch: 19/30 - Batch: 80/91 - Loss: 5.0297346115112305\n",
      "Epoch: 19/30 - Batch: 85/91 - Loss: 5.253168106079102\n",
      "Epoch: 19/30 - Batch: 90/91 - Loss: 4.969904899597168\n",
      "Epoch: 19/30 - Loss: 5.4703369140625\n",
      "Epoch: 20/30 - Batch: 5/91 - Loss: 5.272241115570068\n",
      "Epoch: 20/30 - Batch: 10/91 - Loss: 5.065439224243164\n",
      "Epoch: 20/30 - Batch: 15/91 - Loss: 5.245168685913086\n",
      "Epoch: 20/30 - Batch: 20/91 - Loss: 5.286154270172119\n",
      "Epoch: 20/30 - Batch: 25/91 - Loss: 5.300024032592773\n",
      "Epoch: 20/30 - Batch: 30/91 - Loss: 5.150888442993164\n",
      "Epoch: 20/30 - Batch: 35/91 - Loss: 5.199697971343994\n",
      "Epoch: 20/30 - Batch: 40/91 - Loss: 4.927413463592529\n",
      "Epoch: 20/30 - Batch: 45/91 - Loss: 5.065827369689941\n",
      "Epoch: 20/30 - Batch: 50/91 - Loss: 5.135974884033203\n",
      "Epoch: 20/30 - Batch: 55/91 - Loss: 5.028753757476807\n",
      "Epoch: 20/30 - Batch: 60/91 - Loss: 4.948066711425781\n",
      "Epoch: 20/30 - Batch: 65/91 - Loss: 5.036891460418701\n",
      "Epoch: 20/30 - Batch: 70/91 - Loss: 5.073437690734863\n",
      "Epoch: 20/30 - Batch: 75/91 - Loss: 5.142484188079834\n",
      "Epoch: 20/30 - Batch: 80/91 - Loss: 4.984625339508057\n",
      "Epoch: 20/30 - Batch: 85/91 - Loss: 5.21469259262085\n",
      "Epoch: 20/30 - Batch: 90/91 - Loss: 4.923190593719482\n",
      "Epoch: 20/30 - Loss: 5.409445285797119\n",
      "Epoch: 21/30 - Batch: 5/91 - Loss: 5.234899997711182\n",
      "Epoch: 21/30 - Batch: 10/91 - Loss: 5.017887115478516\n",
      "Epoch: 21/30 - Batch: 15/91 - Loss: 5.203124523162842\n",
      "Epoch: 21/30 - Batch: 20/91 - Loss: 5.251072883605957\n",
      "Epoch: 21/30 - Batch: 25/91 - Loss: 5.254789352416992\n",
      "Epoch: 21/30 - Batch: 30/91 - Loss: 5.100070953369141\n",
      "Epoch: 21/30 - Batch: 35/91 - Loss: 5.1405839920043945\n",
      "Epoch: 21/30 - Batch: 40/91 - Loss: 4.879770278930664\n",
      "Epoch: 21/30 - Batch: 45/91 - Loss: 5.016477584838867\n",
      "Epoch: 21/30 - Batch: 50/91 - Loss: 5.096014499664307\n",
      "Epoch: 21/30 - Batch: 55/91 - Loss: 4.989586353302002\n",
      "Epoch: 21/30 - Batch: 60/91 - Loss: 4.9063801765441895\n",
      "Epoch: 21/30 - Batch: 65/91 - Loss: 4.984371185302734\n",
      "Epoch: 21/30 - Batch: 70/91 - Loss: 5.0331244468688965\n",
      "Epoch: 21/30 - Batch: 75/91 - Loss: 5.10117769241333\n",
      "Epoch: 21/30 - Batch: 80/91 - Loss: 4.9399824142456055\n",
      "Epoch: 21/30 - Batch: 85/91 - Loss: 5.1558942794799805\n",
      "Epoch: 21/30 - Batch: 90/91 - Loss: 4.8866071701049805\n",
      "Epoch: 21/30 - Loss: 5.345376014709473\n",
      "Epoch: 22/30 - Batch: 5/91 - Loss: 5.174985408782959\n",
      "Epoch: 22/30 - Batch: 10/91 - Loss: 4.973007678985596\n",
      "Epoch: 22/30 - Batch: 15/91 - Loss: 5.163106918334961\n",
      "Epoch: 22/30 - Batch: 20/91 - Loss: 5.209590435028076\n",
      "Epoch: 22/30 - Batch: 25/91 - Loss: 5.2155537605285645\n",
      "Epoch: 22/30 - Batch: 30/91 - Loss: 5.061391830444336\n",
      "Epoch: 22/30 - Batch: 35/91 - Loss: 5.123722553253174\n",
      "Epoch: 22/30 - Batch: 40/91 - Loss: 4.842640399932861\n",
      "Epoch: 22/30 - Batch: 45/91 - Loss: 4.97633171081543\n",
      "Epoch: 22/30 - Batch: 50/91 - Loss: 5.061866283416748\n",
      "Epoch: 22/30 - Batch: 55/91 - Loss: 4.953736305236816\n",
      "Epoch: 22/30 - Batch: 60/91 - Loss: 4.8590989112854\n",
      "Epoch: 22/30 - Batch: 65/91 - Loss: 4.946140289306641\n",
      "Epoch: 22/30 - Batch: 70/91 - Loss: 4.996987819671631\n",
      "Epoch: 22/30 - Batch: 75/91 - Loss: 5.088487148284912\n",
      "Epoch: 22/30 - Batch: 80/91 - Loss: 4.897294044494629\n",
      "Epoch: 22/30 - Batch: 85/91 - Loss: 5.115732669830322\n",
      "Epoch: 22/30 - Batch: 90/91 - Loss: 4.844873428344727\n",
      "Epoch: 22/30 - Loss: 5.29997444152832\n",
      "Epoch: 23/30 - Batch: 5/91 - Loss: 5.13832950592041\n",
      "Epoch: 23/30 - Batch: 10/91 - Loss: 4.929090976715088\n",
      "Epoch: 23/30 - Batch: 15/91 - Loss: 5.121965408325195\n",
      "Epoch: 23/30 - Batch: 20/91 - Loss: 5.162164688110352\n",
      "Epoch: 23/30 - Batch: 25/91 - Loss: 5.168160438537598\n",
      "Epoch: 23/30 - Batch: 30/91 - Loss: 5.026045799255371\n",
      "Epoch: 23/30 - Batch: 35/91 - Loss: 5.069110870361328\n",
      "Epoch: 23/30 - Batch: 40/91 - Loss: 4.805463790893555\n",
      "Epoch: 23/30 - Batch: 45/91 - Loss: 4.946384906768799\n",
      "Epoch: 23/30 - Batch: 50/91 - Loss: 5.020979881286621\n",
      "Epoch: 23/30 - Batch: 55/91 - Loss: 4.921199798583984\n",
      "Epoch: 23/30 - Batch: 60/91 - Loss: 4.8308634757995605\n",
      "Epoch: 23/30 - Batch: 65/91 - Loss: 4.894924163818359\n",
      "Epoch: 23/30 - Batch: 70/91 - Loss: 4.950621128082275\n",
      "Epoch: 23/30 - Batch: 75/91 - Loss: 5.01396369934082\n",
      "Epoch: 23/30 - Batch: 80/91 - Loss: 4.8531341552734375\n",
      "Epoch: 23/30 - Batch: 85/91 - Loss: 5.092045783996582\n",
      "Epoch: 23/30 - Batch: 90/91 - Loss: 4.804939270019531\n",
      "Epoch: 23/30 - Loss: 5.224560737609863\n",
      "Epoch: 24/30 - Batch: 5/91 - Loss: 5.103122711181641\n",
      "Epoch: 24/30 - Batch: 10/91 - Loss: 4.897634983062744\n",
      "Epoch: 24/30 - Batch: 15/91 - Loss: 5.092025279998779\n",
      "Epoch: 24/30 - Batch: 20/91 - Loss: 5.130919933319092\n",
      "Epoch: 24/30 - Batch: 25/91 - Loss: 5.125280857086182\n",
      "Epoch: 24/30 - Batch: 30/91 - Loss: 4.9947710037231445\n",
      "Epoch: 24/30 - Batch: 35/91 - Loss: 5.032256126403809\n",
      "Epoch: 24/30 - Batch: 40/91 - Loss: 4.773505687713623\n",
      "Epoch: 24/30 - Batch: 45/91 - Loss: 4.906163692474365\n",
      "Epoch: 24/30 - Batch: 50/91 - Loss: 4.984507083892822\n",
      "Epoch: 24/30 - Batch: 55/91 - Loss: 4.883740425109863\n",
      "Epoch: 24/30 - Batch: 60/91 - Loss: 4.7978196144104\n",
      "Epoch: 24/30 - Batch: 65/91 - Loss: 4.858249664306641\n",
      "Epoch: 24/30 - Batch: 70/91 - Loss: 4.9322404861450195\n",
      "Epoch: 24/30 - Batch: 75/91 - Loss: 4.968178749084473\n",
      "Epoch: 24/30 - Batch: 80/91 - Loss: 4.812706470489502\n",
      "Epoch: 24/30 - Batch: 85/91 - Loss: 5.037080764770508\n",
      "Epoch: 24/30 - Batch: 90/91 - Loss: 4.788508415222168\n",
      "Epoch: 24/30 - Loss: 5.168615341186523\n",
      "Epoch: 25/30 - Batch: 5/91 - Loss: 5.056054592132568\n",
      "Epoch: 25/30 - Batch: 10/91 - Loss: 4.844076633453369\n",
      "Epoch: 25/30 - Batch: 15/91 - Loss: 5.040551662445068\n",
      "Epoch: 25/30 - Batch: 20/91 - Loss: 5.089585781097412\n",
      "Epoch: 25/30 - Batch: 25/91 - Loss: 5.0861735343933105\n",
      "Epoch: 25/30 - Batch: 30/91 - Loss: 4.952754020690918\n",
      "Epoch: 25/30 - Batch: 35/91 - Loss: 4.997771739959717\n",
      "Epoch: 25/30 - Batch: 40/91 - Loss: 4.7338151931762695\n",
      "Epoch: 25/30 - Batch: 45/91 - Loss: 4.873397350311279\n",
      "Epoch: 25/30 - Batch: 50/91 - Loss: 4.949474334716797\n",
      "Epoch: 25/30 - Batch: 55/91 - Loss: 4.842320442199707\n",
      "Epoch: 25/30 - Batch: 60/91 - Loss: 4.762912273406982\n",
      "Epoch: 25/30 - Batch: 65/91 - Loss: 4.825456619262695\n",
      "Epoch: 25/30 - Batch: 70/91 - Loss: 4.878359794616699\n",
      "Epoch: 25/30 - Batch: 75/91 - Loss: 4.930741786956787\n",
      "Epoch: 25/30 - Batch: 80/91 - Loss: 4.785365581512451\n",
      "Epoch: 25/30 - Batch: 85/91 - Loss: 4.995517730712891\n",
      "Epoch: 25/30 - Batch: 90/91 - Loss: 4.739165782928467\n",
      "Epoch: 25/30 - Loss: 5.099579811096191\n",
      "Epoch: 26/30 - Batch: 5/91 - Loss: 5.014410972595215\n",
      "Epoch: 26/30 - Batch: 10/91 - Loss: 4.805156707763672\n",
      "Epoch: 26/30 - Batch: 15/91 - Loss: 5.012483596801758\n",
      "Epoch: 26/30 - Batch: 20/91 - Loss: 5.058853626251221\n",
      "Epoch: 26/30 - Batch: 25/91 - Loss: 5.048216342926025\n",
      "Epoch: 26/30 - Batch: 30/91 - Loss: 4.919081211090088\n",
      "Epoch: 26/30 - Batch: 35/91 - Loss: 4.950620651245117\n",
      "Epoch: 26/30 - Batch: 40/91 - Loss: 4.714480400085449\n",
      "Epoch: 26/30 - Batch: 45/91 - Loss: 4.831556797027588\n",
      "Epoch: 26/30 - Batch: 50/91 - Loss: 4.905765056610107\n",
      "Epoch: 26/30 - Batch: 55/91 - Loss: 4.818368434906006\n",
      "Epoch: 26/30 - Batch: 60/91 - Loss: 4.731907367706299\n",
      "Epoch: 26/30 - Batch: 65/91 - Loss: 4.801505088806152\n",
      "Epoch: 26/30 - Batch: 70/91 - Loss: 4.85658597946167\n",
      "Epoch: 26/30 - Batch: 75/91 - Loss: 4.922214031219482\n",
      "Epoch: 26/30 - Batch: 80/91 - Loss: 4.752051830291748\n",
      "Epoch: 26/30 - Batch: 85/91 - Loss: 4.977848052978516\n",
      "Epoch: 26/30 - Batch: 90/91 - Loss: 4.711294651031494\n",
      "Epoch: 26/30 - Loss: 5.063015937805176\n",
      "Epoch: 27/30 - Batch: 5/91 - Loss: 4.983739376068115\n",
      "Epoch: 27/30 - Batch: 10/91 - Loss: 4.773764610290527\n",
      "Epoch: 27/30 - Batch: 15/91 - Loss: 4.978572845458984\n",
      "Epoch: 27/30 - Batch: 20/91 - Loss: 5.022763252258301\n",
      "Epoch: 27/30 - Batch: 25/91 - Loss: 5.007397651672363\n",
      "Epoch: 27/30 - Batch: 30/91 - Loss: 4.888266086578369\n",
      "Epoch: 27/30 - Batch: 35/91 - Loss: 4.917288303375244\n",
      "Epoch: 27/30 - Batch: 40/91 - Loss: 4.671387195587158\n",
      "Epoch: 27/30 - Batch: 45/91 - Loss: 4.799383163452148\n",
      "Epoch: 27/30 - Batch: 50/91 - Loss: 4.868991851806641\n",
      "Epoch: 27/30 - Batch: 55/91 - Loss: 4.785554885864258\n",
      "Epoch: 27/30 - Batch: 60/91 - Loss: 4.696221351623535\n",
      "Epoch: 27/30 - Batch: 65/91 - Loss: 4.754740238189697\n",
      "Epoch: 27/30 - Batch: 70/91 - Loss: 4.824949264526367\n",
      "Epoch: 27/30 - Batch: 75/91 - Loss: 4.863870143890381\n",
      "Epoch: 27/30 - Batch: 80/91 - Loss: 4.732875347137451\n",
      "Epoch: 27/30 - Batch: 85/91 - Loss: 4.936198711395264\n",
      "Epoch: 27/30 - Batch: 90/91 - Loss: 4.689777374267578\n",
      "Epoch: 27/30 - Loss: 5.006953239440918\n",
      "Epoch: 28/30 - Batch: 5/91 - Loss: 4.957143783569336\n",
      "Epoch: 28/30 - Batch: 10/91 - Loss: 4.737542629241943\n",
      "Epoch: 28/30 - Batch: 15/91 - Loss: 4.94661808013916\n",
      "Epoch: 28/30 - Batch: 20/91 - Loss: 4.984706878662109\n",
      "Epoch: 28/30 - Batch: 25/91 - Loss: 4.959152698516846\n",
      "Epoch: 28/30 - Batch: 30/91 - Loss: 4.872039794921875\n",
      "Epoch: 28/30 - Batch: 35/91 - Loss: 4.895608901977539\n",
      "Epoch: 28/30 - Batch: 40/91 - Loss: 4.6415181159973145\n",
      "Epoch: 28/30 - Batch: 45/91 - Loss: 4.757941722869873\n",
      "Epoch: 28/30 - Batch: 50/91 - Loss: 4.829749584197998\n",
      "Epoch: 28/30 - Batch: 55/91 - Loss: 4.754583835601807\n",
      "Epoch: 28/30 - Batch: 60/91 - Loss: 4.666933059692383\n",
      "Epoch: 28/30 - Batch: 65/91 - Loss: 4.724432468414307\n",
      "Epoch: 28/30 - Batch: 70/91 - Loss: 4.788850784301758\n",
      "Epoch: 28/30 - Batch: 75/91 - Loss: 4.833612442016602\n",
      "Epoch: 28/30 - Batch: 80/91 - Loss: 4.687503814697266\n",
      "Epoch: 28/30 - Batch: 85/91 - Loss: 4.89470100402832\n",
      "Epoch: 28/30 - Batch: 90/91 - Loss: 4.653257846832275\n",
      "Epoch: 28/30 - Loss: 4.957350730895996\n",
      "Epoch: 29/30 - Batch: 5/91 - Loss: 4.910890579223633\n",
      "Epoch: 29/30 - Batch: 10/91 - Loss: 4.715453624725342\n",
      "Epoch: 29/30 - Batch: 15/91 - Loss: 4.913632869720459\n",
      "Epoch: 29/30 - Batch: 20/91 - Loss: 4.939422607421875\n",
      "Epoch: 29/30 - Batch: 25/91 - Loss: 4.932920455932617\n",
      "Epoch: 29/30 - Batch: 30/91 - Loss: 4.839517593383789\n",
      "Epoch: 29/30 - Batch: 35/91 - Loss: 4.863808631896973\n",
      "Epoch: 29/30 - Batch: 40/91 - Loss: 4.611042022705078\n",
      "Epoch: 29/30 - Batch: 45/91 - Loss: 4.716728687286377\n",
      "Epoch: 29/30 - Batch: 50/91 - Loss: 4.8078999519348145\n",
      "Epoch: 29/30 - Batch: 55/91 - Loss: 4.712130546569824\n",
      "Epoch: 29/30 - Batch: 60/91 - Loss: 4.6300272941589355\n",
      "Epoch: 29/30 - Batch: 65/91 - Loss: 4.708076000213623\n",
      "Epoch: 29/30 - Batch: 70/91 - Loss: 4.777845859527588\n",
      "Epoch: 29/30 - Batch: 75/91 - Loss: 4.804637432098389\n",
      "Epoch: 29/30 - Batch: 80/91 - Loss: 4.678420543670654\n",
      "Epoch: 29/30 - Batch: 85/91 - Loss: 4.872453689575195\n",
      "Epoch: 29/30 - Batch: 90/91 - Loss: 4.645307540893555\n",
      "Epoch: 29/30 - Loss: 4.901966571807861\n",
      "Epoch: 30/30 - Batch: 5/91 - Loss: 4.881229400634766\n",
      "Epoch: 30/30 - Batch: 10/91 - Loss: 4.665651321411133\n",
      "Epoch: 30/30 - Batch: 15/91 - Loss: 4.889822006225586\n",
      "Epoch: 30/30 - Batch: 20/91 - Loss: 4.931597709655762\n",
      "Epoch: 30/30 - Batch: 25/91 - Loss: 4.912037372589111\n",
      "Epoch: 30/30 - Batch: 30/91 - Loss: 4.823312759399414\n",
      "Epoch: 30/30 - Batch: 35/91 - Loss: 4.820186614990234\n",
      "Epoch: 30/30 - Batch: 40/91 - Loss: 4.579560279846191\n",
      "Epoch: 30/30 - Batch: 45/91 - Loss: 4.690633773803711\n",
      "Epoch: 30/30 - Batch: 50/91 - Loss: 4.769590854644775\n",
      "Epoch: 30/30 - Batch: 55/91 - Loss: 4.691145420074463\n",
      "Epoch: 30/30 - Batch: 60/91 - Loss: 4.606697082519531\n",
      "Epoch: 30/30 - Batch: 65/91 - Loss: 4.660018444061279\n",
      "Epoch: 30/30 - Batch: 70/91 - Loss: 4.737432479858398\n",
      "Epoch: 30/30 - Batch: 75/91 - Loss: 4.771742343902588\n",
      "Epoch: 30/30 - Batch: 80/91 - Loss: 4.653273582458496\n",
      "Epoch: 30/30 - Batch: 85/91 - Loss: 4.862450122833252\n",
      "Epoch: 30/30 - Batch: 90/91 - Loss: 4.628871440887451\n",
      "Epoch: 30/30 - Loss: 4.852506160736084\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV1b3/8fc3MyGQkBmSMIR5CgECiFAEUapoRevYSe0g2mp7vfe2tXr783rtcFs7qa0TdWjVVlutiK0jai0CMiTMMwESEhJCBhIIU6b1+yNHH24MGiDJPsPn9Tx5OGfvzcl3uc0ni7XXXtucc4iISOAL87oAERHpHAp0EZEgoUAXEQkSCnQRkSChQBcRCRIRXn3j5ORkN3DgQK++vYhIQCooKKhyzqW0t8+zQB84cCD5+flefXsRkYBkZsWn2qchFxGRIKFAFxEJEgp0EZEgoUAXEQkSCnQRkSChQBcRCRIKdBGRIBFwgV54oJ57/76FhqYWr0sREfErARfoJTVHeXLZHt7dVuF1KSIifiXgAn3GsBTSe8fw/OoSr0sREfErARfo4WHG1XmZ/GtHJWW1x7wuR0TEbwRcoANck5eFc/BiQanXpYiI+I2ADPSsxFimD0nmL6tLaGnRM1FFRCBAAx3gmklZ7Ks9xrJdVV6XIiLiFwI20OeMSiMhNlIXR0VEfAI20GMiw7lifAaLN1dQc6TB63JERDwXsIEOcO2kLBqaW1i4dp/XpYiIeC6gA31Eem9ysxL4y+q9OKeLoyIS2gI60KG1l76jop61JbVelyIi4qmAD/TPjetHbFQ4f9XFUREJcQEf6HHREVya05dX1pdRf6LJ63JERDwT8IEOcO2k/hxtaObVDWVelyIi4pmgCPQJ/RMYkhqnOekiEtKCItDNjOsmZbF2by07Kg57XY6IiCeCItABrhifQWS48Rf10kUkRAVNoCfFRTNnVDovrSnlRFOz1+WIiHS7oAl0aF2w6+DRRhZv0dOMRCT0BFWgTx+STEZCDw27iEhICqpA//BpRksLqyipOep1OSIi3SqoAh3g6rwsAF7Q04xEJMQEXaBnJPRgxtAUXsgvoVlPMxKREPKpgW5mw81s3Ulfh8zs9jbHmJk9aGaFZrbBzCZ0Xcmf7tpJWZTXHWfJzkovyxAR6VafGujOue3OuVznXC4wETgKLGxz2MXAUN/XfOCRzi70dFwwMo3EnlFasEtEQsrpDrnMBnY554rbbJ8HPO1arQASzKxvp1R4BqIiwrhyQgaLt1RQVX/CqzJERLrV6Qb6dcBz7WzPAE7uDpf6tv0fZjbfzPLNLL+ysmuHQ66dlEVTi+Mnr25l+a4qrcQoIkEvoqMHmlkUcBlwZ3u729n2sSuSzrkFwAKAvLy8Lr1iOSS1F1eMz2Dh2n0sXLsPMxiaGkduVgK5WX3IzUpgWFocEeFBd11YREJUhwOd1nHyNc659m7DLAWyTnqfCXi+lu1vrs3l7ktHsb60lnUlrV+Lt1Tw1/zWKY09IsMZmxFPbv8EJvTvw4xhycRGnc5/EhER/3E66fUF2h9uAXgFuM3MngemAHXOufKzLa4z9OkZxczhqcwcngqAc469NUdZV1LL2r2tIf+HZUUsWLKb2Khw5oxKY15uBtOHJhOp3ruIBBDryMOVzSyW1jHybOdcnW/bLQDOuUfNzIDfARfROgvmq865/E/6zLy8PJef/4mHdJuGphYKig/yyvoyXttYTt2xRvrERnJJTl8uG5dB3oA+hIW1N6okItK9zKzAOZfX7r6OBHpX8KdAP1lDUwvv76xk0boyFm+p4FhjM/3iY/hcbj/mjctgZN9etP7+EhHpfgr0M3TkRBNvb61g0boyluyopKnFMSQ1jotGp3PukCQmDuhDdES412WKSAhRoHeCmiMNvL6pnEXryigoPkhziyMmMoxJAxOZNiSZ6UOSGdW3t4ZmRKRLKdA72eHjjazaU8PSwiqWF1az3ffYu4TYSM4dnMS5g1sDfkBSrIZnRKRTfVKga47eGegVE8nskWnMHpkGwIFDx1m+q5plhVUsK6zitY37AchO7snvb8hjcEqcl+WKSIhQD72TOecoqj7K0sIqHnh7B9ER4Sz81rmk9o7xujQRCQKf1EPXROtOZmYMSu7JV84ZwJM3TuLg0QZueGo1h483el2aiAQ5BXoXyslM4JEvT2RnxWFuebZAD68WkS6lQO9i5w1L4b6rclhWWM13X9hAix66ISJdRBdFu8HnJ2RScegEP39jG2m9ovnhpaO8LklEgpACvZvccl42FYeO8/jSPaTHx/CNz2R7XZKIBBkFejcxM/7fpaM4cPg4P351Kym9opmX+7El40VEzpjG0LtReJjx62tymTIoke++sJ5lhVVelyQiQUSB3s1iIsNZcH0e2clx3PxMAZvL6rwuSUSChALdA/E9Ivnj1ybTOyaCG59aTUnNUa9LEpEgoED3SHp8DH/82mQamlq4/slV1Bxp8LokEQlwCnQPDU3rxeM35FFWe4zvvbDe63JEJMAp0D02aWAit18wjHe2HWD5Ll0kFZEzp0D3A1+dNpB+8TH89LWtupNURM6YAt0PxESG872LhrNp3yEWrd/ndTkiEqAU6H5i3rgMxmT05pdv7uB4oxbxEpHTp0D3E2Fhxl0Xj2Rf7TH+sLzI63JEJAAp0P3IuUOSOX9EKg+9W6hpjCJy2hTofubOi0dwpKGJB9/Z6XUpIhJgFOh+ZmhaL66d1J9nVxSzp+qI1+WISABRoPuhf79wKFERYdz3xjavSxGRAKJA90OpvWKYPyOb1zftp6C4xutyRCRAKND91PwZ2aT2iuYnr27FOd1sJCKfToHup2KjIviPC4exZm8tr2/a73U5IhIAFOh+7Oq8LIalxfHzN7bR0NTidTki4uc6FOhmlmBmL5rZNjPbamZT2+yfaWZ1ZrbO93V315QbWsLDjDvnjqS4+ijPrij2uhwR8XMdfaboA8AbzrmrzCwKiG3nmPedc5d2XmkCMHNYCtOGJPHguzu5cmIm8T0ivS5JRPzUp/bQzaw3MAN4AsA51+Ccq+3qwqSVmXHX3JHUHWvk4X8Wel2OiPixjgy5ZAOVwFNmttbMHjeznu0cN9XM1pvZ62Y2ur0PMrP5ZpZvZvmVlZVnU3dIGd0vnivGZ/DU8iJKD+pxdSLSvo4EegQwAXjEOTceOAL8oM0xa4ABzrlxwG+Bl9v7IOfcAudcnnMuLyUl5SzKDj3fnTMcA3755navSxERP9WRQC8FSp1zK33vX6Q14D/inDvknKv3vX4NiDSz5E6tNMT1S+jB16cP4uV1Zawu0s1GIvJxnxrozrn9QImZDfdtmg1sOfkYM0s3M/O9nuz73OpOrjXk3TprCBkJPbjjbxu0ZrqIfExH56F/G/iTmW0AcoGfmtktZnaLb/9VwCYzWw88CFzndHtjp+sZHcH/fn4suyuP8IBWYxSRNjo0bdE5tw7Ia7P50ZP2/w74XSfWJacwY1gKV0/MZMGS3Vwyti9jMuK9LklE/ITuFA1AP7xkFIk9o/j+ixtobNYdpCLSSoEegOJjI/nRvDFsKT/EY//a5XU5IuInFOgB6qIx6Vwyti8PvlPIzorDXpcjIn5AgR7A7rlsNLHR4dzxtw00t+gatEioU6AHsJRe0fz350axZm8tf1xe5HU5IuIxBXqAuzw3g1nDU/jFm9vZW61lAURCmQI9wJkZP7libOtSuws36OlGIiFMgR4E+iX04M65I1hWWM1f80u8LkdEPKJADxJfmNSfKYMS+fGrW6k4dNzrckTEAwr0IBEWZvz8yhwam1v4r4WbNPQiEoIU6EFkYHJP/vPC4by9tYK/byj3uhwR6WYK9CDztemDGJeVwD2vbKa6/oTX5YhIN1KgB5nwMOMXV+Vw+Hgjdy3cqKEXkRCiQA9Cw9J6ccdFI3hzcwWPaK0XkZChQA9SX58+iM+N68cv39zOkh16fqtIKFCgBykz4+dXjmVoai++8/xaSmp0F6lIsFOgB7HYqAge+8pEmlsctzxboMfWiQQ5BXqQG5jckweuy2VL+SHuekkXSUWCmQI9BJw/Io3bZw/jpbX7eGZFsdfliEgXUaCHiG+fP4QLRqZy79+3kF9U43U5ItIFFOghIizM+PW1uWQlxvLNP63Rei8iQUiBHkJ6x0Ty6JcncuREE9/60xoamvSAaZFgokAPMcPTe3HfVTkUFB/kx69u8bocEelEEV4XIN3v0px+bCyt47Elu8nJTOCqiZlelyQinUA99BD1vc8OZ9qQJO5auJFN++q8LkdEOoECPURFhIfx4HXjSYmL5uZnCrQyo0gQUKCHsKS4aB798kSq6k9w8zMFnGjSnaQigUyBHuLGZsbz62tyyS8+yJ26k1QkoCnQhUty+vIfFw7jpTX7ePRfu70uR0TOkGa5CNB6J2nhgXrue3Mb2Sk9+ezodK9LEpHT1KEeupklmNmLZrbNzLaa2dQ2+83MHjSzQjPbYGYTuqZc6Spmxn1X5TAuM4Hbn1+nmS8iAaijQy4PAG8450YA44CtbfZfDAz1fc0HHum0CqXbxESGs+D6ifSJjeSmp/M5oOUBRALKpwa6mfUGZgBPADjnGpxztW0Omwc87VqtABLMrG+nVytdLrVXDI/fMIm6Y43c9IzWUBcJJB3poWcDlcBTZrbWzB43s55tjskASk56X+rb9n+Y2Xwzyzez/MpKPRbNX43q15v7r81lQ2kt331hvWa+iASIjgR6BDABeMQ5Nx44AvygzTHWzt/7WAo45xY45/Kcc3kpKSmnXax0nzmj07njohH8Y0M5D7yz0+tyRKQDOhLopUCpc26l7/2LtAZ822OyTnqfCZSdfXnipZtnZHPlhEzuf3snf1+v0yni7z410J1z+4ESMxvu2zQbaLtM3yvA9b7ZLucAdc658s4tVbqbmfHTz49h8sBEvvvCetaVtL10IiL+pKOzXL4N/MnMNgC5wE/N7BYzu8W3/zVgN1AI/B74VqdXKp6IjgjnkS9PILV3NDc9nU9Z7TGvSxKRUzCvLnjl5eW5/Px8T763nL4dFYe58uHlpMfH8Pz8c0iKi/a6JJGQZGYFzrm89vbp1n/pkGFpvfj9DXnsrTnKV55YRd3RRq9LEpE2FOjSYedkJ7Hg+jwKD9Rzw1OrqD/R5HVJInISBbqclvOGpfC7L45n4746vv6H1Rxr0I1HIv5CgS6nbc7odH5zbS6rimqY/0y+1lEX8RMKdDkjl43rx8+vzOH9nVXc+qe1NDa3eF2SSMhToMsZuyYvi3vnjebtrRX8+1/W0dyiJQJEvKT10OWsXD91IMcamvnf17cRExnOfVfmEBbW3koQItLVFOhy1m4+bzDHGpu5/+2dxESG8aN5YzBTqIt0NwW6dIp/mz2UYw3NPLZkNz0iw7lr7kiFukg3U6BLpzAzfnDxCI41NvP79/fQIzKcf79wmEJdpBsp0KXTmBn3fG40xxubefDdQqqONPA/l40mMlzX3kW6gwJdOlVYmPGzz+eQFBfNI+/toqjqCA9/aQIJsVFelyYS9NR1kk4XFmbccdEIfnX1OPKLDnL5Q8vYVVnvdVkiQU+BLl3myomZPDd/CoePN3H5Q8t4f6ceOyjSlRTo0qUmDkhk0W3TyEjowY1PrebpD4q8LkkkaCnQpctl9onlxW+ey6zhqdy9aDM/fHmjlgoQ6QIKdOkWcdERPPaVidx8XjbPrtjLjU+tovZog9dliQQVBbp0m/Aw486LR/LLq8exak8NVzy8XBdLRTqRAl263VUTM/nzTedQd6yRKx5axnvbD3hdkkhQUKCLJyYNTGTRrdPo57tY+uN/bNG66iJnSYEunslKjOXlW6dx/dQBPL50D1c8tJzCAxqCETlTCnTxVExkOPfOG8Pj1+dRXneMS3/7Ps+t2otzWltd5HQp0MUvXDAqjTdun0HegETufGkj33x2jWbBiJwmBbr4jbTeMTz9tcncNXcE72yr4KL73+eDXdVelyUSMBTo4lfCwoz5Mwaz8FvTiI0K54uPr+AXb27TjUgiHaBAF780JiOef3xnOtfmZfHQP3dx1aMfUFx9xOuyRPyaAl38VmxUBD+7MoeHvzSBPZX1zH3gfZ5Yuocm9dZF2qVAF783d2xf3rh9BpMHJfKjf2xh3kPLWF9S63VZIn5HgS4BoV9CD568cRIPf2kCVfUnuPzhZdy9aBOHjjd6XZqI3+hQoJtZkZltNLN1Zpbfzv6ZZlbn27/OzO7u/FIl1JkZc8f25e3/OI8bpg7kmRXFXPCrf/GPDWWaty7C6fXQZznncp1zeafY/75vf65z7t7OKE6kPb1iIrnnstEsunUaqb2jue3Pa7nxqdXsrT7qdWkintKQiwSsnMwEXv7WNO6+dBT5RTVc+Jt/8dA/C2lo0kVTCU0dDXQHvGVmBWY2/xTHTDWz9Wb2upmNbu8AM5tvZvlmll9ZqceRydmLCA/ja9MH8fZ/nses4an84s3tXPLg+ywvrPK6NJFuZx0ZezSzfs65MjNLBRYD33bOLTlpf2+gxTlXb2ZzgQecc0M/6TPz8vJcfv7HhuNFzso7Wyu4e9Fm9tUeY8awFL7/2eGMyYj3uiyRTmNmBaca+u5QD905V+b78wCwEJjcZv8h51y97/VrQKSZJZ9V1SJnYPbINN75z/P4r7kjWV9Sy6W/Xcp3nlur8XUJCZ8a6GbW08x6ffgamANsanNMupmZ7/Vk3+dqEQ7xRExkODfNyGbJ92fxrZmDeWvLfmb/+j3+e9EmqupPeF2eSJeJ6MAxacBCX15HAH92zr1hZrcAOOceBa4CvmlmTcAx4DqneWTisfgekXz/ohHccO5A7n97J8+u3MsLBaXc9JlsbpqRTVx0R/73FwkcHRpD7woaQ5futquynl+9tZ3XNu4nqWcUt50/hC9O6U90RLjXpYl02CeNoSvQJeSsK6nl569v44Pd1WT26cFts4bw+QmZREVoFq/4PwW6SBvOOZbsrOJXb21nQ2kdGQk9uHXWEK6aqGAX/6ZAFzkF5xzvba/k/nd2sr6kln7xMXxz1hCuycvUUIz4JQW6yKf4sMf+wNs7WLO3lvTeMXxz5mCunZRFTKSCXfyHAl2kg5xzLCus5oF3drC66CCpvaK55bzBfHFKfwW7+AUFushpcs7xwa5q7n9nJ6v21JAcF803PjOIL0zqT3xspNflSQhToIuchRW7q/ntuztZVlhNbFQ4V0/M5KvTBjEwuafXpUkI+qRA150VIp/inOwkzslOYnNZHU8s3cOfV+3l6RXFXDAyjW9MH8TkQYn4brwT8ZR66CKn6cCh4zz9QTHPriym9mgjYzPi+fr0QVyS05fIcE15lK6lIReRLnCsoZmX1pby5NI97Ko8QnrvGK4/dwBfnNyfhNgor8uTIKVAF+lCLS2Of+2o5Imle1haWEVMZBiX52bw5XMGaOle6XQaQxfpQmFhxqwRqcwakcrW8kP8cXkRL6/bx/OrSxjfP4Hrpw7g4jF9Ne1Rupx66CJdoO5YI38rKOXZFcXsrjpCYs8orsnL4ktT+pOVGOt1eRLANOQi4hHnHMt3VfPMB8Us3lpBi3PMGp7KV84ZwIxhKYSHaXaMnB4FuogfKK87xnOrSnhu1V4qD58gK7EH103qz5UTMkmPj/G6PAkQCnQRP9LY3MJbmyt4ZkURK3bXEGZw3rAUrs7LYvbIVC0KJp9IgS7ip4qqjvBiQSl/W1NKed1x+sRGcvn4DK7Jy2Jk395elyd+SIEu4ueaWxxLC6v4a34JizdX0NDcwtiMeK7Jy+SycRlaP0Y+okAXCSAHjzSwaN0+/pJfytbyQ0RFhPHZ0elcntuPGcNSdDdqiFOgiwSoTfvqeCG/hEXry6g92kif2EguyenLvNwMJvbvQ5hmyYQcBbpIgGtoamHJjkoWrS9j8Zb9HG9sISOhB5fl9uPy3AyGp/fyukTpJgp0kSBSf6KJxVv28/LaMpYWVtHc4hiR3ot5uRlcltuPjIQeXpcoXUiBLhKkqupP8OqGchat28eavbUATBzQh4vHpHPx2L4K9yCkQBcJAXurj/LK+n28tnE/W8oPATAuK6E13MekMyBJD+QIBgp0kRBTVHWE1zft5/VN5WworQNgdL/eH/XcB6fEeVyhnCkFukgIK6k5ypub9/PaxvKPhmWGp/XiojHpzB6Zyph+8ZotE0AU6CICtK4n88am/by+aT+ri2pwDlJ6RXP+8Nblf6cPTSYuWqtq+zMFuoh8THX9Cd7bXsm72w+wZHslh080ERUexpTsRM4fkcr5I1I17u6HFOgi8okam1vILzrIu9sqeHfbAXZVHgEgO6Uns0ekMntkGnkD+hChu1Q9p0AXkdNSXH2Ed7cd4N1tB1i5u4aG5hb6xEZy/og05oxOY8bQFHpEaVVIL5x1oJtZEXAYaAaa2n6YmRnwADAXOArc6Jxb80mfqUAXCQz1J5pYsqOStzbv591tBzh0vInoiDA+MzSFOaPSmD0ylaS4aK/LDBmd9UzRWc65qlPsuxgY6vuaAjzi+1NEAlxcdARzx/Zl7ti+NDa3sGpPDYu3VPDW5v28vbWCMGu9menCUWlcOCqdQckad/fK6fTQ804V6Gb2GPCec+453/vtwEznXPmpPlM9dJHA5pxjc9kh3tpSweItFWz13cw0KLknM4enMGt4KpMHJerh2J2sM4Zc9gAHAQc85pxb0Gb/P4CfOeeW+t6/A9zhnMtvc9x8YD5A//79JxYXF59Bc0TEH5XUHOWdrRW8t6OSD3ZVc6KphR6R4UwbksTM4anMHJ5CZh89IPtsdcaQyzTnXJmZpQKLzWybc27Jyd+jnb/zsd8Uvl8EC6C1h97B7y0iASArMZYbpw3ixmmDONbQzIrd1fxze+uF1be3HgBgWFrcR+GeNyCRqAjNmulMpz3LxczuAeqdc788aZuGXESkXc45dlUe4b3tB3hveyUr91TT2OzoGRXO1MFJzBiWwoyhKQzU2HuHnFUP3cx6AmHOucO+13OAe9sc9gpwm5k9T+vF0LpPCnMRCR1mxpDUOIakxvGNz2RTf6KJ5YVVLNlZyZIdVR/13vsnxjJjWDKfGZrCuYOT6BWjx+6dro4MuaQBC1tnJhIB/Nk594aZ3QLgnHsUeI3WKYuFtE5b/GrXlCsigS4uOoI5o9OZMzodaF1IrDXcK3lpzT6eXbGXiDBjQv8+zBiWzPShKYzp11s3NXWAbiwSEb/R0NRCQfHBjwJ+c1nrzJm46AgmDujDlOxEpgxKIiczPmSfrao7RUUkIFXVn2D5rmpW7q5m5Z4aCg/UA9AjMrw14AclMiU7iXFZ8URHhMb0SAW6iASFqvoTrNpT81HAb9t/GIDoiDDG90/gnOwkpmYnkds/IWgDXoEuIkHp4JEGVhXVsHJ3DSv3VLOl/BDOQUxkGHkDEpk6OIlzsoNriEaBLiIhofZoAyv31PDBrmpW7K7+qAffMyqcSYMSmZqdxNTBSYzuF094gD7Uo7PWchER8WsJsVF8dnQ6n/XNoKmuP8HKPTUs31XFB7uqeW97JQC9YiKYNDCRKYMSmTwokTEZwdGDV6CLSNBKiov+aGExgAOHjvPB7mpW+IZo3t3WOgc+Nur/XmTNyQzMi6wachGRkFV52HeRdU81q9q5yDp5UBKTByaS2z/Bbx7NpzF0EZEO+PAi64chv6XsEC0OwgxG9u1N3oA+TByYSN6APvRL6OFJjQp0EZEzcOh4I2v31lJQVEN+8UHW7q3lWGMzAP3iYz4K94kD+jCyb+9uudCqi6IiImegd0wk5w1L4bxhKUDrs1e3lh8iv+ggBcUHWbWnmr+vLwNaZ9KMzognJyOenKwEcjLiGZAUi2/ZlG6hHrqIyBlyzlF68BgFxQdZs/cgG0rr2FJ+iIamFgB6x0SQk5lATma87yuBvvExZxXy6qGLiHQBMyMrMZasxFguH58BtPbit+8/zMZ9dWworWNDaS0LluymqaW185wcF8XNMwZz04zsTq9HgS4i0okiw8MYkxHPmIx4vjC5ddvxxma2lh/6KORTe3fNQ7UV6CIiXSwmMpzx/fswvn+fLv0+gX9rlIiIAAp0EZGgoUAXEQkSCnQRkSChQBcRCRIKdBGRIKFAFxEJEgp0EZEg4dlaLmZWCRS32ZwMVHlQTlcJtvZA8LUp2NoDwdemYGsPnF2bBjjnUtrb4Vmgt8fM8k+16EwgCrb2QPC1KdjaA8HXpmBrD3RdmzTkIiISJBToIiJBwt8CfYHXBXSyYGsPBF+bgq09EHxtCrb2QBe1ya/G0EVE5Mz5Ww9dRETOkAJdRCRI+EWgm9lFZrbdzArN7Ade19MZzKzIzDaa2TozC8iHp5rZk2Z2wMw2nbQt0cwWm9lO359du2J/JzpFe+4xs32+87TOzOZ6WePpMLMsM/unmW01s81m9m++7YF8jk7VpoA8T2YWY2arzGy9rz3/49veJefI8zF0MwsHdgAXAqXAauALzrktnhZ2lsysCMhzzgXsDRFmNgOoB552zo3xbbsPqHHO/cz3y7ePc+4OL+vsqFO05x6g3jn3Sy9rOxNm1hfo65xbY2a9gALgcuBGAvccnapN1xCA58lanwbd0zlXb2aRwFLg34DP0wXnyB966JOBQufcbudcA/A8MM/jmgRwzi0Batpsngf80ff6j7T+sAWEU7QnYDnnyp1za3yvDwNbgQwC+xydqk0BybWq972N9H05uugc+UOgZwAlJ70vJYBP4Ekc8JaZFZjZfK+L6URpzrlyaP3hA1I9rqcz3GZmG3xDMgEzPHEyMxsIjAdWEiTnqE2bIEDPk5mFm9k64ACw2DnXZefIHwLd2tkWDHMppznnJgAXA7f6/rkv/ucRYDCQC5QDv/K2nNNnZnHA34DbnXOHvK6nM7TTpoA9T865ZudcLpAJTDazMV31vfwh0EuBrJPeZwJlHtXSaZxzZb4/DwALaR1aCgYVvnHOD8c7D3hcz1lxzlX4fuBagN8TYOfJNy77N+BPzrmXfJsD+hy116ZAP08Azrla4D3gIrroHPlDoK8GhprZIDOLAq4DXvG4prNiZj19F3Qws57AHGDTJ/+tgPERr/EAAADpSURBVPEKcIPv9Q3AIg9rOWsf/lD5XEEAnSffBbcngK3OuV+ftCtgz9Gp2hSo58nMUswswfe6B3ABsI0uOkeez3IB8E1Buh8IB550zv3E45LOipll09orB4gA/hyIbTKz54CZtC71WQH8N/Ay8FegP7AXuNo5FxAXGk/Rnpm0/jPeAUXAzR+Obfo7M5sOvA9sBFp8m++idcw5UM/Rqdr0BQLwPJlZDq0XPcNp7UD/1Tl3r5kl0QXnyC8CXUREzp4/DLmIiEgnUKCLiAQJBbqISJBQoIuIBAkFuohIkFCgi4gECQW6iEiQ+P9eePSJlT/M9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cond_lstm_w2v, batch_size=batch_size, epochs=epochs, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83fc79af-27cf-4951-9d4d-6f47b15a8e3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 - Batch: 5/91 - Loss: 9.678860664367676\n",
      "Epoch: 1/10 - Batch: 10/91 - Loss: 8.069537162780762\n",
      "Epoch: 1/10 - Batch: 15/91 - Loss: 7.24202823638916\n",
      "Epoch: 1/10 - Batch: 20/91 - Loss: 7.096155166625977\n",
      "Epoch: 1/10 - Batch: 25/91 - Loss: 7.138648509979248\n",
      "Epoch: 1/10 - Batch: 30/91 - Loss: 7.063133716583252\n",
      "Epoch: 1/10 - Batch: 35/91 - Loss: 7.059938907623291\n",
      "Epoch: 1/10 - Batch: 40/91 - Loss: 6.837062358856201\n",
      "Epoch: 1/10 - Batch: 45/91 - Loss: 6.979589939117432\n",
      "Epoch: 1/10 - Batch: 50/91 - Loss: 6.942570209503174\n",
      "Epoch: 1/10 - Batch: 55/91 - Loss: 6.926294326782227\n",
      "Epoch: 1/10 - Batch: 60/91 - Loss: 6.856889247894287\n",
      "Epoch: 1/10 - Batch: 65/91 - Loss: 6.880835056304932\n",
      "Epoch: 1/10 - Batch: 70/91 - Loss: 6.942081451416016\n",
      "Epoch: 1/10 - Batch: 75/91 - Loss: 7.2524518966674805\n",
      "Epoch: 1/10 - Batch: 80/91 - Loss: 6.8485493659973145\n",
      "Epoch: 1/10 - Batch: 85/91 - Loss: 6.947598934173584\n",
      "Epoch: 1/10 - Batch: 90/91 - Loss: 6.798511505126953\n",
      "Epoch: 1/10 - Loss: 8.894052505493164\n",
      "Epoch: 2/10 - Batch: 5/91 - Loss: 7.169904708862305\n",
      "Epoch: 2/10 - Batch: 10/91 - Loss: 6.91634464263916\n",
      "Epoch: 2/10 - Batch: 15/91 - Loss: 6.991496562957764\n",
      "Epoch: 2/10 - Batch: 20/91 - Loss: 7.028489589691162\n",
      "Epoch: 2/10 - Batch: 25/91 - Loss: 6.991413116455078\n",
      "Epoch: 2/10 - Batch: 30/91 - Loss: 6.934293746948242\n",
      "Epoch: 2/10 - Batch: 35/91 - Loss: 6.978297710418701\n",
      "Epoch: 2/10 - Batch: 40/91 - Loss: 6.778400897979736\n",
      "Epoch: 2/10 - Batch: 45/91 - Loss: 6.902254104614258\n",
      "Epoch: 2/10 - Batch: 50/91 - Loss: 6.8906378746032715\n",
      "Epoch: 2/10 - Batch: 55/91 - Loss: 6.8838629722595215\n",
      "Epoch: 2/10 - Batch: 60/91 - Loss: 6.831937313079834\n",
      "Epoch: 2/10 - Batch: 65/91 - Loss: 6.844807147979736\n",
      "Epoch: 2/10 - Batch: 70/91 - Loss: 6.9181294441223145\n",
      "Epoch: 2/10 - Batch: 75/91 - Loss: 7.210324764251709\n",
      "Epoch: 2/10 - Batch: 80/91 - Loss: 6.824921131134033\n",
      "Epoch: 2/10 - Batch: 85/91 - Loss: 6.910050868988037\n",
      "Epoch: 2/10 - Batch: 90/91 - Loss: 6.7714104652404785\n",
      "Epoch: 2/10 - Loss: 8.839794158935547\n",
      "Epoch: 3/10 - Batch: 5/91 - Loss: 7.144281387329102\n",
      "Epoch: 3/10 - Batch: 10/91 - Loss: 6.917844772338867\n",
      "Epoch: 3/10 - Batch: 15/91 - Loss: 6.987504482269287\n",
      "Epoch: 3/10 - Batch: 20/91 - Loss: 7.018039703369141\n",
      "Epoch: 3/10 - Batch: 25/91 - Loss: 6.984122276306152\n",
      "Epoch: 3/10 - Batch: 30/91 - Loss: 6.933760166168213\n",
      "Epoch: 3/10 - Batch: 35/91 - Loss: 6.968055248260498\n",
      "Epoch: 3/10 - Batch: 40/91 - Loss: 6.776178359985352\n",
      "Epoch: 3/10 - Batch: 45/91 - Loss: 6.891748428344727\n",
      "Epoch: 3/10 - Batch: 50/91 - Loss: 6.883363246917725\n",
      "Epoch: 3/10 - Batch: 55/91 - Loss: 6.870017051696777\n",
      "Epoch: 3/10 - Batch: 60/91 - Loss: 6.817829608917236\n",
      "Epoch: 3/10 - Batch: 65/91 - Loss: 6.827754497528076\n",
      "Epoch: 3/10 - Batch: 70/91 - Loss: 6.903438091278076\n",
      "Epoch: 3/10 - Batch: 75/91 - Loss: 7.187506198883057\n",
      "Epoch: 3/10 - Batch: 80/91 - Loss: 6.801870822906494\n",
      "Epoch: 3/10 - Batch: 85/91 - Loss: 6.890325546264648\n",
      "Epoch: 3/10 - Batch: 90/91 - Loss: 6.753951549530029\n",
      "Epoch: 3/10 - Loss: 8.805197715759277\n",
      "Epoch: 4/10 - Batch: 5/91 - Loss: 7.114248752593994\n",
      "Epoch: 4/10 - Batch: 10/91 - Loss: 6.902633190155029\n",
      "Epoch: 4/10 - Batch: 15/91 - Loss: 6.975879669189453\n",
      "Epoch: 4/10 - Batch: 20/91 - Loss: 7.0060601234436035\n",
      "Epoch: 4/10 - Batch: 25/91 - Loss: 6.971417427062988\n",
      "Epoch: 4/10 - Batch: 30/91 - Loss: 6.909450531005859\n",
      "Epoch: 4/10 - Batch: 35/91 - Loss: 6.950551986694336\n",
      "Epoch: 4/10 - Batch: 40/91 - Loss: 6.779067516326904\n",
      "Epoch: 4/10 - Batch: 45/91 - Loss: 6.868890285491943\n",
      "Epoch: 4/10 - Batch: 50/91 - Loss: 6.867750644683838\n",
      "Epoch: 4/10 - Batch: 55/91 - Loss: 6.862396717071533\n",
      "Epoch: 4/10 - Batch: 60/91 - Loss: 6.80642032623291\n",
      "Epoch: 4/10 - Batch: 65/91 - Loss: 6.817455291748047\n",
      "Epoch: 4/10 - Batch: 70/91 - Loss: 6.8855462074279785\n",
      "Epoch: 4/10 - Batch: 75/91 - Loss: 7.176267623901367\n",
      "Epoch: 4/10 - Batch: 80/91 - Loss: 6.799994468688965\n",
      "Epoch: 4/10 - Batch: 85/91 - Loss: 6.886174201965332\n",
      "Epoch: 4/10 - Batch: 90/91 - Loss: 6.745874404907227\n",
      "Epoch: 4/10 - Loss: 8.811700820922852\n",
      "Epoch: 5/10 - Batch: 5/91 - Loss: 7.09765625\n",
      "Epoch: 5/10 - Batch: 10/91 - Loss: 6.899590969085693\n",
      "Epoch: 5/10 - Batch: 15/91 - Loss: 6.9666643142700195\n",
      "Epoch: 5/10 - Batch: 20/91 - Loss: 6.9952616691589355\n",
      "Epoch: 5/10 - Batch: 25/91 - Loss: 6.964896202087402\n",
      "Epoch: 5/10 - Batch: 30/91 - Loss: 6.914972305297852\n",
      "Epoch: 5/10 - Batch: 35/91 - Loss: 6.952923774719238\n",
      "Epoch: 5/10 - Batch: 40/91 - Loss: 6.769251823425293\n",
      "Epoch: 5/10 - Batch: 45/91 - Loss: 6.866391658782959\n",
      "Epoch: 5/10 - Batch: 50/91 - Loss: 6.872122287750244\n",
      "Epoch: 5/10 - Batch: 55/91 - Loss: 6.859158515930176\n",
      "Epoch: 5/10 - Batch: 60/91 - Loss: 6.800203323364258\n",
      "Epoch: 5/10 - Batch: 65/91 - Loss: 6.816092014312744\n",
      "Epoch: 5/10 - Batch: 70/91 - Loss: 6.886295318603516\n",
      "Epoch: 5/10 - Batch: 75/91 - Loss: 7.170736789703369\n",
      "Epoch: 5/10 - Batch: 80/91 - Loss: 6.790863513946533\n",
      "Epoch: 5/10 - Batch: 85/91 - Loss: 6.880313873291016\n",
      "Epoch: 5/10 - Batch: 90/91 - Loss: 6.745350360870361\n",
      "Epoch: 5/10 - Loss: 8.789963722229004\n",
      "Epoch: 6/10 - Batch: 5/91 - Loss: 7.098630428314209\n",
      "Epoch: 6/10 - Batch: 10/91 - Loss: 6.888795852661133\n",
      "Epoch: 6/10 - Batch: 15/91 - Loss: 6.958485126495361\n",
      "Epoch: 6/10 - Batch: 20/91 - Loss: 6.992584705352783\n",
      "Epoch: 6/10 - Batch: 25/91 - Loss: 6.952592372894287\n",
      "Epoch: 6/10 - Batch: 30/91 - Loss: 6.898380279541016\n",
      "Epoch: 6/10 - Batch: 35/91 - Loss: 6.957924842834473\n",
      "Epoch: 6/10 - Batch: 40/91 - Loss: 6.762945175170898\n",
      "Epoch: 6/10 - Batch: 45/91 - Loss: 6.857302188873291\n",
      "Epoch: 6/10 - Batch: 50/91 - Loss: 6.858919143676758\n",
      "Epoch: 6/10 - Batch: 55/91 - Loss: 6.854857921600342\n",
      "Epoch: 6/10 - Batch: 60/91 - Loss: 6.798473358154297\n",
      "Epoch: 6/10 - Batch: 65/91 - Loss: 6.816227436065674\n",
      "Epoch: 6/10 - Batch: 70/91 - Loss: 6.88223123550415\n",
      "Epoch: 6/10 - Batch: 75/91 - Loss: 7.161318302154541\n",
      "Epoch: 6/10 - Batch: 80/91 - Loss: 6.791202545166016\n",
      "Epoch: 6/10 - Batch: 85/91 - Loss: 6.872811317443848\n",
      "Epoch: 6/10 - Batch: 90/91 - Loss: 6.733336925506592\n",
      "Epoch: 6/10 - Loss: 8.767799377441406\n",
      "Epoch: 7/10 - Batch: 5/91 - Loss: 7.082877159118652\n",
      "Epoch: 7/10 - Batch: 10/91 - Loss: 6.88870096206665\n",
      "Epoch: 7/10 - Batch: 15/91 - Loss: 6.955556869506836\n",
      "Epoch: 7/10 - Batch: 20/91 - Loss: 6.979147911071777\n",
      "Epoch: 7/10 - Batch: 25/91 - Loss: 6.950537204742432\n",
      "Epoch: 7/10 - Batch: 30/91 - Loss: 6.8988728523254395\n",
      "Epoch: 7/10 - Batch: 35/91 - Loss: 6.936731338500977\n",
      "Epoch: 7/10 - Batch: 40/91 - Loss: 6.759393692016602\n",
      "Epoch: 7/10 - Batch: 45/91 - Loss: 6.8466081619262695\n",
      "Epoch: 7/10 - Batch: 50/91 - Loss: 6.855842113494873\n",
      "Epoch: 7/10 - Batch: 55/91 - Loss: 6.8504958152771\n",
      "Epoch: 7/10 - Batch: 60/91 - Loss: 6.787057876586914\n",
      "Epoch: 7/10 - Batch: 65/91 - Loss: 6.806417942047119\n",
      "Epoch: 7/10 - Batch: 70/91 - Loss: 6.874710559844971\n",
      "Epoch: 7/10 - Batch: 75/91 - Loss: 7.141740798950195\n",
      "Epoch: 7/10 - Batch: 80/91 - Loss: 6.779275894165039\n",
      "Epoch: 7/10 - Batch: 85/91 - Loss: 6.8631720542907715\n",
      "Epoch: 7/10 - Batch: 90/91 - Loss: 6.719142913818359\n",
      "Epoch: 7/10 - Loss: 8.767607688903809\n",
      "Epoch: 8/10 - Batch: 5/91 - Loss: 7.052412509918213\n",
      "Epoch: 8/10 - Batch: 10/91 - Loss: 6.846004009246826\n",
      "Epoch: 8/10 - Batch: 15/91 - Loss: 6.9092698097229\n",
      "Epoch: 8/10 - Batch: 20/91 - Loss: 6.9254655838012695\n",
      "Epoch: 8/10 - Batch: 25/91 - Loss: 6.892627716064453\n",
      "Epoch: 8/10 - Batch: 30/91 - Loss: 6.83613395690918\n",
      "Epoch: 8/10 - Batch: 35/91 - Loss: 6.875523090362549\n",
      "Epoch: 8/10 - Batch: 40/91 - Loss: 6.693435192108154\n",
      "Epoch: 8/10 - Batch: 45/91 - Loss: 6.765935897827148\n",
      "Epoch: 8/10 - Batch: 50/91 - Loss: 6.756779670715332\n",
      "Epoch: 8/10 - Batch: 55/91 - Loss: 6.740774154663086\n",
      "Epoch: 8/10 - Batch: 60/91 - Loss: 6.671319484710693\n",
      "Epoch: 8/10 - Batch: 65/91 - Loss: 6.676849365234375\n",
      "Epoch: 8/10 - Batch: 70/91 - Loss: 6.744997024536133\n",
      "Epoch: 8/10 - Batch: 75/91 - Loss: 6.990650177001953\n",
      "Epoch: 8/10 - Batch: 80/91 - Loss: 6.625197887420654\n",
      "Epoch: 8/10 - Batch: 85/91 - Loss: 6.7183685302734375\n",
      "Epoch: 8/10 - Batch: 90/91 - Loss: 6.541325092315674\n",
      "Epoch: 8/10 - Loss: 8.55003547668457\n",
      "Epoch: 9/10 - Batch: 5/91 - Loss: 6.8791937828063965\n",
      "Epoch: 9/10 - Batch: 10/91 - Loss: 6.679197788238525\n",
      "Epoch: 9/10 - Batch: 15/91 - Loss: 6.742008686065674\n",
      "Epoch: 9/10 - Batch: 20/91 - Loss: 6.74077033996582\n",
      "Epoch: 9/10 - Batch: 25/91 - Loss: 6.697413444519043\n",
      "Epoch: 9/10 - Batch: 30/91 - Loss: 6.6531243324279785\n",
      "Epoch: 9/10 - Batch: 35/91 - Loss: 6.679147720336914\n",
      "Epoch: 9/10 - Batch: 40/91 - Loss: 6.4744768142700195\n",
      "Epoch: 9/10 - Batch: 45/91 - Loss: 6.573887825012207\n",
      "Epoch: 9/10 - Batch: 50/91 - Loss: 6.548074245452881\n",
      "Epoch: 9/10 - Batch: 55/91 - Loss: 6.54642391204834\n",
      "Epoch: 9/10 - Batch: 60/91 - Loss: 6.4671502113342285\n",
      "Epoch: 9/10 - Batch: 65/91 - Loss: 6.473516941070557\n",
      "Epoch: 9/10 - Batch: 70/91 - Loss: 6.5406694412231445\n",
      "Epoch: 9/10 - Batch: 75/91 - Loss: 6.768945693969727\n",
      "Epoch: 9/10 - Batch: 80/91 - Loss: 6.43193244934082\n",
      "Epoch: 9/10 - Batch: 85/91 - Loss: 6.5438456535339355\n",
      "Epoch: 9/10 - Batch: 90/91 - Loss: 6.342529296875\n",
      "Epoch: 9/10 - Loss: 8.089664459228516\n",
      "Epoch: 10/10 - Batch: 5/91 - Loss: 6.694686412811279\n",
      "Epoch: 10/10 - Batch: 10/91 - Loss: 6.506821155548096\n",
      "Epoch: 10/10 - Batch: 15/91 - Loss: 6.596604824066162\n",
      "Epoch: 10/10 - Batch: 20/91 - Loss: 6.592665195465088\n",
      "Epoch: 10/10 - Batch: 25/91 - Loss: 6.562221527099609\n",
      "Epoch: 10/10 - Batch: 30/91 - Loss: 6.497979164123535\n",
      "Epoch: 10/10 - Batch: 35/91 - Loss: 6.530149936676025\n",
      "Epoch: 10/10 - Batch: 40/91 - Loss: 6.312530994415283\n",
      "Epoch: 10/10 - Batch: 45/91 - Loss: 6.430561065673828\n",
      "Epoch: 10/10 - Batch: 50/91 - Loss: 6.404323577880859\n",
      "Epoch: 10/10 - Batch: 55/91 - Loss: 6.401872158050537\n",
      "Epoch: 10/10 - Batch: 60/91 - Loss: 6.3307414054870605\n",
      "Epoch: 10/10 - Batch: 65/91 - Loss: 6.336245059967041\n",
      "Epoch: 10/10 - Batch: 70/91 - Loss: 6.402529239654541\n",
      "Epoch: 10/10 - Batch: 75/91 - Loss: 6.552293300628662\n",
      "Epoch: 10/10 - Batch: 80/91 - Loss: 6.291318893432617\n",
      "Epoch: 10/10 - Batch: 85/91 - Loss: 6.421512603759766\n",
      "Epoch: 10/10 - Batch: 90/91 - Loss: 6.207857131958008\n",
      "Epoch: 10/10 - Loss: 7.424000263214111\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc9ElEQVR4nO3deXxU9b3/8ddnZrITEiBhhwRc2GSJRFncWrXu1WqrQlUorbW2daG1vV1ve3t729tffz+tWG/lWqqiF8EW8Wexaq221SqKBoislrIT1gQSCNmX7/1jBgxpJAlMcmbOvJ+PRx4zc+ZkzrtTeZ+TM985X3POISIi8S/gdQAREYkOFbqIiE+o0EVEfEKFLiLiEyp0ERGfCHm14ZycHJefn+/V5kVE4tKKFSvKnHO5bT3nWaHn5+dTVFTk1eZFROKSmW3/qOd0ykVExCdU6CIiPqFCFxHxCRW6iIhPqNBFRHxChS4i4hMqdBERn4i7Qt9ceoR/X7qehqZmr6OIiMSUuCv0HQeqeeytrfxx3V6vo4iIxJS4K/SLzsxlaO90nlz2kV+WEhFJSHFX6IGAMWNKHu9uO8j63Ye9jiMiEjPirtABbpw4hNSkAE+9s83rKCIiMSMuCz0rPYnrCwbx3KpdHKpu8DqOiEhMiMtCB7htcj61Dc38bsVOr6OIiMSEuC300QN7cm5+b558ezvNzc7rOCIinovbQgeYMTWPHQereX1jqddRREQ8F9eFfvmY/vTNTGH+29u8jiIi4rm4LvSkYIBbJuXx17+Xsq2syus4IiKeiutCB5g+aQhJQeOpd/RFIxFJbHFf6H0zU7nyrAH8tmgn1fWNXscREfFM3Bc6wIwpeVTWNvJ88W6vo4iIeMYXhT4xrxejB/Rk/rJtOKchjCKSmHxR6GbGzKl5fLC3kve2lXsdR0TEE74odIBrxw8iKy1JQxhFJGH5ptDTkoPcfM4Q/rh2L3sP1XodR0Sk2/mm0AFunZRHk3M8/e4Or6OIiHQ7XxX60D7pXDyiL08v30F9o6aoE5HE4qtCB5gxNZ+yI3W8tHaP11FERLqV7wr9gtNzGJaTwZNv65ujIpJYfFfogYBx2+Q8VmwvZ+2uQ17HERHpNr4rdIBPTxxMenKQJ9/e5nUUEZFu48tCz0oLT1H3fPFuyqvqvY4jItItfFnoADOm5FPX2MxvizRFnYgkBt8W+oj+mUwe3pun3tlOk6aoE5EE4NtCB5g5JZ+S8hr+8sF+r6OIiHQ5Xxf6J0b3o3/PVF3fRUQSQruFbmYjzKy4xc9hM5vdap1bzGx15GeZmY3vusgdFwoGuGXSUP72jzK2lB7xOo6ISJdqt9Cdc393zk1wzk0AJgLVwHOtVtsKXOScGwf8GHg06klP0rRzh2qKOhFJCJ095XIJsNk5d1w7OueWOeeOXoj8HWBwNMJFQ25mClePHcDiohKq6jRFnYj4V2cLfRqwsJ11vgC81NYTZnaHmRWZWVFpaWknN33yZkzNp7KukedW7eq2bYqIdLcOF7qZJQPXAr87wTofJ1zo32rreefco865QudcYW5ubmeznrSCIdmMHZTFk29rijoR8a/OHKFfCax0zu1r60kzGwfMA65zzh2IRrhoMTNmTMlj474jvLPloNdxRES6RGcKfTofcbrFzIYCS4DbnHMboxEs2j45fiC90pN0fRcR8a0OFbqZpQOfIFzaR5fdaWZ3Rh7+AOgD/CoytLEo6klPUWpSkJvPGcor6/exu6LG6zgiIlHXoUJ3zlU75/o45w61WDbXOTc3cv9251yvo8MbnXOFXRX4VNwyaSjOOZ5erinqRMR/fP1N0daG9E7nklH9WPjuDuoam7yOIyISVQlV6BC+vsuBqnpeXKMp6kTEXxKu0M87vQ/DczOYv0zfHBURf0m4QjczZk7Jp3hnBe/vrPA6johI1CRcoQPccPYgMpKDmkhaRHwlIQs9MzWJG84ezNLVuzlwpM7rOCIiUZGQhQ4wY0oe9Y3NPKMp6kTEJxK20M/ol8nU0/qw4J0dNDY1ex1HROSUJWyhQ3gi6V0VNbymKepExAcSutAvHdWXgVmpPKUPR0XEBxK60EPBALdMzuPNTWVs2l/pdRwRkVOS0IUOMO2cISQHAzpKF5G4l/CF3qdHCteMH8DiFSVU1jZ4HUdE5KQlfKFD+PouVfVNmqJOROKaCh0YPySb8UOymb9MU9SJSPxSoUfMnJLH5tIqlm2OqdnzREQ6TIUecdXYAfTJSGb+sm1eRxEROSkq9IjUpCDTzh3Cqxv2UVJe7XUcEZFOU6G3cMukPAAWaIo6EYlDKvQWBmancdno/ix6dwe1DZqiTkTiiwq9lRlT8iivbuCF1ZqiTkTiiwq9lSmn9eH0vj00hFFE4o4KvZXwFHV5rNl1iGJNUScicUSF3obrzx5Mj5SQpqgTkbiiQm9Dj5QQn5k4mD+s3kNppaaoE5H4oEL/CLdOzqO+qZln3tMQRhGJDyr0j3B63x5ccEYOC5ZrijoRiQ8q9BOYMSWfPYdqeXXDPq+jiIi0S4V+AheP7Mug7DTmL9OHoyIS+1ToJxAMGLdNyePtLQfYuE9T1IlIbFOht+PmwiGkhAI8+fY2r6OIiJyQCr0dvTKSuXb8QJas3MVhTVEnIjFMhd4BM6fmU13fxLMrSryOIiLykVToHXDWoCzOHprNU29vp7lZ13cRkdikQu+gmVPz2VJWxZubyryOIiLSJhV6B11xVn9yeiTrw1ERiVkq9A5KCQWZfu5QXvtgPzsPaoo6EYk9KvRO+OykoQTM+J939EUjEYk97Ra6mY0ws+IWP4fNbHardczMHjKzTWa22szO7rrI3hmQlcblY/qx6L2d1NRrijoRiS3tFrpz7u/OuQnOuQnARKAaeK7ValcCZ0R+7gAeiXbQWDFjSj6HahpY+v5ur6OIiByns6dcLgE2O+dan3O4DnjShb0DZJvZgKgkjDGThvVmRL9MntAUdSISYzpb6NOAhW0sHwTsbPG4JLLsOGZ2h5kVmVlRaWlpJzcdG8yMGVPzWL/nMCt3lHsdR0TkmA4XupklA9cCv2vr6TaW/dPhq3PuUedcoXOuMDc3t+MpY8ynJgwiMzXEI3/dzPrdhymtrKNJXzgSEY+FOrHulcBK51xbFwcvAYa0eDwY8O1J5oyUELdOzuORv27m1Q37AQgY9M5IITczhZweyeRmhu/n9ji67MPH2elJmLW1DxQROXmdKfTptH26BeD3wF1mtgiYBBxyzu051XCx7JuXjeDSUf0orayltLIu/HOkntLKOsqO1LGltIrSI3XUN/7zbEehgB0r+Jbl37L0cyLLMlNCKn8R6ZAOFbqZpQOfAL7UYtmdAM65ucCLwFXAJsKjYGZFPWmMCQSMiXm9TriOc47KusZjhV92pO6f7x+pY8OeSsqO1NHYxmmblFCgRfmntDjyP35HkNMjhYyUzuyfRcRvzKuRGoWFha6oqMiTbcei5mZHRU1D26UfKf6jyw5U1dPW/21pSUFyMpPDxR85yg/fDy/LabFTyEgO6shfJA6Z2QrnXGFbz+mQLkYEAkbvjGR6ZyRzZr/ME67b2NTMwer6FsVfT9mROsqO7gSO1LH9QDUrtpdzsLrt8k9NChw7sg+XfHKL+0eXJ5Oj0z4icUOFHodCwQB9M1Ppm5na7rpHy7+ssp7SFqUf/gnvCErKqyneWc7BqnraGqyTHAqEj/gjp3k+3BEkHzvqz+mRQs/UEClJQVKTAiQHA9oJiHQzFbrPdab8m5odB6vqjxV+aWWL4o+c9tlVUUvxzkMcrKprs/yPMguf/09NCh67TQ0FSUkKHLtNCYXL/7h1PmJ5Sovf/ah1UpOCBAPaiUjiUqHLMcGAHfugtT1NzY7y6qOnesK3lXWN1DU0UdfYTG2L2+PvN1PX2ERlbSOlDeFRQLUNTdQ2NlMXuT2VMf2hgJGWHCQjOUR6cvDY/bTkIBkpQdKSwsvTU4KkJ4XCy5KD4WWR3zl6e/T30pODpCUFCWhnITFOhS4nJRgZepnTIwX6R/e1G5qaT7gzqIvc1ja03nE0U9vYRE19E9X1jVTXN0V+GqmormdXxYfPVdU3tTmk9ETSkoIf7gCSQuGdwnE7guN3Cpmp4Z+eaUn0TA3RMzWJzNQkMlPD6+iUlESbCl1iTlIwQFIwQI8uHobZ2NRMdcPRkm+iqq6RmobIbYudQXV9E1X1TdS02kkcvX+wqubY45r6JqrqG9v8ILqlYMDCZR8p+MwWhd8zLRS+bfF8z7Sj64WXZ6YmkRzS1a/leCp0SVihYICewQA9U5Oi+rrOOWobmqmsbeBwbSOHaxuorG0MP66J3EaWHa6J3NY2sONg9YfL6hrb3U5qUuDYEX/L4u/ZovgH90rnmnEDCAVV/olAhS4SZWbh8/hpyUH69jy512hqdhypa70TCN+2LP2WO4TDtY3sqqg5tn5d5JTSr/+2hf/z6XGcNSgriv8rJRap0EViUDBgZKUlkZWWBCf+QvJHqmts4tX1+/m3peu47r/e4vbzhzH70jNJSw5GN6zEDP0dJuJTKaEgV48bwKtfu4gbJw7mv9/YwuUPvsFbm8q8jiZdRIUu4nNZ6Un87NPjePqLkwgY3DJvOd/43ftUVNd7HU2iTIUukiCmnpbDy7Mv5CsfO43nVu3i0gde5/fv79bMWz6iQhdJIKlJQf7lipEsvet8Bmancc/CVXxhfhG7Kmq8jiZRoEIXSUCjB/ZkyZen8v2rR/H25gNc9sDrzF+2TTNvxTkVukiCCgUD3H7BcF752oWcndeLH/5+HTfOXcbGfZVeR5OTpEIXSXBDeqfz5OfP5Rc3j2drWRVXP/Q3HvjTRuoam7yOJp2kQhcRzIzrCwbz6tcv4uqxA3jotX9w1Zy/UbTtoNfRpBNU6CJyTJ8eKTw4rYDHZ51DbUMzn5n7Nt///2uorG3wOpp0gApdRP7Jx0f05ZWvXcjnzxvGguU7+MQDb/DKur1ex5J2qNBFpE0ZKSF+8MnRLPnyVLLTk7jjqRV8ZcEK9lfWeh1NPoIKXUROqGBoL5befT7fvHwEr27Yz6X3v84z7+3QF5JikApdRNqVFAzw1Y+fzkv3XsDIAT351rNr+Oyvl7O1rMrraNKCCl1EOuy03B4s+uJk/vOGsazdfYgrHnyDX/11Ew1NnZv9SbqGCl1EOiUQMKafO5RXv34RHx/Rl5+//HeuffgtVpdUeB0t4anQReSk9OuZytzbJjL31okcOFLHp/7rLf7jhfVU17c/25J0DRW6iJySK87qz5++fhHTzh3KvDe3ctkv3uCNjaVex0pIKnQROWVZaUn89PqxPHPHZJJDAWY89i5ff6aYg1W65np3UqGLSNRMGt6HF++5gLsvPp3fv7+bSx94neeLd2mIYzdRoYtIVKUmBbnvshG8cM/5DO2dzr2Livnq0yt1ad5uoEIXkS4xsn9Pnv3yVO77xJm8uGYvc1/f7HUk31Ohi0iXCQaMuy4+nWvGDeCBP21kxXZdvbErqdBFpEuZGT+9YSwDs1O5Z2Exh2p05cauokIXkS7XMzWJh6YVsO9wLd9ZslofknYRFbqIdIuCob2477IRvLhmLwvf3el1HF9SoYtIt/nShcO54IwcfrR0neYu7QIqdBHpNoGAcf9N48lMDXHX0yupbdC8pdGkQheRbtU3M5X7b5rAxn1H+PEL672O4ysqdBHpdhedmcsdFw5nwfIdvLRmj9dxfKNDhW5m2Wa22Mw+MLMNZjal1fNZZrbUzN43s3VmNqtr4oqIX3zjshGMH5zFt55dTUl5tddxfKGjR+hzgJedcyOB8cCGVs9/FVjvnBsPfAy438ySo5ZSRHwnORTgoekFNDuYvaiYRk2SccraLXQz6wlcCPwGwDlX75xrfSV7B2SamQE9gIOALoosIieU1yeDn1x/FkXby5nz2j+8jhP3OnKEPhwoBR43s1VmNs/MMlqt8zAwCtgNrAHudc790+7WzO4wsyIzKyot1fWSRQSumzCIGycO5uG/bGLZ5jKv48S1jhR6CDgbeMQ5VwBUAd9utc7lQDEwEJgAPBw5sj+Oc+5R51yhc64wNzf31JKLiG/86LoxDMvJYPaiYg4cqfM6TtzqSKGXACXOueWRx4sJF3xLs4AlLmwTsBUYGb2YIuJn6ckhfjm9gIrqBr65WJcGOFntFrpzbi+w08xGRBZdArQePLojshwz6weMALZEMaeI+NyYgVl896qR/PmD/Tz21jav48SlUAfXuxtYEBm5sgWYZWZ3Ajjn5gI/Bp4wszWAAd9yzulkmIh0ysyp+by56QA/e2kDk4b15qxBWV5Hiivm1Z82hYWFrqioyJNti0jsKq+q58o5fyMtOcjSu8+nR0pHjzsTg5mtcM4VtvWcvikqIjGlV0YyD06bwPYDVfzg+bVex4krKnQRiTmTh/fhrovPYMnKXTy3qsTrOHFDhS4iMemei0/n3PzefP+5tWwtq/I6TlxQoYtITAoFAzw4bQKhYIB7Fq6ivlGXBmiPCl1EYtbA7DR+/plxrNl1iJ+//IHXcWKeCl1EYtrlY/ozY0oe897cyl/+vt/rODFNhS4iMe+7V41iZP9MvvHb99l/uNbrODFLhS4iMS81KcjDny2gur6J2c8U09SsSwO0RYUuInHh9L6Z/Nu1o1m2+QBzX9/sdZyYpEIXkbhxU+EQrhk3gAf+tJEV2w96HSfmqNBFJG6YGT+9YSwDs1O5Z2Exh2oavI4UU1ToIhJXeqYm8dC0AvYdruU7S3Sp3ZZU6CISdwqG9uK+y0bw4pq9LHx3p9dxYoYKXUTi0pcuHM4FZ+Two6Xr2Liv0us4MUGFLiJxKRAw7r9pPJmpIe56eiW1DU1eR/KcCl1E4lbfzFTuv2kCG/cd4ccvtJ5ILfGo0EUkrl10Zi5funA4C5bv4KU1e7yO4ykVuojEvfsuG8H4wVl869nVlJRXex3HMyp0EYl7yaEAv5x+Ns0OZi8qprEpMS+1q0IXEV8Y2iedn1x/FkXby5nz2j+8juMJFbqI+MZ1EwZx48TBPPyXTSzbXOZ1nG6nQhcRX/nRdWMYlpPB7EXFHDhS53WcbqVCFxFfSU8O8cvpBVRUN/DNxYl1aQAVuoj4zpiBWXz3qpH8+YP9PPbWNq/jdBsVuoj40syp+Vw6qh8/e2kDa3cd8jpOt1Chi4gvmRn/9zPj6JORwt0LV3GkrtHrSF1OhS4ivtUrI5k50yaw/UAVP3h+rddxupwKXUR8bdLwPtx98RksWbmLZ1eUeB2nS6nQRcT37r74dCYN6813nltD0Tb/Tl2nQhcR3wsFA8y9dSKDstO4/ckitpQe8TpSl1Chi0hC6JWRzBOzziFoxucef48yH37pSIUuIgkjr08G82YWsr+yltvnF1FT769JMVToIpJQCob2Ys60At4vqeDeRatoavbPN0lV6CKScC4f058fXjOaV9bv89VMRyGvA4iIeOFz5w1jZ3kNv3lzK0N6p/OF84d5HemUqdBFJGF976pR7K6o4T/+sJ6BWalcOXaA15FOiU65iEjCCgSMX9w8gYIh2cx+ppgV2+N7jLoKXUQSWmpSkHkzz2FAViq3zy9ia1mV15FOWocK3cyyzWyxmX1gZhvMbEob63zMzIrNbJ2ZvR79qCIiXaN3RjJPzDoXM+Nzj78btxNjdPQIfQ7wsnNuJDAe2NDySTPLBn4FXOucGwPcGNWUIiJdLD8ng1/PKGTvoVpuf7KI2ob4G6PebqGbWU/gQuA3AM65eudcRavVPgsscc7tiKyzP9pBRUS62sS8XsyZNoHinfE5Rr0jR+jDgVLgcTNbZWbzzCyj1TpnAr3M7K9mtsLMZrT1QmZ2h5kVmVlRaWnpKUYXEYm+K84awL9ePZo/rtvHT/6wof1fiCEdKfQQcDbwiHOuAKgCvt3GOhOBq4HLgX81szNbv5Bz7lHnXKFzrjA3N/fUkouIdJHPnz+Mz583jMfe2spjb271Ok6HdaTQS4AS59zyyOPFhAu+9TovO+eqnHNlwBuEz7WLiMSl7109isvH9OPHf1jPy2v3eh2nQ9otdOfcXmCnmY2ILLoEaP1d2eeBC8wsZGbpwCRafXAqIhJPggHjwZsLmDAkm3sXrWLljnKvI7Wro6Nc7gYWmNlqYALwUzO708zuBHDObQBeBlYD7wLznHP+n+9JRHwtLTnIvBmF9I+MUd8W42PUzTlvPsUtLCx0RUVFnmxbRKQztpZVccOv3iIrLYklXzmP3hnJnmUxsxXOucK2ntM3RUVE2jEsJ3wd9d2Harl9/nsxO0ZdhS4i0gET83oz5+YJrNpZwdeeKaY5Bseoq9BFRDroyrED+N5Vo3hp7V5++mLsjfvQ5XNFRDrhC+cPo6S8hnlvbmVQrzRmnRc711FXoYuIdIKZ8a/XjGZ3RQ3//sJ6BmancfmY/l7HAnTKRUSk04IBY860AsYNDo9RXxUjY9RV6CIiJyEtOchvZhbSNzM8Rn37Ae/HqKvQRUROUk6PFJ6YdQ5NzvG5x9/jYFW9p3lU6CIip2B4bg/mzShkV0UNX/T4OuoqdBGRU1SY35tf3DSBFdvLue+373s2Rl2jXEREouDqcQPYXTGKn7y4gUG90vjuVaO6PYMKXUQkSm6/YBgl5dU8+sYWBmWnMXNqfrduX4UuIhIlZsYPPjmGXRW1/GjpOgZkpXJZN45R1zl0EZEoCgaMX04vYOygLO5ZtIrina2nYO46KnQRkShLSw4yb+Y55GamcPv899hxoLpbtqtCFxHpArmZKTwx61wamx2fe+JdyrthjLoKXUSki5yW24NfzyikpLyGO57q+jHqKnQRkS50Tn5v7r9xPO9tK+e+33XtGHWNchER6WKfHD+Q3RU1/OdLHzA4O43vdNEYdRW6iEg3uOPC4ZSU1/Dfb2xhcK80bpuSH/VtqNBFRLqBmfHDT46moqaBAVlpXbINFbqISDcJBQP8cnpBl72+PhQVEfEJFbqIiE+o0EVEfEKFLiLiEyp0ERGfUKGLiPiECl1ExCdU6CIiPmHOeTOZqZmVAts92Xj05ABlXoeIIXo/jqf340N6L453Ku9HnnMut60nPCt0PzCzIudcodc5YoXej+Pp/fiQ3ovjddX7oVMuIiI+oUIXEfEJFfqpedTrADFG78fx9H58SO/F8brk/dA5dBERn9ARuoiIT6jQRUR8QoV+EsxsiJn9xcw2mNk6M7vX60xeM7Ogma0ysxe8zuI1M8s2s8Vm9kHkv5EpXmfykpl9LfLvZK2ZLTSzVK8zdScze8zM9pvZ2hbLepvZn8zsH5HbXtHYlgr95DQC9znnRgGTga+a2WiPM3ntXmCD1yFixBzgZefcSGA8Cfy+mNkg4B6g0Dl3FhAEpnmbqts9AVzRatm3gdecc2cAr0UenzIV+klwzu1xzq2M3K8k/A92kLepvGNmg4GrgXleZ/GamfUELgR+A+Ccq3fOVXibynMhIM3MQkA6sNvjPN3KOfcGcLDV4uuA+ZH784FPRWNbKvRTZGb5QAGw3NsknnoQ+Beg2esgMWA4UAo8HjkFNc/MMrwO5RXn3C7g/wE7gD3AIefcK96mign9nHN7IHyACPSNxouq0E+BmfUAngVmO+cOe53HC2Z2DbDfObfC6ywxIgScDTzinCsAqojSn9PxKHJu+DpgGDAQyDCzW71N5V8q9JNkZkmEy3yBc26J13k8dB5wrZltAxYBF5vZ/3gbyVMlQIlz7uhfbIsJF3yiuhTY6pwrdc41AEuAqR5nigX7zGwAQOR2fzReVIV+EszMCJ8j3eCce8DrPF5yzn3HOTfYOZdP+MOuPzvnEvYIzDm3F9hpZiMiiy4B1nsYyWs7gMlmlh75d3MJCfwhcQu/B2ZG7s8Eno/Gi4ai8SIJ6DzgNmCNmRVHln3XOfeih5kkdtwNLDCzZGALMMvjPJ5xzi03s8XASsKjw1aRYJcBMLOFwMeAHDMrAX4I/Az4rZl9gfBO78aobEtf/RcR8QedchER8QkVuoiIT6jQRUR8QoUuIuITKnQREZ9QoYuI+IQKXUTEJ/4XPKmVx0HJnuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cond_lstm, batch_size=batch_size, epochs=epochs, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89627b-3be6-44f7-9fda-4fdf55d6c71f",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8800402e-da35-4466-9735-896cf7640a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cond_lstm_w2v.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc5115f3-1009-448a-9560-4200dc90928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_lstm_w2v.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b514b226-8530-4fcf-9bf1-1b91cef49268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CondLSTM_Word2Vec(\n",
       "  (emb_layer): Embedding(17862, 100)\n",
       "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_lstm_w2v.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f627ac-1cca-4628-97a9-731307742eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_layer.weight \t torch.Size([17862, 100])\n",
      "lstm.weight_ih_l0 \t torch.Size([1024, 100])\n",
      "lstm.weight_hh_l0 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 \t torch.Size([1024])\n",
      "lstm.bias_hh_l0 \t torch.Size([1024])\n",
      "lstm.weight_ih_l1 \t torch.Size([1024, 256])\n",
      "lstm.weight_hh_l1 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l1 \t torch.Size([1024])\n",
      "lstm.bias_hh_l1 \t torch.Size([1024])\n",
      "lstm.weight_ih_l2 \t torch.Size([1024, 256])\n",
      "lstm.weight_hh_l2 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l2 \t torch.Size([1024])\n",
      "lstm.bias_hh_l2 \t torch.Size([1024])\n",
      "fc.weight \t torch.Size([17862, 256])\n",
      "fc.bias \t torch.Size([17862])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in cond_lstm_w2v.state_dict():\n",
    "    print(param_tensor, \"\\t\", cond_lstm_w2v.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2cfe-fa23-43f0-ac9a-873c02886fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fec25ee-1384-4327-bf8f-2b574865057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(model, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_id(t)]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top n values\n",
    "    top_ids = p.argsort()[-10:][::-1]\n",
    "\n",
    "    # sample id of next word from top n values\n",
    "    next_id = top_ids[random.sample([0,1,2,3,4,5,6,7,8,9],1)[0]]\n",
    "\n",
    "    # return the value of the predicted word and the hidden state\n",
    "    return id_to_word(next_id), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for generation batch size\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model=cond_lstm, n=10, prompt='in this paper'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.ones(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    words = prompt.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, t, h)\n",
    "    \n",
    "    words.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(n-1):\n",
    "        token, h = predict(model, words[-1], h)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adab21b4-3b4c-42bb-be3c-a66726aaaafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper for both english language detection in addition that there could help use both the number that language pairs the task can help use the user of natural languages we describe a framework of the task which includes the development for semeval shared shared model we introduce a neural framework based'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be8a26be-7a2e-4e3c-be86-fff764e819fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we show that such language modeling in terms for semantic classification our framework combines semantic role information of words that is used with lexical resources which makes more robust information to the same extent that are a challenging process in order of a single way we show in particular how we provide an approach and how a task for automatic speech resolution classification the task includes several language processing systems that is able that the process between languages can use a set the goal to build both data that is possible and present an approach for evaluating different strategies to be'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=100, prompt='in this paper we')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec48bd6-4fd3-4d53-9d9a-ed9f4d4c048b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe735f7-b224-471c-b3be-14a0462607c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
