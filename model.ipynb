{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f89a46-8f42-477f-90ff-592fffa0b62b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized.txt','r') as f:\n",
    "    tokenized = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f210f4c1-b171-4e41-be01-ee4dd311b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.load('data/x.npy')\n",
    "y_arr = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c71560-f7a1-437a-b023-fd9004a7b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17628, 133)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b21bf4-ad35-4c76-abc0-307060aa3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4084a92-0f22-4e42-ab06-95ebeffccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(x_arr)\n",
    "ids = list(range(dataset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13ec988-2ff5-47f3-b15c-c48307304e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = int(np.floor(valid_split * dataset_len))\n",
    "val_ids = np.random.choice(ids, size=val_len, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd395d6c-bada-4844-a2e0-4926815bf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.array(x_arr)[val_ids] \n",
    "y_val = np.array(y_arr)[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ca0ddd-101d-437c-a1d6-8b43eb9da709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3525, 133), (3525, 133))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67520dd9-c365-44c1-8f2d-9ebed3878bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.delete(x_arr, val_ids, axis=0)\n",
    "y_train = np.delete(y_arr, val_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5147fa1-457d-44b2-a9da-c58739c7ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14103, 133), (14103, 133))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x_arr, y_arr, batch_size):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for n in range(batch_size, x_arr.shape[0], batch_size):\n",
    "        x = x_arr[pos:n]\n",
    "        y = y_arr[pos:n]\n",
    "        pos = n\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27066b95-79ce-4403-97d8-1b918ca73497",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'train': x_train , 'val': x_val}\n",
    "y_data = {'train': y_train , 'val': y_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4ae5c6-cf7a-4286-ab02-9700f0eb6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load('w2v.model', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33831, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2fa4c61-c375-42d9-8c23-a6a39478fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = w2v_model.wv.vectors.shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a8372-24fd-40e9-a4ea-d3d0294d5995",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6557fe4-f6e8-4784-85f6-259ecb9ae92e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8fcd3e-895e-4874-b6d1-b6a78ccd101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters used for LSA and PCA\n",
    "n_hidden = 256\n",
    "batch_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10435 unique tokens: ['adaptation', 'address', 'albert', 'attention', 'auxiliary']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1a2d-98a7-45c5-8dd6-7851aa323e51",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffbc817-9458-491f-a8d1-ee7d9fabb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef742f30-a219-408b-a39a-bc77dc2e8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.607*\"de\" + 0.316*\"d\" + 0.257*\"la\" + 0.253*\"des\" + 0.241*\"les\" + 0.205*\"et\" + 0.204*\"l\" + 0.144*\"le\" + 0.137*\"r\" + 0.135*\"un\"'),\n",
       " (1,\n",
       "  '0.118*\"attention\" + 0.114*\"question\" + 0.104*\"generation\" + 0.098*\"pre\" + 0.096*\"sentences\" + 0.093*\"entity\" + 0.093*\"speech\" + 0.092*\"sentiment\" + 0.091*\"quality\" + 0.090*\"target\"'),\n",
       " (2,\n",
       "  '-0.546*\"question\" + -0.320*\"questions\" + -0.248*\"answer\" + -0.221*\"answering\" + 0.189*\"sentiment\" + 0.188*\"speech\" + -0.187*\"qa\" + -0.181*\"generation\" + 0.122*\"social\" + 0.121*\"detection\"'),\n",
       " (3,\n",
       "  '-0.258*\"social\" + -0.230*\"news\" + -0.226*\"sentiment\" + -0.197*\"user\" + 0.193*\"cross\" + -0.189*\"media\" + -0.180*\"detection\" + 0.162*\"lingual\" + -0.157*\"question\" + 0.157*\"target\"'),\n",
       " (4,\n",
       "  '-0.330*\"speech\" + 0.314*\"attention\" + 0.296*\"sentiment\" + 0.255*\"entity\" + 0.200*\"graph\" + 0.169*\"relation\" + -0.162*\"resource\" + 0.155*\"extraction\" + 0.147*\"entities\" + -0.146*\"quality\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5b70180a-f67f-4092-9569-40997bf0b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('social', -0.25794907374180537),\n",
       " ('news', -0.23017367810397968),\n",
       " ('sentiment', -0.22566767338805596),\n",
       " ('user', -0.19738698810528404),\n",
       " ('cross', 0.19303718978296186),\n",
       " ('media', -0.18930381963194298),\n",
       " ('detection', -0.1803576343787139),\n",
       " ('lingual', 0.16228186894560276),\n",
       " ('question', -0.1571902667860817),\n",
       " ('target', 0.15669335826614586)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d2d4cc-f09c-4096-9c53-1e6d7d05eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(lsi.projection.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62faf-ac59-4f61-87a7-bf2de09c82cb",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fd01de-ac01-43ef-a370-6bd234108e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics = PCA(n_components=n_layers*batch_size, svd_solver='full').fit_transform(trans_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d50757e-d8ca-482a-88b7-99636cf4f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a1e13-b90c-4616-9120-cbd7498a29fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08093e-f769-4dc9-8c2e-2753e70772d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conditioned_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                  torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "385453f3-8dc1-46e1-b9c2-d37088ab4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioned_LSTM(\n",
      "  (emb_layer): Embedding(33831, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=33831, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm = Conditioned_LSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e0f1-f4e5-46cf-afed-4f9941b79994",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Condtiioned LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a9b0329-f90a-4121-9308-931b5c2b6b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conditioned_LSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                  torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioned_LSTM_Word2Vec(\n",
      "  (emb_layer): Embedding(33831, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=3, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=33831, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm_w2v = Conditioned_LSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers, drop_prob=0)\n",
    "\n",
    "print(cond_lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa0ae6-4c96-4ad1-970e-98dea4b9bfa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3dc1f2f-8f97-402d-a896-af196d1e88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # loss values\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # dict to assing to right loss list according to phase\n",
    "    losses = {'train': train_loss , 'val': val_loss}\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train(True) if phase == 'train' else model.train(False)\n",
    "            \n",
    "            batch = 0\n",
    "\n",
    "            train_epoch_loss = []\n",
    "            val_epoch_loss = []\n",
    "            \n",
    "            # dict to assing to right epoch loss list according to phase\n",
    "            epoch_loss = {'train': train_epoch_loss , 'val': val_epoch_loss} \n",
    "\n",
    "            for x, y in batches(x_data[phase], y_data[phase], batch_size):\n",
    "\n",
    "                batch += 1\n",
    "\n",
    "                # initialize hidden state\n",
    "                h = model.init_hidden(batch_size)\n",
    "\n",
    "                # convert numpy arrays to PyTorch arrays\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                # detach hidden states\n",
    "                h = tuple([each.data for each in h])\n",
    "\n",
    "                # zero accumulated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # get the output from the model\n",
    "                output, h = model(inputs, h)\n",
    "\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # back-propagate error\n",
    "                    loss.backward()\n",
    "\n",
    "                    # `clip_grad_norm` helps prevent exploding gradient\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                    # update weigths\n",
    "                    opt.step()\n",
    "                \n",
    "                # add current batch loss to epoch loss list\n",
    "                epoch_loss[phase].append(loss.item())\n",
    "\n",
    "                # show epoch - batch - loss every n batches\n",
    "                if batch % print_every == 0:\n",
    "\n",
    "                    tot_batches = int(x_data[phase].shape[0] / batch_size)\n",
    "\n",
    "                    print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{} -\".format(batch, tot_batches),\n",
    "                          \"{} loss: {:.5f}\".format(phase.capitalize(), loss))\n",
    "                    \n",
    "            # calculate average epoch loss\n",
    "            avg_epoch_loss = sum(epoch_loss[phase])/len(epoch_loss[phase])\n",
    "                    \n",
    "            # print average train and val loss at the end of each epoch\n",
    "            print(\"\\nEpoch: {}/{} -\".format(e+1, epochs),\n",
    "                  \"Average {} loss: {:.5f}\\n\".format(phase, avg_epoch_loss))           \n",
    "\n",
    "            # save average epoch loss for training and validation\n",
    "            losses[phase].append(avg_epoch_loss)\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'weights/training_checkpoints.pt'\n",
    "epochs = 10\n",
    "\n",
    "# optimizers\n",
    "lstm_opt = torch.optim.Adam(cond_lstm.parameters(), lr=0.001)\n",
    "lstm_w2v_opt = torch.optim.Adam(cond_lstm_w2v.parameters(), lr=0.001)\n",
    "\n",
    "# loss criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save checkpoints\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'loss': criterion,\n",
    "            'lstm_state_dict': cond_lstm.state_dict(),\n",
    "            'lstm_w2v_state_dict': cond_lstm_w2v.state_dict(),\n",
    "            'lst_opt_state_dict': lstm_opt.state_dict(),\n",
    "            'lst_opt_w2v_state_dict': lstm_w2v_opt.state_dict(),\n",
    "            }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83fc79af-27cf-4951-9d4d-6f47b15a8e3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 27/220 - Train loss: 6.90484\n",
      "Epoch: 1/20 - Batch: 54/220 - Train loss: 6.90856\n",
      "Epoch: 1/20 - Batch: 81/220 - Train loss: 7.00684\n",
      "Epoch: 1/20 - Batch: 108/220 - Train loss: 7.09564\n",
      "Epoch: 1/20 - Batch: 135/220 - Train loss: 6.92284\n",
      "Epoch: 1/20 - Batch: 162/220 - Train loss: 6.96157\n",
      "Epoch: 1/20 - Batch: 189/220 - Train loss: 7.00223\n",
      "Epoch: 1/20 - Batch: 216/220 - Train loss: 7.11695\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 7.01820\n",
      "\n",
      "Epoch: 1/20 - Batch: 27/55 - Val loss: 6.99480\n",
      "Epoch: 1/20 - Batch: 54/55 - Val loss: 6.89165\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.97588\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/220 - Train loss: 6.80712\n",
      "Epoch: 2/20 - Batch: 54/220 - Train loss: 6.84844\n",
      "Epoch: 2/20 - Batch: 81/220 - Train loss: 6.94332\n",
      "Epoch: 2/20 - Batch: 108/220 - Train loss: 6.97183\n",
      "Epoch: 2/20 - Batch: 135/220 - Train loss: 6.82876\n",
      "Epoch: 2/20 - Batch: 162/220 - Train loss: 6.85239\n",
      "Epoch: 2/20 - Batch: 189/220 - Train loss: 6.81584\n",
      "Epoch: 2/20 - Batch: 216/220 - Train loss: 6.74818\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 6.88906\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/55 - Val loss: 6.71416\n",
      "Epoch: 2/20 - Batch: 54/55 - Val loss: 6.60908\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 6.69926\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/220 - Train loss: 6.46956\n",
      "Epoch: 3/20 - Batch: 54/220 - Train loss: 6.42263\n",
      "Epoch: 3/20 - Batch: 81/220 - Train loss: 6.46480\n",
      "Epoch: 3/20 - Batch: 108/220 - Train loss: 6.51231\n",
      "Epoch: 3/20 - Batch: 135/220 - Train loss: 6.21928\n",
      "Epoch: 3/20 - Batch: 162/220 - Train loss: 6.32962\n",
      "Epoch: 3/20 - Batch: 189/220 - Train loss: 6.28430\n",
      "Epoch: 3/20 - Batch: 216/220 - Train loss: 5.97005\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.35259\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/55 - Val loss: 6.13543\n",
      "Epoch: 3/20 - Batch: 54/55 - Val loss: 6.04280\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.13324\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/220 - Train loss: 6.01140\n",
      "Epoch: 4/20 - Batch: 54/220 - Train loss: 5.98601\n",
      "Epoch: 4/20 - Batch: 81/220 - Train loss: 6.04254\n",
      "Epoch: 4/20 - Batch: 108/220 - Train loss: 6.14997\n",
      "Epoch: 4/20 - Batch: 135/220 - Train loss: 5.82582\n",
      "Epoch: 4/20 - Batch: 162/220 - Train loss: 6.01197\n",
      "Epoch: 4/20 - Batch: 189/220 - Train loss: 5.96806\n",
      "Epoch: 4/20 - Batch: 216/220 - Train loss: 5.64235\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 5.94780\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/55 - Val loss: 5.82440\n",
      "Epoch: 4/20 - Batch: 54/55 - Val loss: 5.73112\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 5.82748\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/220 - Train loss: 5.71941\n",
      "Epoch: 5/20 - Batch: 54/220 - Train loss: 5.69600\n",
      "Epoch: 5/20 - Batch: 81/220 - Train loss: 5.77444\n",
      "Epoch: 5/20 - Batch: 108/220 - Train loss: 5.92604\n",
      "Epoch: 5/20 - Batch: 135/220 - Train loss: 5.57698\n",
      "Epoch: 5/20 - Batch: 162/220 - Train loss: 5.80915\n",
      "Epoch: 5/20 - Batch: 189/220 - Train loss: 5.76869\n",
      "Epoch: 5/20 - Batch: 216/220 - Train loss: 5.42832\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 5.69520\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/55 - Val loss: 5.63943\n",
      "Epoch: 5/20 - Batch: 54/55 - Val loss: 5.53965\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 5.64208\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/220 - Train loss: 5.53333\n",
      "Epoch: 6/20 - Batch: 54/220 - Train loss: 5.52160\n",
      "Epoch: 6/20 - Batch: 81/220 - Train loss: 5.58943\n",
      "Epoch: 6/20 - Batch: 108/220 - Train loss: 5.76979\n",
      "Epoch: 6/20 - Batch: 135/220 - Train loss: 5.41169\n",
      "Epoch: 6/20 - Batch: 162/220 - Train loss: 5.66354\n",
      "Epoch: 6/20 - Batch: 189/220 - Train loss: 5.63389\n",
      "Epoch: 6/20 - Batch: 216/220 - Train loss: 5.28951\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 5.52367\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/55 - Val loss: 5.51567\n",
      "Epoch: 6/20 - Batch: 54/55 - Val loss: 5.40903\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 5.51584\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/220 - Train loss: 5.39902\n",
      "Epoch: 7/20 - Batch: 54/220 - Train loss: 5.38502\n",
      "Epoch: 7/20 - Batch: 81/220 - Train loss: 5.46074\n",
      "Epoch: 7/20 - Batch: 108/220 - Train loss: 5.65406\n",
      "Epoch: 7/20 - Batch: 135/220 - Train loss: 5.29507\n",
      "Epoch: 7/20 - Batch: 162/220 - Train loss: 5.54992\n",
      "Epoch: 7/20 - Batch: 189/220 - Train loss: 5.52567\n",
      "Epoch: 7/20 - Batch: 216/220 - Train loss: 5.15976\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 5.39319\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/55 - Val loss: 5.41946\n",
      "Epoch: 7/20 - Batch: 54/55 - Val loss: 5.30715\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 5.41652\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/220 - Train loss: 5.29573\n",
      "Epoch: 8/20 - Batch: 54/220 - Train loss: 5.28486\n",
      "Epoch: 8/20 - Batch: 81/220 - Train loss: 5.35579\n",
      "Epoch: 8/20 - Batch: 108/220 - Train loss: 5.55338\n",
      "Epoch: 8/20 - Batch: 135/220 - Train loss: 5.18501\n",
      "Epoch: 8/20 - Batch: 162/220 - Train loss: 5.46134\n",
      "Epoch: 8/20 - Batch: 189/220 - Train loss: 5.43371\n",
      "Epoch: 8/20 - Batch: 216/220 - Train loss: 5.06375\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 5.28744\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/55 - Val loss: 5.34621\n",
      "Epoch: 8/20 - Batch: 54/55 - Val loss: 5.22706\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 5.33925\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/220 - Train loss: 5.19337\n",
      "Epoch: 9/20 - Batch: 54/220 - Train loss: 5.19256\n",
      "Epoch: 9/20 - Batch: 81/220 - Train loss: 5.25914\n",
      "Epoch: 9/20 - Batch: 108/220 - Train loss: 5.45395\n",
      "Epoch: 9/20 - Batch: 135/220 - Train loss: 5.09434\n",
      "Epoch: 9/20 - Batch: 162/220 - Train loss: 5.38426\n",
      "Epoch: 9/20 - Batch: 189/220 - Train loss: 5.37121\n",
      "Epoch: 9/20 - Batch: 216/220 - Train loss: 4.98101\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 5.19769\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/55 - Val loss: 5.28768\n",
      "Epoch: 9/20 - Batch: 54/55 - Val loss: 5.16293\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 5.27607\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/220 - Train loss: 5.11907\n",
      "Epoch: 10/20 - Batch: 54/220 - Train loss: 5.10847\n",
      "Epoch: 10/20 - Batch: 81/220 - Train loss: 5.19241\n",
      "Epoch: 10/20 - Batch: 108/220 - Train loss: 5.37689\n",
      "Epoch: 10/20 - Batch: 135/220 - Train loss: 5.02670\n",
      "Epoch: 10/20 - Batch: 162/220 - Train loss: 5.30687\n",
      "Epoch: 10/20 - Batch: 189/220 - Train loss: 5.29922\n",
      "Epoch: 10/20 - Batch: 216/220 - Train loss: 4.90713\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 5.12033\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/55 - Val loss: 5.24005\n",
      "Epoch: 10/20 - Batch: 54/55 - Val loss: 5.10813\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 5.22382\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/220 - Train loss: 5.05424\n",
      "Epoch: 11/20 - Batch: 54/220 - Train loss: 5.05809\n",
      "Epoch: 11/20 - Batch: 81/220 - Train loss: 5.13733\n",
      "Epoch: 11/20 - Batch: 108/220 - Train loss: 5.30213\n",
      "Epoch: 11/20 - Batch: 135/220 - Train loss: 4.95490\n",
      "Epoch: 11/20 - Batch: 162/220 - Train loss: 5.25270\n",
      "Epoch: 11/20 - Batch: 189/220 - Train loss: 5.25815\n",
      "Epoch: 11/20 - Batch: 216/220 - Train loss: 4.83302\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.05303\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/55 - Val loss: 5.20550\n",
      "Epoch: 11/20 - Batch: 54/55 - Val loss: 5.06701\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.18548\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/220 - Train loss: 4.99226\n",
      "Epoch: 12/20 - Batch: 54/220 - Train loss: 4.99444\n",
      "Epoch: 12/20 - Batch: 81/220 - Train loss: 5.06701\n",
      "Epoch: 12/20 - Batch: 108/220 - Train loss: 5.25437\n",
      "Epoch: 12/20 - Batch: 135/220 - Train loss: 4.90411\n",
      "Epoch: 12/20 - Batch: 162/220 - Train loss: 5.18960\n",
      "Epoch: 12/20 - Batch: 189/220 - Train loss: 5.19432\n",
      "Epoch: 12/20 - Batch: 216/220 - Train loss: 4.78332\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 4.99398\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/55 - Val loss: 5.17409\n",
      "Epoch: 12/20 - Batch: 54/55 - Val loss: 5.03328\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.15291\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/220 - Train loss: 4.95364\n",
      "Epoch: 13/20 - Batch: 54/220 - Train loss: 4.93516\n",
      "Epoch: 13/20 - Batch: 81/220 - Train loss: 5.02350\n",
      "Epoch: 13/20 - Batch: 108/220 - Train loss: 5.20737\n",
      "Epoch: 13/20 - Batch: 135/220 - Train loss: 4.84518\n",
      "Epoch: 13/20 - Batch: 162/220 - Train loss: 5.14020\n",
      "Epoch: 13/20 - Batch: 189/220 - Train loss: 5.14642\n",
      "Epoch: 13/20 - Batch: 216/220 - Train loss: 4.72353\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 4.94107\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/55 - Val loss: 5.14125\n",
      "Epoch: 13/20 - Batch: 54/55 - Val loss: 5.00563\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.12252\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/220 - Train loss: 4.90212\n",
      "Epoch: 14/20 - Batch: 54/220 - Train loss: 4.89298\n",
      "Epoch: 14/20 - Batch: 81/220 - Train loss: 4.97095\n",
      "Epoch: 14/20 - Batch: 108/220 - Train loss: 5.16296\n",
      "Epoch: 14/20 - Batch: 135/220 - Train loss: 4.81558\n",
      "Epoch: 14/20 - Batch: 162/220 - Train loss: 5.09998\n",
      "Epoch: 14/20 - Batch: 189/220 - Train loss: 5.09324\n",
      "Epoch: 14/20 - Batch: 216/220 - Train loss: 4.67658\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 4.89220\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/55 - Val loss: 5.11736\n",
      "Epoch: 14/20 - Batch: 54/55 - Val loss: 4.97912\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.09560\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/220 - Train loss: 4.86438\n",
      "Epoch: 15/20 - Batch: 54/220 - Train loss: 4.85668\n",
      "Epoch: 15/20 - Batch: 81/220 - Train loss: 4.93045\n",
      "Epoch: 15/20 - Batch: 108/220 - Train loss: 5.11790\n",
      "Epoch: 15/20 - Batch: 135/220 - Train loss: 4.78421\n",
      "Epoch: 15/20 - Batch: 162/220 - Train loss: 5.05647\n",
      "Epoch: 15/20 - Batch: 189/220 - Train loss: 5.06201\n",
      "Epoch: 15/20 - Batch: 216/220 - Train loss: 4.64399\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 4.85074\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/55 - Val loss: 5.09813\n",
      "Epoch: 15/20 - Batch: 54/55 - Val loss: 4.95640\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.07670\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/220 - Train loss: 4.82683\n",
      "Epoch: 16/20 - Batch: 54/220 - Train loss: 4.80302\n",
      "Epoch: 16/20 - Batch: 81/220 - Train loss: 4.89114\n",
      "Epoch: 16/20 - Batch: 108/220 - Train loss: 5.07389\n",
      "Epoch: 16/20 - Batch: 135/220 - Train loss: 4.73688\n",
      "Epoch: 16/20 - Batch: 162/220 - Train loss: 5.02061\n",
      "Epoch: 16/20 - Batch: 189/220 - Train loss: 5.00507\n",
      "Epoch: 16/20 - Batch: 216/220 - Train loss: 4.59809\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 4.80965\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/55 - Val loss: 5.08395\n",
      "Epoch: 16/20 - Batch: 54/55 - Val loss: 4.93967\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.05997\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/220 - Train loss: 4.78480\n",
      "Epoch: 17/20 - Batch: 54/220 - Train loss: 4.77930\n",
      "Epoch: 17/20 - Batch: 81/220 - Train loss: 4.85896\n",
      "Epoch: 17/20 - Batch: 108/220 - Train loss: 5.03805\n",
      "Epoch: 17/20 - Batch: 135/220 - Train loss: 4.69501\n",
      "Epoch: 17/20 - Batch: 162/220 - Train loss: 4.98518\n",
      "Epoch: 17/20 - Batch: 189/220 - Train loss: 4.99487\n",
      "Epoch: 17/20 - Batch: 216/220 - Train loss: 4.56531\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 4.77233\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/55 - Val loss: 5.06720\n",
      "Epoch: 17/20 - Batch: 54/55 - Val loss: 4.92100\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.04439\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/220 - Train loss: 4.75554\n",
      "Epoch: 18/20 - Batch: 54/220 - Train loss: 4.75109\n",
      "Epoch: 18/20 - Batch: 81/220 - Train loss: 4.83586\n",
      "Epoch: 18/20 - Batch: 108/220 - Train loss: 5.00288\n",
      "Epoch: 18/20 - Batch: 135/220 - Train loss: 4.68125\n",
      "Epoch: 18/20 - Batch: 162/220 - Train loss: 4.93854\n",
      "Epoch: 18/20 - Batch: 189/220 - Train loss: 4.95574\n",
      "Epoch: 18/20 - Batch: 216/220 - Train loss: 4.51735\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 4.73641\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/55 - Val loss: 5.05713\n",
      "Epoch: 18/20 - Batch: 54/55 - Val loss: 4.90988\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.03316\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/220 - Train loss: 4.71767\n",
      "Epoch: 19/20 - Batch: 54/220 - Train loss: 4.71006\n",
      "Epoch: 19/20 - Batch: 81/220 - Train loss: 4.78056\n",
      "Epoch: 19/20 - Batch: 108/220 - Train loss: 4.95541\n",
      "Epoch: 19/20 - Batch: 135/220 - Train loss: 4.63721\n",
      "Epoch: 19/20 - Batch: 162/220 - Train loss: 4.92563\n",
      "Epoch: 19/20 - Batch: 189/220 - Train loss: 4.91736\n",
      "Epoch: 19/20 - Batch: 216/220 - Train loss: 4.49679\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 4.70457\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/55 - Val loss: 5.05141\n",
      "Epoch: 19/20 - Batch: 54/55 - Val loss: 4.89851\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.02554\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/220 - Train loss: 4.67882\n",
      "Epoch: 20/20 - Batch: 54/220 - Train loss: 4.67985\n",
      "Epoch: 20/20 - Batch: 81/220 - Train loss: 4.74922\n",
      "Epoch: 20/20 - Batch: 108/220 - Train loss: 4.93530\n",
      "Epoch: 20/20 - Batch: 135/220 - Train loss: 4.60506\n",
      "Epoch: 20/20 - Batch: 162/220 - Train loss: 4.89999\n",
      "Epoch: 20/20 - Batch: 189/220 - Train loss: 4.88778\n",
      "Epoch: 20/20 - Batch: 216/220 - Train loss: 4.46224\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 4.67259\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/55 - Val loss: 5.04309\n",
      "Epoch: 20/20 - Batch: 54/55 - Val loss: 4.88629\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.01551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DELETE\n",
    "# train_loss, val_loss = train(cond_lstm, batch_size=batch_size, epochs=20, print_every=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ead137d-07c6-4f65-a355-4f457d60814f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 27/220 - Train loss: 6.94394\n",
      "Epoch: 1/20 - Batch: 54/220 - Train loss: 7.46500\n",
      "Epoch: 1/20 - Batch: 81/220 - Train loss: 7.02347\n",
      "Epoch: 1/20 - Batch: 108/220 - Train loss: 7.03634\n",
      "Epoch: 1/20 - Batch: 135/220 - Train loss: 6.96275\n",
      "Epoch: 1/20 - Batch: 162/220 - Train loss: 7.11021\n",
      "Epoch: 1/20 - Batch: 189/220 - Train loss: 7.02176\n",
      "Epoch: 1/20 - Batch: 216/220 - Train loss: 7.22654\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 7.19274\n",
      "\n",
      "Epoch: 1/20 - Batch: 27/55 - Val loss: 6.93077\n",
      "Epoch: 1/20 - Batch: 54/55 - Val loss: 6.98899\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.98778\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/220 - Train loss: 6.83380\n",
      "Epoch: 2/20 - Batch: 54/220 - Train loss: 7.35449\n",
      "Epoch: 2/20 - Batch: 81/220 - Train loss: 6.99110\n",
      "Epoch: 2/20 - Batch: 108/220 - Train loss: 7.00515\n",
      "Epoch: 2/20 - Batch: 135/220 - Train loss: 6.93535\n",
      "Epoch: 2/20 - Batch: 162/220 - Train loss: 7.08040\n",
      "Epoch: 2/20 - Batch: 189/220 - Train loss: 6.99813\n",
      "Epoch: 2/20 - Batch: 216/220 - Train loss: 7.20359\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 7.00146\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/55 - Val loss: 6.93691\n",
      "Epoch: 2/20 - Batch: 54/55 - Val loss: 6.99584\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 6.99251\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/220 - Train loss: 6.82754\n",
      "Epoch: 3/20 - Batch: 54/220 - Train loss: 7.34864\n",
      "Epoch: 3/20 - Batch: 81/220 - Train loss: 6.98328\n",
      "Epoch: 3/20 - Batch: 108/220 - Train loss: 6.99706\n",
      "Epoch: 3/20 - Batch: 135/220 - Train loss: 6.92409\n",
      "Epoch: 3/20 - Batch: 162/220 - Train loss: 7.07974\n",
      "Epoch: 3/20 - Batch: 189/220 - Train loss: 6.98708\n",
      "Epoch: 3/20 - Batch: 216/220 - Train loss: 7.19367\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.99377\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/55 - Val loss: 6.94128\n",
      "Epoch: 3/20 - Batch: 54/55 - Val loss: 7.00050\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.99625\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/220 - Train loss: 6.82290\n",
      "Epoch: 4/20 - Batch: 54/220 - Train loss: 7.34297\n",
      "Epoch: 4/20 - Batch: 81/220 - Train loss: 6.98041\n",
      "Epoch: 4/20 - Batch: 108/220 - Train loss: 6.99645\n",
      "Epoch: 4/20 - Batch: 135/220 - Train loss: 6.92311\n",
      "Epoch: 4/20 - Batch: 162/220 - Train loss: 7.07807\n",
      "Epoch: 4/20 - Batch: 189/220 - Train loss: 6.98117\n",
      "Epoch: 4/20 - Batch: 216/220 - Train loss: 7.18824\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 6.98879\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/55 - Val loss: 6.94127\n",
      "Epoch: 4/20 - Batch: 54/55 - Val loss: 7.00124\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 6.99622\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/220 - Train loss: 6.81285\n",
      "Epoch: 5/20 - Batch: 54/220 - Train loss: 7.33374\n",
      "Epoch: 5/20 - Batch: 81/220 - Train loss: 6.97570\n",
      "Epoch: 5/20 - Batch: 108/220 - Train loss: 6.99060\n",
      "Epoch: 5/20 - Batch: 135/220 - Train loss: 6.91857\n",
      "Epoch: 5/20 - Batch: 162/220 - Train loss: 7.06680\n",
      "Epoch: 5/20 - Batch: 189/220 - Train loss: 6.97421\n",
      "Epoch: 5/20 - Batch: 216/220 - Train loss: 7.17908\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 6.98229\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/55 - Val loss: 6.94196\n",
      "Epoch: 5/20 - Batch: 54/55 - Val loss: 7.00235\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 6.99700\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/220 - Train loss: 6.81148\n",
      "Epoch: 6/20 - Batch: 54/220 - Train loss: 7.33509\n",
      "Epoch: 6/20 - Batch: 81/220 - Train loss: 6.97139\n",
      "Epoch: 6/20 - Batch: 108/220 - Train loss: 6.98684\n",
      "Epoch: 6/20 - Batch: 135/220 - Train loss: 6.91326\n",
      "Epoch: 6/20 - Batch: 162/220 - Train loss: 7.04655\n",
      "Epoch: 6/20 - Batch: 189/220 - Train loss: 6.92390\n",
      "Epoch: 6/20 - Batch: 216/220 - Train loss: 7.07711\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 6.96347\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/55 - Val loss: 6.82482\n",
      "Epoch: 6/20 - Batch: 54/55 - Val loss: 6.87768\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 6.87510\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/220 - Train loss: 6.66268\n",
      "Epoch: 7/20 - Batch: 54/220 - Train loss: 7.13230\n",
      "Epoch: 7/20 - Batch: 81/220 - Train loss: 6.78338\n",
      "Epoch: 7/20 - Batch: 108/220 - Train loss: 6.76592\n",
      "Epoch: 7/20 - Batch: 135/220 - Train loss: 6.64731\n",
      "Epoch: 7/20 - Batch: 162/220 - Train loss: 6.79056\n",
      "Epoch: 7/20 - Batch: 189/220 - Train loss: 6.68377\n",
      "Epoch: 7/20 - Batch: 216/220 - Train loss: 6.78158\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 6.74698\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/55 - Val loss: 6.59501\n",
      "Epoch: 7/20 - Batch: 54/55 - Val loss: 6.64092\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 6.63828\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/220 - Train loss: 6.43787\n",
      "Epoch: 8/20 - Batch: 54/220 - Train loss: 6.82686\n",
      "Epoch: 8/20 - Batch: 81/220 - Train loss: 6.57892\n",
      "Epoch: 8/20 - Batch: 108/220 - Train loss: 6.59091\n",
      "Epoch: 8/20 - Batch: 135/220 - Train loss: 6.45233\n",
      "Epoch: 8/20 - Batch: 162/220 - Train loss: 6.58035\n",
      "Epoch: 8/20 - Batch: 189/220 - Train loss: 6.46495\n",
      "Epoch: 8/20 - Batch: 216/220 - Train loss: 6.32706\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 6.51953\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/55 - Val loss: 6.35838\n",
      "Epoch: 8/20 - Batch: 54/55 - Val loss: 6.33967\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 6.36008\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/220 - Train loss: 6.20030\n",
      "Epoch: 9/20 - Batch: 54/220 - Train loss: 6.27186\n",
      "Epoch: 9/20 - Batch: 81/220 - Train loss: 6.33078\n",
      "Epoch: 9/20 - Batch: 108/220 - Train loss: 6.38380\n",
      "Epoch: 9/20 - Batch: 135/220 - Train loss: 6.19178\n",
      "Epoch: 9/20 - Batch: 162/220 - Train loss: 6.31387\n",
      "Epoch: 9/20 - Batch: 189/220 - Train loss: 6.23652\n",
      "Epoch: 9/20 - Batch: 216/220 - Train loss: 6.02161\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 6.25421\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/55 - Val loss: 6.13735\n",
      "Epoch: 9/20 - Batch: 54/55 - Val loss: 6.10132\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 6.13092\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/220 - Train loss: 5.97990\n",
      "Epoch: 10/20 - Batch: 54/220 - Train loss: 6.00716\n",
      "Epoch: 10/20 - Batch: 81/220 - Train loss: 6.14822\n",
      "Epoch: 10/20 - Batch: 108/220 - Train loss: 6.19631\n",
      "Epoch: 10/20 - Batch: 135/220 - Train loss: 5.99457\n",
      "Epoch: 10/20 - Batch: 162/220 - Train loss: 6.13118\n",
      "Epoch: 10/20 - Batch: 189/220 - Train loss: 6.04713\n",
      "Epoch: 10/20 - Batch: 216/220 - Train loss: 5.82447\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 6.04775\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/55 - Val loss: 5.95829\n",
      "Epoch: 10/20 - Batch: 54/55 - Val loss: 5.91536\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 5.95108\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/220 - Train loss: 5.80310\n",
      "Epoch: 11/20 - Batch: 54/220 - Train loss: 5.80564\n",
      "Epoch: 11/20 - Batch: 81/220 - Train loss: 5.97026\n",
      "Epoch: 11/20 - Batch: 108/220 - Train loss: 6.05380\n",
      "Epoch: 11/20 - Batch: 135/220 - Train loss: 5.82239\n",
      "Epoch: 11/20 - Batch: 162/220 - Train loss: 5.98603\n",
      "Epoch: 11/20 - Batch: 189/220 - Train loss: 5.88525\n",
      "Epoch: 11/20 - Batch: 216/220 - Train loss: 5.65908\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.87805\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/55 - Val loss: 5.81733\n",
      "Epoch: 11/20 - Batch: 54/55 - Val loss: 5.77507\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.81082\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/220 - Train loss: 5.65960\n",
      "Epoch: 12/20 - Batch: 54/220 - Train loss: 5.65339\n",
      "Epoch: 12/20 - Batch: 81/220 - Train loss: 5.83079\n",
      "Epoch: 12/20 - Batch: 108/220 - Train loss: 5.92474\n",
      "Epoch: 12/20 - Batch: 135/220 - Train loss: 5.68649\n",
      "Epoch: 12/20 - Batch: 162/220 - Train loss: 5.87430\n",
      "Epoch: 12/20 - Batch: 189/220 - Train loss: 5.77060\n",
      "Epoch: 12/20 - Batch: 216/220 - Train loss: 5.51315\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 5.74077\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/55 - Val loss: 5.70088\n",
      "Epoch: 12/20 - Batch: 54/55 - Val loss: 5.66332\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.69764\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/220 - Train loss: 5.54195\n",
      "Epoch: 13/20 - Batch: 54/220 - Train loss: 5.52806\n",
      "Epoch: 13/20 - Batch: 81/220 - Train loss: 5.70756\n",
      "Epoch: 13/20 - Batch: 108/220 - Train loss: 5.84301\n",
      "Epoch: 13/20 - Batch: 135/220 - Train loss: 5.56874\n",
      "Epoch: 13/20 - Batch: 162/220 - Train loss: 5.78473\n",
      "Epoch: 13/20 - Batch: 189/220 - Train loss: 5.67042\n",
      "Epoch: 13/20 - Batch: 216/220 - Train loss: 5.40035\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 5.62642\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/55 - Val loss: 5.60806\n",
      "Epoch: 13/20 - Batch: 54/55 - Val loss: 5.57362\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.60554\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/220 - Train loss: 5.44341\n",
      "Epoch: 14/20 - Batch: 54/220 - Train loss: 5.41002\n",
      "Epoch: 14/20 - Batch: 81/220 - Train loss: 5.60713\n",
      "Epoch: 14/20 - Batch: 108/220 - Train loss: 5.75325\n",
      "Epoch: 14/20 - Batch: 135/220 - Train loss: 5.47868\n",
      "Epoch: 14/20 - Batch: 162/220 - Train loss: 5.70102\n",
      "Epoch: 14/20 - Batch: 189/220 - Train loss: 5.57820\n",
      "Epoch: 14/20 - Batch: 216/220 - Train loss: 5.30720\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 5.52770\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/55 - Val loss: 5.53447\n",
      "Epoch: 14/20 - Batch: 54/55 - Val loss: 5.49860\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.53068\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/220 - Train loss: 5.35313\n",
      "Epoch: 15/20 - Batch: 54/220 - Train loss: 5.31490\n",
      "Epoch: 15/20 - Batch: 81/220 - Train loss: 5.52558\n",
      "Epoch: 15/20 - Batch: 108/220 - Train loss: 5.68763\n",
      "Epoch: 15/20 - Batch: 135/220 - Train loss: 5.38757\n",
      "Epoch: 15/20 - Batch: 162/220 - Train loss: 5.62352\n",
      "Epoch: 15/20 - Batch: 189/220 - Train loss: 5.51870\n",
      "Epoch: 15/20 - Batch: 216/220 - Train loss: 5.22134\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 5.44231\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/55 - Val loss: 5.47939\n",
      "Epoch: 15/20 - Batch: 54/55 - Val loss: 5.43744\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.47112\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/220 - Train loss: 5.27047\n",
      "Epoch: 16/20 - Batch: 54/220 - Train loss: 5.22551\n",
      "Epoch: 16/20 - Batch: 81/220 - Train loss: 5.45501\n",
      "Epoch: 16/20 - Batch: 108/220 - Train loss: 5.63232\n",
      "Epoch: 16/20 - Batch: 135/220 - Train loss: 5.31575\n",
      "Epoch: 16/20 - Batch: 162/220 - Train loss: 5.55408\n",
      "Epoch: 16/20 - Batch: 189/220 - Train loss: 5.44537\n",
      "Epoch: 16/20 - Batch: 216/220 - Train loss: 5.13405\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 5.36665\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/55 - Val loss: 5.42531\n",
      "Epoch: 16/20 - Batch: 54/55 - Val loss: 5.37842\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.41421\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/220 - Train loss: 5.20704\n",
      "Epoch: 17/20 - Batch: 54/220 - Train loss: 5.14451\n",
      "Epoch: 17/20 - Batch: 81/220 - Train loss: 5.38356\n",
      "Epoch: 17/20 - Batch: 108/220 - Train loss: 5.57275\n",
      "Epoch: 17/20 - Batch: 135/220 - Train loss: 5.24277\n",
      "Epoch: 17/20 - Batch: 162/220 - Train loss: 5.49960\n",
      "Epoch: 17/20 - Batch: 189/220 - Train loss: 5.38907\n",
      "Epoch: 17/20 - Batch: 216/220 - Train loss: 5.07851\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 5.29764\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/55 - Val loss: 5.38019\n",
      "Epoch: 17/20 - Batch: 54/55 - Val loss: 5.32570\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.36497\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/220 - Train loss: 5.14721\n",
      "Epoch: 18/20 - Batch: 54/220 - Train loss: 5.06633\n",
      "Epoch: 18/20 - Batch: 81/220 - Train loss: 5.32877\n",
      "Epoch: 18/20 - Batch: 108/220 - Train loss: 5.51741\n",
      "Epoch: 18/20 - Batch: 135/220 - Train loss: 5.18255\n",
      "Epoch: 18/20 - Batch: 162/220 - Train loss: 5.44793\n",
      "Epoch: 18/20 - Batch: 189/220 - Train loss: 5.33683\n",
      "Epoch: 18/20 - Batch: 216/220 - Train loss: 5.01145\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 5.23363\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/55 - Val loss: 5.33505\n",
      "Epoch: 18/20 - Batch: 54/55 - Val loss: 5.27623\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.31565\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/220 - Train loss: 5.08362\n",
      "Epoch: 19/20 - Batch: 54/220 - Train loss: 4.99855\n",
      "Epoch: 19/20 - Batch: 81/220 - Train loss: 5.27482\n",
      "Epoch: 19/20 - Batch: 108/220 - Train loss: 5.47782\n",
      "Epoch: 19/20 - Batch: 135/220 - Train loss: 5.11537\n",
      "Epoch: 19/20 - Batch: 162/220 - Train loss: 5.40204\n",
      "Epoch: 19/20 - Batch: 189/220 - Train loss: 5.28121\n",
      "Epoch: 19/20 - Batch: 216/220 - Train loss: 4.94702\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 5.17537\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/55 - Val loss: 5.29449\n",
      "Epoch: 19/20 - Batch: 54/55 - Val loss: 5.23417\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.27533\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/220 - Train loss: 5.03199\n",
      "Epoch: 20/20 - Batch: 54/220 - Train loss: 4.93168\n",
      "Epoch: 20/20 - Batch: 81/220 - Train loss: 5.22022\n",
      "Epoch: 20/20 - Batch: 108/220 - Train loss: 5.41601\n",
      "Epoch: 20/20 - Batch: 135/220 - Train loss: 5.07687\n",
      "Epoch: 20/20 - Batch: 162/220 - Train loss: 5.34606\n",
      "Epoch: 20/20 - Batch: 189/220 - Train loss: 5.23285\n",
      "Epoch: 20/20 - Batch: 216/220 - Train loss: 4.89395\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 5.12179\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/55 - Val loss: 5.26428\n",
      "Epoch: 20/20 - Batch: 54/55 - Val loss: 5.19733\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.24062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(cond_lstm, batch_size=batch_size, epochs=20, print_every=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "331c2da1-1270-4c3f-b46f-c170a4f06c67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 27/220 - Train loss: 6.94467\n",
      "Epoch: 1/20 - Batch: 54/220 - Train loss: 6.89420\n",
      "Epoch: 1/20 - Batch: 81/220 - Train loss: 6.97950\n",
      "Epoch: 1/20 - Batch: 108/220 - Train loss: 7.03187\n",
      "Epoch: 1/20 - Batch: 135/220 - Train loss: 6.90102\n",
      "Epoch: 1/20 - Batch: 162/220 - Train loss: 6.92901\n",
      "Epoch: 1/20 - Batch: 189/220 - Train loss: 7.00218\n",
      "Epoch: 1/20 - Batch: 216/220 - Train loss: 7.10326\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 7.15523\n",
      "\n",
      "Epoch: 1/20 - Batch: 27/55 - Val loss: 7.00852\n",
      "Epoch: 1/20 - Batch: 54/55 - Val loss: 6.89758\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.99487\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/220 - Train loss: 6.84926\n",
      "Epoch: 2/20 - Batch: 54/220 - Train loss: 6.85337\n",
      "Epoch: 2/20 - Batch: 81/220 - Train loss: 6.95939\n",
      "Epoch: 2/20 - Batch: 108/220 - Train loss: 7.00601\n",
      "Epoch: 2/20 - Batch: 135/220 - Train loss: 6.88898\n",
      "Epoch: 2/20 - Batch: 162/220 - Train loss: 6.91657\n",
      "Epoch: 2/20 - Batch: 189/220 - Train loss: 6.98525\n",
      "Epoch: 2/20 - Batch: 216/220 - Train loss: 7.08495\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 6.97194\n",
      "\n",
      "Epoch: 2/20 - Batch: 27/55 - Val loss: 7.01796\n",
      "Epoch: 2/20 - Batch: 54/55 - Val loss: 6.90504\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 7.00469\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/220 - Train loss: 6.85485\n",
      "Epoch: 3/20 - Batch: 54/220 - Train loss: 6.85565\n",
      "Epoch: 3/20 - Batch: 81/220 - Train loss: 6.96292\n",
      "Epoch: 3/20 - Batch: 108/220 - Train loss: 7.00938\n",
      "Epoch: 3/20 - Batch: 135/220 - Train loss: 6.88952\n",
      "Epoch: 3/20 - Batch: 162/220 - Train loss: 6.92037\n",
      "Epoch: 3/20 - Batch: 189/220 - Train loss: 6.98713\n",
      "Epoch: 3/20 - Batch: 216/220 - Train loss: 7.08046\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.97468\n",
      "\n",
      "Epoch: 3/20 - Batch: 27/55 - Val loss: 7.01060\n",
      "Epoch: 3/20 - Batch: 54/55 - Val loss: 6.89723\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.99677\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/220 - Train loss: 6.80537\n",
      "Epoch: 4/20 - Batch: 54/220 - Train loss: 6.75442\n",
      "Epoch: 4/20 - Batch: 81/220 - Train loss: 6.82581\n",
      "Epoch: 4/20 - Batch: 108/220 - Train loss: 6.83126\n",
      "Epoch: 4/20 - Batch: 135/220 - Train loss: 6.66101\n",
      "Epoch: 4/20 - Batch: 162/220 - Train loss: 6.67692\n",
      "Epoch: 4/20 - Batch: 189/220 - Train loss: 6.70263\n",
      "Epoch: 4/20 - Batch: 216/220 - Train loss: 6.70915\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 6.79002\n",
      "\n",
      "Epoch: 4/20 - Batch: 27/55 - Val loss: 6.68076\n",
      "Epoch: 4/20 - Batch: 54/55 - Val loss: 6.56268\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 6.66567\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/220 - Train loss: 6.47633\n",
      "Epoch: 5/20 - Batch: 54/220 - Train loss: 6.43594\n",
      "Epoch: 5/20 - Batch: 81/220 - Train loss: 6.51028\n",
      "Epoch: 5/20 - Batch: 108/220 - Train loss: 6.56101\n",
      "Epoch: 5/20 - Batch: 135/220 - Train loss: 6.35229\n",
      "Epoch: 5/20 - Batch: 162/220 - Train loss: 6.40840\n",
      "Epoch: 5/20 - Batch: 189/220 - Train loss: 6.40337\n",
      "Epoch: 5/20 - Batch: 216/220 - Train loss: 6.20952\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 6.45804\n",
      "\n",
      "Epoch: 5/20 - Batch: 27/55 - Val loss: 6.33233\n",
      "Epoch: 5/20 - Batch: 54/55 - Val loss: 6.22627\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 6.31869\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/220 - Train loss: 6.17296\n",
      "Epoch: 6/20 - Batch: 54/220 - Train loss: 6.13783\n",
      "Epoch: 6/20 - Batch: 81/220 - Train loss: 6.20153\n",
      "Epoch: 6/20 - Batch: 108/220 - Train loss: 6.27713\n",
      "Epoch: 6/20 - Batch: 135/220 - Train loss: 6.02165\n",
      "Epoch: 6/20 - Batch: 162/220 - Train loss: 6.12327\n",
      "Epoch: 6/20 - Batch: 189/220 - Train loss: 6.11777\n",
      "Epoch: 6/20 - Batch: 216/220 - Train loss: 5.84795\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 6.12656\n",
      "\n",
      "Epoch: 6/20 - Batch: 27/55 - Val loss: 6.02175\n",
      "Epoch: 6/20 - Batch: 54/55 - Val loss: 5.92366\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 6.01562\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/220 - Train loss: 5.88047\n",
      "Epoch: 7/20 - Batch: 54/220 - Train loss: 5.84162\n",
      "Epoch: 7/20 - Batch: 81/220 - Train loss: 5.90543\n",
      "Epoch: 7/20 - Batch: 108/220 - Train loss: 6.03078\n",
      "Epoch: 7/20 - Batch: 135/220 - Train loss: 5.74907\n",
      "Epoch: 7/20 - Batch: 162/220 - Train loss: 5.91132\n",
      "Epoch: 7/20 - Batch: 189/220 - Train loss: 5.90592\n",
      "Epoch: 7/20 - Batch: 216/220 - Train loss: 5.61986\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 5.85311\n",
      "\n",
      "Epoch: 7/20 - Batch: 27/55 - Val loss: 5.81385\n",
      "Epoch: 7/20 - Batch: 54/55 - Val loss: 5.71291\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 5.81305\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/220 - Train loss: 5.67253\n",
      "Epoch: 8/20 - Batch: 54/220 - Train loss: 5.64975\n",
      "Epoch: 8/20 - Batch: 81/220 - Train loss: 5.71101\n",
      "Epoch: 8/20 - Batch: 108/220 - Train loss: 5.88112\n",
      "Epoch: 8/20 - Batch: 135/220 - Train loss: 5.55971\n",
      "Epoch: 8/20 - Batch: 162/220 - Train loss: 5.76410\n",
      "Epoch: 8/20 - Batch: 189/220 - Train loss: 5.75344\n",
      "Epoch: 8/20 - Batch: 216/220 - Train loss: 5.44817\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 5.66614\n",
      "\n",
      "Epoch: 8/20 - Batch: 27/55 - Val loss: 5.66842\n",
      "Epoch: 8/20 - Batch: 54/55 - Val loss: 5.56914\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 5.67149\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/220 - Train loss: 5.52769\n",
      "Epoch: 9/20 - Batch: 54/220 - Train loss: 5.51219\n",
      "Epoch: 9/20 - Batch: 81/220 - Train loss: 5.56784\n",
      "Epoch: 9/20 - Batch: 108/220 - Train loss: 5.75778\n",
      "Epoch: 9/20 - Batch: 135/220 - Train loss: 5.42075\n",
      "Epoch: 9/20 - Batch: 162/220 - Train loss: 5.65939\n",
      "Epoch: 9/20 - Batch: 189/220 - Train loss: 5.64176\n",
      "Epoch: 9/20 - Batch: 216/220 - Train loss: 5.31608\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 5.52686\n",
      "\n",
      "Epoch: 9/20 - Batch: 27/55 - Val loss: 5.56459\n",
      "Epoch: 9/20 - Batch: 54/55 - Val loss: 5.46413\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 5.56864\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/220 - Train loss: 5.41093\n",
      "Epoch: 10/20 - Batch: 54/220 - Train loss: 5.38941\n",
      "Epoch: 10/20 - Batch: 81/220 - Train loss: 5.46140\n",
      "Epoch: 10/20 - Batch: 108/220 - Train loss: 5.65598\n",
      "Epoch: 10/20 - Batch: 135/220 - Train loss: 5.31206\n",
      "Epoch: 10/20 - Batch: 162/220 - Train loss: 5.56745\n",
      "Epoch: 10/20 - Batch: 189/220 - Train loss: 5.54431\n",
      "Epoch: 10/20 - Batch: 216/220 - Train loss: 5.20039\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 5.41370\n",
      "\n",
      "Epoch: 10/20 - Batch: 27/55 - Val loss: 5.48523\n",
      "Epoch: 10/20 - Batch: 54/55 - Val loss: 5.38089\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 5.48680\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/220 - Train loss: 5.30520\n",
      "Epoch: 11/20 - Batch: 54/220 - Train loss: 5.28363\n",
      "Epoch: 11/20 - Batch: 81/220 - Train loss: 5.36771\n",
      "Epoch: 11/20 - Batch: 108/220 - Train loss: 5.56483\n",
      "Epoch: 11/20 - Batch: 135/220 - Train loss: 5.21004\n",
      "Epoch: 11/20 - Batch: 162/220 - Train loss: 5.48022\n",
      "Epoch: 11/20 - Batch: 189/220 - Train loss: 5.45599\n",
      "Epoch: 11/20 - Batch: 216/220 - Train loss: 5.10297\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.31254\n",
      "\n",
      "Epoch: 11/20 - Batch: 27/55 - Val loss: 5.40906\n",
      "Epoch: 11/20 - Batch: 54/55 - Val loss: 5.29736\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.40741\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/220 - Train loss: 5.20931\n",
      "Epoch: 12/20 - Batch: 54/220 - Train loss: 5.18968\n",
      "Epoch: 12/20 - Batch: 81/220 - Train loss: 5.28156\n",
      "Epoch: 12/20 - Batch: 108/220 - Train loss: 5.48537\n",
      "Epoch: 12/20 - Batch: 135/220 - Train loss: 5.11947\n",
      "Epoch: 12/20 - Batch: 162/220 - Train loss: 5.40094\n",
      "Epoch: 12/20 - Batch: 189/220 - Train loss: 5.38004\n",
      "Epoch: 12/20 - Batch: 216/220 - Train loss: 5.00827\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 5.22055\n",
      "\n",
      "Epoch: 12/20 - Batch: 27/55 - Val loss: 5.34076\n",
      "Epoch: 12/20 - Batch: 54/55 - Val loss: 5.22556\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.33739\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/220 - Train loss: 5.12338\n",
      "Epoch: 13/20 - Batch: 54/220 - Train loss: 5.11042\n",
      "Epoch: 13/20 - Batch: 81/220 - Train loss: 5.20050\n",
      "Epoch: 13/20 - Batch: 108/220 - Train loss: 5.40584\n",
      "Epoch: 13/20 - Batch: 135/220 - Train loss: 5.04033\n",
      "Epoch: 13/20 - Batch: 162/220 - Train loss: 5.32976\n",
      "Epoch: 13/20 - Batch: 189/220 - Train loss: 5.30863\n",
      "Epoch: 13/20 - Batch: 216/220 - Train loss: 4.92638\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 5.13714\n",
      "\n",
      "Epoch: 13/20 - Batch: 27/55 - Val loss: 5.28261\n",
      "Epoch: 13/20 - Batch: 54/55 - Val loss: 5.16231\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.27703\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/220 - Train loss: 5.05279\n",
      "Epoch: 14/20 - Batch: 54/220 - Train loss: 5.03926\n",
      "Epoch: 14/20 - Batch: 81/220 - Train loss: 5.13164\n",
      "Epoch: 14/20 - Batch: 108/220 - Train loss: 5.33732\n",
      "Epoch: 14/20 - Batch: 135/220 - Train loss: 4.97400\n",
      "Epoch: 14/20 - Batch: 162/220 - Train loss: 5.26594\n",
      "Epoch: 14/20 - Batch: 189/220 - Train loss: 5.24254\n",
      "Epoch: 14/20 - Batch: 216/220 - Train loss: 4.85629\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 5.06618\n",
      "\n",
      "Epoch: 14/20 - Batch: 27/55 - Val loss: 5.23752\n",
      "Epoch: 14/20 - Batch: 54/55 - Val loss: 5.11154\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.22802\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/220 - Train loss: 4.98711\n",
      "Epoch: 15/20 - Batch: 54/220 - Train loss: 4.97509\n",
      "Epoch: 15/20 - Batch: 81/220 - Train loss: 5.06980\n",
      "Epoch: 15/20 - Batch: 108/220 - Train loss: 5.27000\n",
      "Epoch: 15/20 - Batch: 135/220 - Train loss: 4.90966\n",
      "Epoch: 15/20 - Batch: 162/220 - Train loss: 5.20715\n",
      "Epoch: 15/20 - Batch: 189/220 - Train loss: 5.18862\n",
      "Epoch: 15/20 - Batch: 216/220 - Train loss: 4.79945\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 5.00127\n",
      "\n",
      "Epoch: 15/20 - Batch: 27/55 - Val loss: 5.20585\n",
      "Epoch: 15/20 - Batch: 54/55 - Val loss: 5.07259\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.19247\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/220 - Train loss: 4.93343\n",
      "Epoch: 16/20 - Batch: 54/220 - Train loss: 4.91665\n",
      "Epoch: 16/20 - Batch: 81/220 - Train loss: 5.01549\n",
      "Epoch: 16/20 - Batch: 108/220 - Train loss: 5.21161\n",
      "Epoch: 16/20 - Batch: 135/220 - Train loss: 4.85099\n",
      "Epoch: 16/20 - Batch: 162/220 - Train loss: 5.15050\n",
      "Epoch: 16/20 - Batch: 189/220 - Train loss: 5.13887\n",
      "Epoch: 16/20 - Batch: 216/220 - Train loss: 4.73391\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 4.94293\n",
      "\n",
      "Epoch: 16/20 - Batch: 27/55 - Val loss: 5.16883\n",
      "Epoch: 16/20 - Batch: 54/55 - Val loss: 5.03066\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.15181\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/220 - Train loss: 4.88210\n",
      "Epoch: 17/20 - Batch: 54/220 - Train loss: 4.86527\n",
      "Epoch: 17/20 - Batch: 81/220 - Train loss: 4.96610\n",
      "Epoch: 17/20 - Batch: 108/220 - Train loss: 5.15559\n",
      "Epoch: 17/20 - Batch: 135/220 - Train loss: 4.79911\n",
      "Epoch: 17/20 - Batch: 162/220 - Train loss: 5.10085\n",
      "Epoch: 17/20 - Batch: 189/220 - Train loss: 5.09233\n",
      "Epoch: 17/20 - Batch: 216/220 - Train loss: 4.68000\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 4.88909\n",
      "\n",
      "Epoch: 17/20 - Batch: 27/55 - Val loss: 5.14199\n",
      "Epoch: 17/20 - Batch: 54/55 - Val loss: 4.99914\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.12229\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/220 - Train loss: 4.83650\n",
      "Epoch: 18/20 - Batch: 54/220 - Train loss: 4.82300\n",
      "Epoch: 18/20 - Batch: 81/220 - Train loss: 4.92071\n",
      "Epoch: 18/20 - Batch: 108/220 - Train loss: 5.10626\n",
      "Epoch: 18/20 - Batch: 135/220 - Train loss: 4.75432\n",
      "Epoch: 18/20 - Batch: 162/220 - Train loss: 5.05436\n",
      "Epoch: 18/20 - Batch: 189/220 - Train loss: 5.04850\n",
      "Epoch: 18/20 - Batch: 216/220 - Train loss: 4.62686\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 4.84195\n",
      "\n",
      "Epoch: 18/20 - Batch: 27/55 - Val loss: 5.12486\n",
      "Epoch: 18/20 - Batch: 54/55 - Val loss: 4.97618\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.10114\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/220 - Train loss: 4.79421\n",
      "Epoch: 19/20 - Batch: 54/220 - Train loss: 4.78636\n",
      "Epoch: 19/20 - Batch: 81/220 - Train loss: 4.87916\n",
      "Epoch: 19/20 - Batch: 108/220 - Train loss: 5.05542\n",
      "Epoch: 19/20 - Batch: 135/220 - Train loss: 4.71352\n",
      "Epoch: 19/20 - Batch: 162/220 - Train loss: 5.00700\n",
      "Epoch: 19/20 - Batch: 189/220 - Train loss: 5.01170\n",
      "Epoch: 19/20 - Batch: 216/220 - Train loss: 4.58392\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 4.79775\n",
      "\n",
      "Epoch: 19/20 - Batch: 27/55 - Val loss: 5.11101\n",
      "Epoch: 19/20 - Batch: 54/55 - Val loss: 4.95930\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.08600\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/220 - Train loss: 4.75777\n",
      "Epoch: 20/20 - Batch: 54/220 - Train loss: 4.74485\n",
      "Epoch: 20/20 - Batch: 81/220 - Train loss: 4.84391\n",
      "Epoch: 20/20 - Batch: 108/220 - Train loss: 5.02235\n",
      "Epoch: 20/20 - Batch: 135/220 - Train loss: 4.69375\n",
      "Epoch: 20/20 - Batch: 162/220 - Train loss: 4.96803\n",
      "Epoch: 20/20 - Batch: 189/220 - Train loss: 4.97388\n",
      "Epoch: 20/20 - Batch: 216/220 - Train loss: 4.54326\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 4.76042\n",
      "\n",
      "Epoch: 20/20 - Batch: 27/55 - Val loss: 5.09239\n",
      "Epoch: 20/20 - Batch: 54/55 - Val loss: 4.93841\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.06577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_w2v, val_loss_w2v = train(cond_lstm_w2v, batch_size=batch_size, epochs=20, print_every=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89627b-3be6-44f7-9fda-4fdf55d6c71f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4453c45-a930-4d8b-bc28-4fd11a4cd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'weights/cond_lstm_w2v.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8800402e-da35-4466-9735-896cf7640a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cond_lstm_w2v.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc5115f3-1009-448a-9560-4200dc90928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_lstm_w2v.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbbe97-0834-42a8-bb3a-e8a7e07c1458",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b6d099f8-fcdf-4e86-abe7-74943f63bc88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.192744077335704,\n",
       "  7.001464646512812,\n",
       "  6.993771587718617,\n",
       "  6.988789254968816,\n",
       "  6.982287261702798,\n",
       "  6.963468779217113,\n",
       "  6.74697754166343,\n",
       "  6.5195257143540815,\n",
       "  6.2542088010094385,\n",
       "  6.047751118920067,\n",
       "  5.878048027645458,\n",
       "  5.74076516194777,\n",
       "  5.626420890201222,\n",
       "  5.5276961066506125,\n",
       "  5.442306908694181,\n",
       "  5.366653793508356,\n",
       "  5.297639699415727,\n",
       "  5.233629018610174,\n",
       "  5.1753698674115265,\n",
       "  5.12178896557201],\n",
       " [6.98778190612793,\n",
       "  6.992512113397772,\n",
       "  6.996249493685636,\n",
       "  6.9962212909351695,\n",
       "  6.996997313065962,\n",
       "  6.875098583915017,\n",
       "  6.638282316381281,\n",
       "  6.360084577040239,\n",
       "  6.130921606584029,\n",
       "  5.951080721074884,\n",
       "  5.810819591175426,\n",
       "  5.697640670429577,\n",
       "  5.605538593639027,\n",
       "  5.530676009438254,\n",
       "  5.471121640638872,\n",
       "  5.4142057852311565,\n",
       "  5.364971108870073,\n",
       "  5.315647749467329,\n",
       "  5.275325454365124,\n",
       "  5.240615584633567])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "54c51e5c-2b74-4cf5-8c2e-eb9dd801bb6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.155226031216708,\n",
       "  6.9719382589513605,\n",
       "  6.974679099429737,\n",
       "  6.790019997683438,\n",
       "  6.458037547631697,\n",
       "  6.126556474512274,\n",
       "  5.853110153024847,\n",
       "  5.6661373528567225,\n",
       "  5.526860568740151,\n",
       "  5.413695066625421,\n",
       "  5.312535361810164,\n",
       "  5.22055108980699,\n",
       "  5.137143674763766,\n",
       "  5.066176596554842,\n",
       "  5.001273636384444,\n",
       "  4.942927423390475,\n",
       "  4.8890940471128985,\n",
       "  4.8419455116445365,\n",
       "  4.797754567319696,\n",
       "  4.760415124893188],\n",
       " [6.994870706038042,\n",
       "  7.004690699143843,\n",
       "  6.99676533612338,\n",
       "  6.665666103363037,\n",
       "  6.3186883232810285,\n",
       "  6.015620431033048,\n",
       "  5.813047192313454,\n",
       "  5.67148740941828,\n",
       "  5.568636547435414,\n",
       "  5.486801086772572,\n",
       "  5.407413664731112,\n",
       "  5.337385429035534,\n",
       "  5.277027754350142,\n",
       "  5.228021621704102,\n",
       "  5.192472700639205,\n",
       "  5.151810004494407,\n",
       "  5.122291720997204,\n",
       "  5.101137343319979,\n",
       "  5.08600345958363,\n",
       "  5.065771007537842])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_w2v, val_loss_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91386743-60e1-496e-b89c-af5e2b8397d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0d48cb0f-1907-46d8-a5e7-b5d84d143626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss, 'r')\n",
    "\n",
    "plt.title('Conditioned LSTM Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower left')\n",
    "\n",
    "plt.yticks(range(2,9,1))\n",
    "plt.xticks(range(0,20,2))\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "plt.savefig('plots/cond_lstm_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "615d1f46-eb0d-4c02-86e2-06cac356d20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_w2v)\n",
    "plt.plot(val_loss_w2v, 'r')\n",
    "\n",
    "plt.title('Conditioned LSTM + Word2Vec Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower left')\n",
    "\n",
    "plt.yticks(range(2,9,1))\n",
    "plt.xticks(range(0,20,2))\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "plt.savefig('plots/cond_lstm_w2v_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c49c0a-0714-4859-96f7-011e26392d20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fec25ee-1384-4327-bf8f-2b574865057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token using top k sampling\n",
    "def predict(model, top_k, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_id(t)]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top n values\n",
    "    top_ids = p.argsort()[-top_k:][::-1]\n",
    "\n",
    "    # sample id of next word from top n values\n",
    "    next_id = top_ids[random.sample(range(top_k),1)[0]]\n",
    "\n",
    "    # return the value of the predicted word and the hidden state\n",
    "    return id_to_word(next_id), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for generation batch size\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model=cond_lstm, prompt='in this paper', n=10, top_k=10):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.zeros(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    words = prompt.split()\n",
    "\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, top_k, t, h)\n",
    "    \n",
    "    words.append(token)\n",
    "\n",
    "    # predict subsequent token\n",
    "    for i in range(n-1):\n",
    "        token, h = predict(model, top_k, words[-1], h)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adab21b4-3b4c-42bb-be3c-a66726aaaafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper presents it is a large number for the corpus and the first annotated corpus our corpus includes to build and'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be8a26be-7a2e-4e3c-be86-fff764e819fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here we propose a better approach to identify relevant information with high resource text generation we present this approach based on two tasks that do the system used at each time step for training for each task this is a simple solution and provides an overview is available for english in the english chinese hindi chinese text'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50, prompt='here we propose a better approach to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d63faca4-ea0b-4ba4-b0d6-9a910fb7facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even though our approach is giving good results we use an attention approach for training the model with a large margin which can also effectively exploit new data in which training and evaluation systems are based on simple models trained at different domains this paper proposes to evaluate several neural neural approaches that require large amounts corpus as well as it will lead the need towards many research purposes to address these limitations for different language generation datasets and is often used in various datasets and the performance that in order of their data will serve this paper will provide valuable research annotation resources in nlp we describe our'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=100, prompt='even though our approach is giving good results we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2ecbe7d-80f3-4b16-9838-08e0574e1c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even though our approach is giving satisfactory results it could be improved by using both the training time of text specific and a new data based evaluation study that can provide high correlation for out of source data and other language models in order in terms for low language pairs and are used to be made available we describe some challenges on this domain to facilitate the evaluation the paper analysis on a wide number to study and compare them as features for the annotation process the dataset was created at different languages'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=80, prompt='even though our approach is giving satisfactory results it could be improved by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "073d50ba-4166-4543-9009-59588b57b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in summary the present study demonstrated that by using an attention architecture that enables an efficient training strategy in a supervised model to produce multiple training sets for both different word types and words from different words the proposed system consists the proposed system which uses a model which can capture and learn knowledge for each document'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50, prompt='in summary the present study demonstrated that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e6cdec93-f9be-43e8-8453-49c5e85ee2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we present a novel approach that incorporates multi sentence relation based on an input tree and an attention mechanism for automatically linking this problem the model predicts two semantic structures to the source word and propose two types and propose to evaluate different strategies for a language based approach and that our algorithm can generate'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50, prompt='in this paper we present a novel approach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c719f92b-9460-40a6-a65e-c0394202eed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we present a novel approach for the first step of the task of identifying the sentiment and semantic relations of a sentence and a given sentence in the same sentence we present an ensemble method that uses the word level and the word embeddings and the word level and a set and then used the'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50, prompt='in this paper we present a novel approach', top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e0bb15ff-befc-40fb-95ba-33d33e199d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we present three experiments that use the word and cross sentence level analysis we introduce three models of syntactic relations from wikipedia to annotate tweets in english we have created an effective system that combines the word embedding space in this model to generate new word sentences we also present state for'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=50, prompt='we present', top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdc3ce-eab1-456c-bdea-bc9c0fd56dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "78c1fb7f-38c5-4b5d-b974-884aba2e49e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = {cond_lstm: 'Conditioned LSTM', cond_lstm_w2v: 'Conditioned LSTM + Word2Vec'}\n",
    "loss = {cond_lstm: val_loss, cond_lstm_w2v: val_loss_w2v}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8013e-ea4c-4250-9b51-5bbc19f7a3f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "36ea3e07-fa5c-48b8-9950-0299976cedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimimum validation loss within a set num of epochs\n",
    "def min_val_loss(model, max_epochs=100):\n",
    "    return min(loss[model][:max_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "215f2919-77c9-487f-acb4-2bca4b25fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss for Conditioned LSTM: 5.24062\n",
      "Perplexity for model Conditioned LSTM: 188.79\n",
      "\n",
      "Minimum validation loss for Conditioned LSTM + Word2Vec: 5.06577\n",
      "Perplexity for model Conditioned LSTM + Word2Vec: 158.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in name.keys():\n",
    "    print(\"Minimum validation loss for {}: {:.5f}\".format(name[m], min_val_loss(m, 50)))\n",
    "    print(\"Perplexity for model {}: {:.2f}\\n\".format(name[m], math.exp(min_val_loss(m, 50))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc14719-f50a-458f-ad9e-6c15de0fee50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "101f1ae7-0124-4df1-a862-6f3d7d13534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [cond_lstm, cond_lstm_w2v]\n",
    "prompts = ['a', 'the']\n",
    "n_list = [10, 20]\n",
    "k_list = [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a317c9df-c32b-42d9-98f4-1fd2fedc937d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a novel neural model for training and inference of our system\n",
      "a variety of methods and to improve these approaches in this\n",
      "a number and the annotation process and an evaluation study in this work the first attempt to provide a detailed description\n",
      "a new task to detect a question and their correct information this task has gained considerable performance due at building data\n",
      "the use in our work on our study of the annotation\n",
      "the use the results are presented at different aspects for both\n",
      "the most challenging tasks and a variety for the task of a sentence this approach has shown significant improvements in the\n",
      "the main problem that a word can provide more reliable responses to text based methods this is a case and a\n",
      "a novel model that uses neural machine networks a novel approach\n",
      "a novel framework that uses a simple attention based attention architecture\n",
      "a new system that is used as an annotation system for a given corpus for this shared problem for speech detection\n",
      "a single dataset and that a system learns an external knowledge bases to improve a new step in addition of an\n",
      "the results are also publicly used by english english english and\n",
      "the use in addition it was evaluated in three subtasks b\n",
      "the results and that of these methods were evaluated on the benchmark datasets with a large margin this work introduces two\n",
      "the use task that allows in many cases of automatic machine recognition and language adaptation for this time s knowledge can\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        for n in n_list:\n",
    "            for top_k in k_list:\n",
    "                print(generate(model=model, prompt=prompt, n=n, top_k=top_k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
