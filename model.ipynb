{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized.txt','r') as f:\n",
    "    tokenized = eval(f.read())\n",
    "    \n",
    "with open('data/tokens.txt','r') as f:\n",
    "    tokens = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f210f4c1-b171-4e41-be01-ee4dd311b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.load('data/x.npy')\n",
    "y_arr = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        x = arr_x[pos:n]\n",
    "        y = arr_y[pos:n]\n",
    "        pos = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4ae5c6-cf7a-4286-ab02-9700f0eb6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17862, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fa4c61-c375-42d9-8c23-a6a39478fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = w2v_model.wv.vectors.shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbd194-1eee-4b01-abdd-5de52fc08690",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5485 unique tokens: ['across', 'all', 'annotation', 'arabic', 'baselines']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1a2d-98a7-45c5-8dd6-7851aa323e51",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b2143a-22ba-4535-801d-37fb06ec47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "batch_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffbc817-9458-491f-a8d1-ee7d9fabb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef742f30-a219-408b-a39a-bc77dc2e8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.862*\"e\" + 0.293*\"de\" + 0.163*\"d\" + 0.125*\"les\" + 0.122*\"la\" + 0.121*\"des\" + 0.108*\"l\" + 0.095*\"et\" + 0.087*\"s\" + 0.081*\"le\"'),\n",
       " (1,\n",
       "  '0.177*\"word\" + 0.151*\"text\" + 0.137*\"learning\" + 0.135*\"using\" + 0.134*\"information\" + 0.132*\"it\" + 0.130*\"performance\" + 0.126*\"tasks\" + 0.122*\"training\" + 0.121*\"or\"'),\n",
       " (2,\n",
       "  '-0.777*\"word\" + -0.237*\"embeddings\" + -0.202*\"words\" + 0.142*\"text\" + -0.107*\"languages\" + 0.100*\"domain\" + 0.096*\"knowledge\" + 0.094*\"dataset\" + 0.091*\"question\" + 0.083*\"training\"'),\n",
       " (3,\n",
       "  '-0.663*\"translation\" + -0.271*\"english\" + -0.270*\"machine\" + -0.168*\"nmt\" + -0.158*\"languages\" + 0.126*\"information\" + -0.118*\"parallel\" + -0.117*\"mt\" + 0.110*\"knowledge\" + -0.095*\"source\"'),\n",
       " (4,\n",
       "  '0.305*\"corpus\" + -0.185*\"neural\" + -0.184*\"training\" + 0.180*\"languages\" + -0.162*\"translation\" + 0.154*\"speech\" + -0.152*\"tasks\" + -0.144*\"propose\" + 0.135*\"system\" + -0.134*\"sentence\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b70180a-f67f-4092-9569-40997bf0b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('translation', -0.6633304780443076),\n",
       " ('english', -0.27091113943706513),\n",
       " ('machine', -0.2704780448384449),\n",
       " ('nmt', -0.16824400154641173),\n",
       " ('languages', -0.15798055200160313),\n",
       " ('information', 0.1264872744898781),\n",
       " ('parallel', -0.11846117411026573),\n",
       " ('mt', -0.1174195417046398),\n",
       " ('knowledge', 0.11031805985128075),\n",
       " ('source', -0.09548953213364667)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d2d4cc-f09c-4096-9c53-1e6d7d05eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(lsi.projection.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62faf-ac59-4f61-87a7-bf2de09c82cb",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fd01de-ac01-43ef-a370-6bd234108e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics = PCA(n_components=n_layers*batch_size, svd_solver='full').fit_transform(trans_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d50757e-d8ca-482a-88b7-99636cf4f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1570d-87e9-4581-9f83-3c9b54956822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab6422-e377-4938-8384-5dca63116fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "385453f3-8dc1-46e1-b9c2-d37088ab4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionedLSTM(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm = ConditionedLSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e0f1-f4e5-46cf-afed-4f9941b79994",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Condtiioned LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b0329-f90a-4121-9308-931b5c2b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondLSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "cond_lstm_w2v = CondLSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e7eba-ecc4-4c76-b121-7b59b496bbdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcf8113d-a604-4d21-89fc-e3505f0a9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4497a4-357f-47eb-9466-5e3001770e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # loss values\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        batch = 0\n",
    "        \n",
    "        epoch_loss= []\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_arr, y_arr, batch_size):\n",
    "            batch += 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "            \n",
    "            # add current batch loss to epoch loss list\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()    \n",
    "            \n",
    "            \n",
    "            # show epoch - batch - loss every n batches\n",
    "            if batch % print_every == 0:\n",
    "                \n",
    "                tot_batches = int(x_arr.shape[0] / batch_size)\n",
    "            \n",
    "                print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                      \"Batch: {}/{} -\".format(batch, tot_batches),\n",
    "                      \"Loss: {}\".format(loss))\n",
    "        \n",
    "        # print loss at the end of each epoch\n",
    "        print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "              \"Loss: {}\".format(loss))\n",
    "        \n",
    "        # save average epoch loss\n",
    "        losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "        \n",
    "    my_plot(np.linspace(1, epochs, epochs).astype(int), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "path = 'weights/cond_lstm_batch.pt'\n",
    "loss = 0.2\n",
    "\n",
    "optimizer = torch.optim.Adam(cond_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': cond_lstm.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83fc79af-27cf-4951-9d4d-6f47b15a8e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 - Batch: 5/91 - Loss: 9.675541877746582\n",
      "Epoch: 1/10 - Batch: 10/91 - Loss: 7.9967241287231445\n",
      "Epoch: 1/10 - Batch: 15/91 - Loss: 7.240325450897217\n",
      "Epoch: 1/10 - Batch: 20/91 - Loss: 7.109735488891602\n",
      "Epoch: 1/10 - Batch: 25/91 - Loss: 7.094906806945801\n",
      "Epoch: 1/10 - Batch: 30/91 - Loss: 7.0765380859375\n",
      "Epoch: 1/10 - Batch: 35/91 - Loss: 7.051101207733154\n",
      "Epoch: 1/10 - Batch: 40/91 - Loss: 6.820530891418457\n",
      "Epoch: 1/10 - Batch: 45/91 - Loss: 6.965612411499023\n",
      "Epoch: 1/10 - Batch: 50/91 - Loss: 6.938690185546875\n",
      "Epoch: 1/10 - Batch: 55/91 - Loss: 6.932405471801758\n",
      "Epoch: 1/10 - Batch: 60/91 - Loss: 6.855745315551758\n",
      "Epoch: 1/10 - Batch: 65/91 - Loss: 6.872091293334961\n",
      "Epoch: 1/10 - Batch: 70/91 - Loss: 6.94943380355835\n",
      "Epoch: 1/10 - Batch: 75/91 - Loss: 7.250450611114502\n",
      "Epoch: 1/10 - Batch: 80/91 - Loss: 6.856518745422363\n",
      "Epoch: 1/10 - Batch: 85/91 - Loss: 6.942818641662598\n",
      "Epoch: 1/10 - Batch: 90/91 - Loss: 6.789602756500244\n",
      "Epoch: 1/10 - Loss: 8.872097969055176\n",
      "Epoch: 2/10 - Batch: 5/91 - Loss: 7.172055244445801\n",
      "Epoch: 2/10 - Batch: 10/91 - Loss: 6.920770645141602\n",
      "Epoch: 2/10 - Batch: 15/91 - Loss: 6.984338760375977\n",
      "Epoch: 2/10 - Batch: 20/91 - Loss: 7.0232133865356445\n",
      "Epoch: 2/10 - Batch: 25/91 - Loss: 6.993063449859619\n",
      "Epoch: 2/10 - Batch: 30/91 - Loss: 6.9395880699157715\n",
      "Epoch: 2/10 - Batch: 35/91 - Loss: 6.967512607574463\n",
      "Epoch: 2/10 - Batch: 40/91 - Loss: 6.7837395668029785\n",
      "Epoch: 2/10 - Batch: 45/91 - Loss: 6.898657321929932\n",
      "Epoch: 2/10 - Batch: 50/91 - Loss: 6.894094467163086\n",
      "Epoch: 2/10 - Batch: 55/91 - Loss: 6.889086723327637\n",
      "Epoch: 2/10 - Batch: 60/91 - Loss: 6.829265594482422\n",
      "Epoch: 2/10 - Batch: 65/91 - Loss: 6.843580722808838\n",
      "Epoch: 2/10 - Batch: 70/91 - Loss: 6.9159016609191895\n",
      "Epoch: 2/10 - Batch: 75/91 - Loss: 7.208822250366211\n",
      "Epoch: 2/10 - Batch: 80/91 - Loss: 6.821870803833008\n",
      "Epoch: 2/10 - Batch: 85/91 - Loss: 6.91297721862793\n",
      "Epoch: 2/10 - Batch: 90/91 - Loss: 6.769954681396484\n",
      "Epoch: 2/10 - Loss: 8.82637882232666\n",
      "Epoch: 3/10 - Batch: 5/91 - Loss: 7.143234729766846\n",
      "Epoch: 3/10 - Batch: 10/91 - Loss: 6.920608043670654\n",
      "Epoch: 3/10 - Batch: 15/91 - Loss: 6.986735820770264\n",
      "Epoch: 3/10 - Batch: 20/91 - Loss: 7.028891563415527\n",
      "Epoch: 3/10 - Batch: 25/91 - Loss: 6.982065200805664\n",
      "Epoch: 3/10 - Batch: 30/91 - Loss: 6.9302520751953125\n",
      "Epoch: 3/10 - Batch: 35/91 - Loss: 6.97274923324585\n",
      "Epoch: 3/10 - Batch: 40/91 - Loss: 6.775819301605225\n",
      "Epoch: 3/10 - Batch: 45/91 - Loss: 6.884729385375977\n",
      "Epoch: 3/10 - Batch: 50/91 - Loss: 6.8842549324035645\n",
      "Epoch: 3/10 - Batch: 55/91 - Loss: 6.877828598022461\n",
      "Epoch: 3/10 - Batch: 60/91 - Loss: 6.814723968505859\n",
      "Epoch: 3/10 - Batch: 65/91 - Loss: 6.8299407958984375\n",
      "Epoch: 3/10 - Batch: 70/91 - Loss: 6.911110877990723\n",
      "Epoch: 3/10 - Batch: 75/91 - Loss: 7.1938676834106445\n",
      "Epoch: 3/10 - Batch: 80/91 - Loss: 6.807589530944824\n",
      "Epoch: 3/10 - Batch: 85/91 - Loss: 6.895166397094727\n",
      "Epoch: 3/10 - Batch: 90/91 - Loss: 6.759451866149902\n",
      "Epoch: 3/10 - Loss: 8.80385971069336\n",
      "Epoch: 4/10 - Batch: 5/91 - Loss: 7.1209330558776855\n",
      "Epoch: 4/10 - Batch: 10/91 - Loss: 6.904876232147217\n",
      "Epoch: 4/10 - Batch: 15/91 - Loss: 6.9816718101501465\n",
      "Epoch: 4/10 - Batch: 20/91 - Loss: 7.013036727905273\n",
      "Epoch: 4/10 - Batch: 25/91 - Loss: 6.9732160568237305\n",
      "Epoch: 4/10 - Batch: 30/91 - Loss: 6.915963649749756\n",
      "Epoch: 4/10 - Batch: 35/91 - Loss: 6.960684299468994\n",
      "Epoch: 4/10 - Batch: 40/91 - Loss: 6.7663679122924805\n",
      "Epoch: 4/10 - Batch: 45/91 - Loss: 6.8691229820251465\n",
      "Epoch: 4/10 - Batch: 50/91 - Loss: 6.8698811531066895\n",
      "Epoch: 4/10 - Batch: 55/91 - Loss: 6.867639064788818\n",
      "Epoch: 4/10 - Batch: 60/91 - Loss: 6.806879997253418\n",
      "Epoch: 4/10 - Batch: 65/91 - Loss: 6.820713520050049\n",
      "Epoch: 4/10 - Batch: 70/91 - Loss: 6.888025283813477\n",
      "Epoch: 4/10 - Batch: 75/91 - Loss: 7.176219463348389\n",
      "Epoch: 4/10 - Batch: 80/91 - Loss: 6.799281597137451\n",
      "Epoch: 4/10 - Batch: 85/91 - Loss: 6.8881001472473145\n",
      "Epoch: 4/10 - Batch: 90/91 - Loss: 6.743085861206055\n",
      "Epoch: 4/10 - Loss: 8.779366493225098\n",
      "Epoch: 5/10 - Batch: 5/91 - Loss: 7.102026462554932\n",
      "Epoch: 5/10 - Batch: 10/91 - Loss: 6.8944830894470215\n",
      "Epoch: 5/10 - Batch: 15/91 - Loss: 6.961765766143799\n",
      "Epoch: 5/10 - Batch: 20/91 - Loss: 6.991045951843262\n",
      "Epoch: 5/10 - Batch: 25/91 - Loss: 6.956573009490967\n",
      "Epoch: 5/10 - Batch: 30/91 - Loss: 6.9087233543396\n",
      "Epoch: 5/10 - Batch: 35/91 - Loss: 6.951533317565918\n",
      "Epoch: 5/10 - Batch: 40/91 - Loss: 6.768608570098877\n",
      "Epoch: 5/10 - Batch: 45/91 - Loss: 6.852049350738525\n",
      "Epoch: 5/10 - Batch: 50/91 - Loss: 6.862530708312988\n",
      "Epoch: 5/10 - Batch: 55/91 - Loss: 6.851872444152832\n",
      "Epoch: 5/10 - Batch: 60/91 - Loss: 6.79776668548584\n",
      "Epoch: 5/10 - Batch: 65/91 - Loss: 6.810737133026123\n",
      "Epoch: 5/10 - Batch: 70/91 - Loss: 6.884155750274658\n",
      "Epoch: 5/10 - Batch: 75/91 - Loss: 7.16435432434082\n",
      "Epoch: 5/10 - Batch: 80/91 - Loss: 6.788632392883301\n",
      "Epoch: 5/10 - Batch: 85/91 - Loss: 6.875687122344971\n",
      "Epoch: 5/10 - Batch: 90/91 - Loss: 6.741275787353516\n",
      "Epoch: 5/10 - Loss: 8.738227844238281\n",
      "Epoch: 6/10 - Batch: 5/91 - Loss: 7.080593109130859\n",
      "Epoch: 6/10 - Batch: 10/91 - Loss: 6.887163162231445\n",
      "Epoch: 6/10 - Batch: 15/91 - Loss: 6.950740814208984\n",
      "Epoch: 6/10 - Batch: 20/91 - Loss: 6.9882307052612305\n",
      "Epoch: 6/10 - Batch: 25/91 - Loss: 6.949029922485352\n",
      "Epoch: 6/10 - Batch: 30/91 - Loss: 6.905079364776611\n",
      "Epoch: 6/10 - Batch: 35/91 - Loss: 6.94167423248291\n",
      "Epoch: 6/10 - Batch: 40/91 - Loss: 6.761696815490723\n",
      "Epoch: 6/10 - Batch: 45/91 - Loss: 6.849828720092773\n",
      "Epoch: 6/10 - Batch: 50/91 - Loss: 6.859481334686279\n",
      "Epoch: 6/10 - Batch: 55/91 - Loss: 6.849508762359619\n",
      "Epoch: 6/10 - Batch: 60/91 - Loss: 6.789823532104492\n",
      "Epoch: 6/10 - Batch: 65/91 - Loss: 6.806514263153076\n",
      "Epoch: 6/10 - Batch: 70/91 - Loss: 6.874269485473633\n",
      "Epoch: 6/10 - Batch: 75/91 - Loss: 7.1462297439575195\n",
      "Epoch: 6/10 - Batch: 80/91 - Loss: 6.7834792137146\n",
      "Epoch: 6/10 - Batch: 85/91 - Loss: 6.8722734451293945\n",
      "Epoch: 6/10 - Batch: 90/91 - Loss: 6.733675003051758\n",
      "Epoch: 6/10 - Loss: 8.730110168457031\n",
      "Epoch: 7/10 - Batch: 5/91 - Loss: 7.074655055999756\n",
      "Epoch: 7/10 - Batch: 10/91 - Loss: 6.872133731842041\n",
      "Epoch: 7/10 - Batch: 15/91 - Loss: 6.948725700378418\n",
      "Epoch: 7/10 - Batch: 20/91 - Loss: 6.973194599151611\n",
      "Epoch: 7/10 - Batch: 25/91 - Loss: 6.938226222991943\n",
      "Epoch: 7/10 - Batch: 30/91 - Loss: 6.905774116516113\n",
      "Epoch: 7/10 - Batch: 35/91 - Loss: 6.931838512420654\n",
      "Epoch: 7/10 - Batch: 40/91 - Loss: 6.7574334144592285\n",
      "Epoch: 7/10 - Batch: 45/91 - Loss: 6.83924674987793\n",
      "Epoch: 7/10 - Batch: 50/91 - Loss: 6.8544087409973145\n",
      "Epoch: 7/10 - Batch: 55/91 - Loss: 6.8453803062438965\n",
      "Epoch: 7/10 - Batch: 60/91 - Loss: 6.785107612609863\n",
      "Epoch: 7/10 - Batch: 65/91 - Loss: 6.799582004547119\n",
      "Epoch: 7/10 - Batch: 70/91 - Loss: 6.873188495635986\n",
      "Epoch: 7/10 - Batch: 75/91 - Loss: 7.137956142425537\n",
      "Epoch: 7/10 - Batch: 80/91 - Loss: 6.777951717376709\n",
      "Epoch: 7/10 - Batch: 85/91 - Loss: 6.864526748657227\n",
      "Epoch: 7/10 - Batch: 90/91 - Loss: 6.728663921356201\n",
      "Epoch: 7/10 - Loss: 8.711384773254395\n",
      "Epoch: 8/10 - Batch: 5/91 - Loss: 7.061105728149414\n",
      "Epoch: 8/10 - Batch: 10/91 - Loss: 6.8715500831604\n",
      "Epoch: 8/10 - Batch: 15/91 - Loss: 6.9379706382751465\n",
      "Epoch: 8/10 - Batch: 20/91 - Loss: 6.973259449005127\n",
      "Epoch: 8/10 - Batch: 25/91 - Loss: 6.937158584594727\n",
      "Epoch: 8/10 - Batch: 30/91 - Loss: 6.902729034423828\n",
      "Epoch: 8/10 - Batch: 35/91 - Loss: 6.926759243011475\n",
      "Epoch: 8/10 - Batch: 40/91 - Loss: 6.7560038566589355\n",
      "Epoch: 8/10 - Batch: 45/91 - Loss: 6.83620548248291\n",
      "Epoch: 8/10 - Batch: 50/91 - Loss: 6.8513383865356445\n",
      "Epoch: 8/10 - Batch: 55/91 - Loss: 6.843690872192383\n",
      "Epoch: 8/10 - Batch: 60/91 - Loss: 6.782593250274658\n",
      "Epoch: 8/10 - Batch: 65/91 - Loss: 6.7977986335754395\n",
      "Epoch: 8/10 - Batch: 70/91 - Loss: 6.869708061218262\n",
      "Epoch: 8/10 - Batch: 75/91 - Loss: 7.136944770812988\n",
      "Epoch: 8/10 - Batch: 80/91 - Loss: 6.782960414886475\n",
      "Epoch: 8/10 - Batch: 85/91 - Loss: 6.864243984222412\n",
      "Epoch: 8/10 - Batch: 90/91 - Loss: 6.7250542640686035\n",
      "Epoch: 8/10 - Loss: 8.72704792022705\n",
      "Epoch: 9/10 - Batch: 5/91 - Loss: 7.060266494750977\n",
      "Epoch: 9/10 - Batch: 10/91 - Loss: 6.865083694458008\n",
      "Epoch: 9/10 - Batch: 15/91 - Loss: 6.935697555541992\n",
      "Epoch: 9/10 - Batch: 20/91 - Loss: 6.969022274017334\n",
      "Epoch: 9/10 - Batch: 25/91 - Loss: 6.941359043121338\n",
      "Epoch: 9/10 - Batch: 30/91 - Loss: 6.893181324005127\n",
      "Epoch: 9/10 - Batch: 35/91 - Loss: 6.924900054931641\n",
      "Epoch: 9/10 - Batch: 40/91 - Loss: 6.7596635818481445\n",
      "Epoch: 9/10 - Batch: 45/91 - Loss: 6.830927848815918\n",
      "Epoch: 9/10 - Batch: 50/91 - Loss: 6.8482232093811035\n",
      "Epoch: 9/10 - Batch: 55/91 - Loss: 6.841364860534668\n",
      "Epoch: 9/10 - Batch: 60/91 - Loss: 6.782230854034424\n",
      "Epoch: 9/10 - Batch: 65/91 - Loss: 6.801041126251221\n",
      "Epoch: 9/10 - Batch: 70/91 - Loss: 6.86934757232666\n",
      "Epoch: 9/10 - Batch: 75/91 - Loss: 7.1309027671813965\n",
      "Epoch: 9/10 - Batch: 80/91 - Loss: 6.777273178100586\n",
      "Epoch: 9/10 - Batch: 85/91 - Loss: 6.865149974822998\n",
      "Epoch: 9/10 - Batch: 90/91 - Loss: 6.718753814697266\n",
      "Epoch: 9/10 - Loss: 8.713338851928711\n",
      "Epoch: 10/10 - Batch: 5/91 - Loss: 7.043908596038818\n",
      "Epoch: 10/10 - Batch: 10/91 - Loss: 6.860806941986084\n",
      "Epoch: 10/10 - Batch: 15/91 - Loss: 6.934823036193848\n",
      "Epoch: 10/10 - Batch: 20/91 - Loss: 6.960122585296631\n",
      "Epoch: 10/10 - Batch: 25/91 - Loss: 6.927112579345703\n",
      "Epoch: 10/10 - Batch: 30/91 - Loss: 6.891067028045654\n",
      "Epoch: 10/10 - Batch: 35/91 - Loss: 6.923753261566162\n",
      "Epoch: 10/10 - Batch: 40/91 - Loss: 6.758593559265137\n",
      "Epoch: 10/10 - Batch: 45/91 - Loss: 6.831287384033203\n",
      "Epoch: 10/10 - Batch: 50/91 - Loss: 6.846498966217041\n",
      "Epoch: 10/10 - Batch: 55/91 - Loss: 6.841842174530029\n",
      "Epoch: 10/10 - Batch: 60/91 - Loss: 6.784666061401367\n",
      "Epoch: 10/10 - Batch: 65/91 - Loss: 6.798748970031738\n",
      "Epoch: 10/10 - Batch: 70/91 - Loss: 6.870053291320801\n",
      "Epoch: 10/10 - Batch: 75/91 - Loss: 7.127758026123047\n",
      "Epoch: 10/10 - Batch: 80/91 - Loss: 6.784569263458252\n",
      "Epoch: 10/10 - Batch: 85/91 - Loss: 6.871309280395508\n",
      "Epoch: 10/10 - Batch: 90/91 - Loss: 6.717109203338623\n",
      "Epoch: 10/10 - Loss: 8.72475814819336\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cond_lstm, batch_size=batch_size, epochs=epochs, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2cfe-fa23-43f0-ac9a-873c02886fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec25ee-1384-4327-bf8f-2b574865057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(model, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_id(t)]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top n values\n",
    "    top_ids = p.argsort()[-10:][::-1]\n",
    "\n",
    "    # sample id of next word from top n values\n",
    "    next_id = top_ids[random.sample([0,1,2,3,4,5,6,7,8,9],1)[0]]\n",
    "\n",
    "    # return the value of the predicted word and the hidden state\n",
    "    return id_to_word(next_id), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for generation batch size\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model=cond_lstm, n=10, prompt='in this paper'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.ones(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    words = prompt.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, t, h)\n",
    "    \n",
    "    words.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(n-1):\n",
    "        token, h = predict(model, words[-1], h)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8dd9-4a9c-4059-8c23-28e753ca0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350a124-6cd4-4cef-8261-8ca8aeea2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(n=100, prompt='in this paper we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab21b4-3b4c-42bb-be3c-a66726aaaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a26be-7a2e-4e3c-be86-fff764e819fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(n=100, prompt='in this paper we')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec48bd6-4fd3-4d53-9d9a-ed9f4d4c048b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
