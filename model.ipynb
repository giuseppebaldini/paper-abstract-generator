{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {},
   "source": [
    "# Conditional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import preprocessing\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac9ef4-d424-4830-a904-8ffcbae0c133",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = preprocessing.tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5429687-ca25-4206-a822-f7cdc5062eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "\n",
    "for a in tokenized:\n",
    "    joined = ' '.join(a)\n",
    "    abstracts.append(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af0b257-6d6b-4275-9d77-6709061c6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words = int(len(preprocessing.tokens)/len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e068ac-fa9a-463e-84a8-7b090d0a322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = avg_words + 1\n",
    "\n",
    "def sequence(l, n):\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "        \n",
    "seqs = list(sequence(preprocessing.tokens, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29944832-1ae3-4c01-91df-b71c148a41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "    x.append(\" \".join(s[:-1]))\n",
    "    y.append(\" \".join(s[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "fe2261ca-1b39-45cb-9c33-db5a846ecbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6643, 'bmw')"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "counter = 0\n",
    "\n",
    "for w in set(\" \".join(abstracts).split()):\n",
    "    int2token[counter] = w\n",
    "    counter += 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: a for a, t in int2token.items()}\n",
    "\n",
    "token2int[\"the\"], int2token[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "3519a103-be94-4618-b692-fd37a944ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37613"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "cce3a579-a521-48a5-aba1-8b01e7e1f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "    return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int_all = [get_integer_seq(i) for i in x]\n",
    "y_int_all = [get_integer_seq(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "7b325c81-603e-4424-b0b3-4c571d2b2c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262490, 262490)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_int_all),len(y_int_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2057e88f-3e11-4c92-a4a7-411ce3f85b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all sequences not == len_seq\n",
    "\n",
    "x_int = list(filter(lambda x: (len(x) == seq_len-1), x_int_all))\n",
    "y_int = list(filter(lambda y: (len(y) == seq_len-1), y_int_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e76882-4f7a-4d84-a9de-9d10e83984e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262489, 262489)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_int),len(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e948cd60-1d6e-41d3-9146-27c717950f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        x = arr_x[prv:n]\n",
    "        y = arr_y[prv:n]\n",
    "        prv = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "047fc657-6568-4780-9e62-1a88cdadde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_abs_model = KeyedVectors.load('word2vec_arxiv_abstracts.model', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37613 100\n"
     ]
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_abs_model.wv.vectors.shape\n",
    "print(vocab_size,emdedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "d6ad5ef3-243d-42b8-858d-aa33b49e441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = w2v_abs_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "b5508300-ce48-4a73-9c50-edbb9dd06959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37613, 100)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6afdc0-f0ee-44fc-810a-2eb2f2cd67e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-442-d89ee797d82c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             self.add_lifecycle_event(\n\u001b[0;32m     81\u001b[0m                 \u001b[1;34m\"created\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"built %s from %i documents (total %i corpus positions)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37582"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(37582 unique tokens: ['adaptation', 'address', 'albert', 'approach', 'art']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c389e-0e7b-4075-a25b-35fd0c3ec7c3",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a23f290-e1c1-4802-a2d2-78c99c312940",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=20, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "320931a9-1e16-4890-8fbc-dcd1311646c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.862*\"e\" + 0.299*\"de\" + 0.155*\"d\" + 0.132*\"la\" + 0.123*\"des\" + 0.122*\"les\" + 0.101*\"l\" + 0.097*\"et\" + 0.088*\"s\" + 0.082*\"le\"'),\n",
       " (1,\n",
       "  '0.218*\"word\" + 0.143*\"using\" + 0.138*\"corpus\" + 0.137*\"it\" + 0.134*\"text\" + 0.132*\"information\" + 0.128*\"translation\" + 0.123*\"neural\" + 0.122*\"or\" + 0.121*\"system\"'),\n",
       " (2,\n",
       "  '-0.761*\"word\" + -0.264*\"embeddings\" + 0.249*\"corpus\" + -0.178*\"words\" + 0.125*\"annotation\" + 0.115*\"system\" + -0.093*\"representations\" + -0.091*\"embedding\" + 0.091*\"translation\" + 0.085*\"text\"'),\n",
       " (3,\n",
       "  '0.619*\"translation\" + 0.287*\"machine\" + -0.248*\"corpus\" + 0.229*\"neural\" + 0.205*\"nmt\" + 0.157*\"system\" + -0.132*\"annotation\" + 0.122*\"english\" + -0.117*\"text\" + 0.099*\"mt\"'),\n",
       " (4,\n",
       "  '0.458*\"corpus\" + 0.302*\"word\" + 0.289*\"translation\" + -0.195*\"neural\" + 0.171*\"languages\" + 0.160*\"annotation\" + -0.153*\"learning\" + 0.152*\"english\" + -0.129*\"state\" + -0.123*\"network\"'),\n",
       " (5,\n",
       "  '-0.537*\"sentiment\" + 0.301*\"corpus\" + -0.269*\"analysis\" + 0.228*\"semantic\" + -0.140*\"text\" + -0.137*\"or\" + -0.134*\"translation\" + 0.122*\"system\" + -0.108*\"will\" + 0.106*\"relation\"'),\n",
       " (6,\n",
       "  '0.418*\"corpus\" + -0.406*\"semantic\" + 0.233*\"sentiment\" + 0.203*\"neural\" + -0.179*\"knowledge\" + -0.153*\"languages\" + -0.126*\"relations\" + 0.125*\"domain\" + 0.120*\"network\" + -0.117*\"between\"'),\n",
       " (7,\n",
       "  '0.688*\"system\" + -0.181*\"neural\" + 0.165*\"systems\" + -0.164*\"corpus\" + -0.160*\"semantic\" + -0.140*\"sentence\" + -0.140*\"annotation\" + -0.139*\"translation\" + -0.135*\"attention\" + 0.129*\"word\"'),\n",
       " (8,\n",
       "  '0.548*\"languages\" + -0.223*\"information\" + 0.214*\"training\" + -0.180*\"system\" + 0.161*\"learning\" + 0.151*\"approach\" + 0.141*\"cross\" + 0.139*\"english\" + -0.138*\"word\" + -0.131*\"corpus\"'),\n",
       " (9,\n",
       "  '-0.353*\"semantic\" + 0.292*\"learning\" + -0.247*\"sentiment\" + -0.234*\"features\" + -0.230*\"system\" + 0.172*\"knowledge\" + -0.169*\"sentence\" + 0.160*\"nlp\" + -0.146*\"information\" + 0.141*\"training\"'),\n",
       " (10,\n",
       "  '0.402*\"knowledge\" + 0.363*\"domain\" + -0.210*\"neural\" + -0.188*\"speech\" + 0.188*\"text\" + -0.160*\"features\" + -0.157*\"languages\" + 0.155*\"method\" + 0.155*\"translation\" + 0.146*\"sentiment\"'),\n",
       " (11,\n",
       "  '-0.567*\"text\" + 0.328*\"sentiment\" + 0.267*\"domain\" + 0.213*\"semantic\" + -0.208*\"information\" + 0.188*\"learning\" + -0.180*\"languages\" + 0.156*\"knowledge\" + 0.130*\"analysis\" + 0.119*\"system\"'),\n",
       " (12,\n",
       "  '-0.381*\"information\" + -0.345*\"knowledge\" + 0.337*\"semantic\" + 0.336*\"text\" + -0.207*\"languages\" + 0.194*\"learning\" + -0.189*\"de\" + -0.185*\"it\" + 0.129*\"e\" + 0.109*\"tasks\"'),\n",
       " (13,\n",
       "  '0.433*\"text\" + 0.201*\"semantic\" + 0.193*\"knowledge\" + -0.185*\"evaluation\" + 0.177*\"de\" + 0.171*\"neural\" + -0.163*\"method\" + 0.143*\"features\" + 0.140*\"et\" + -0.130*\"approach\"'),\n",
       " (14,\n",
       "  '0.466*\"et\" + 0.383*\"de\" + -0.330*\"e\" + 0.217*\"al\" + -0.203*\"neural\" + 0.174*\"d\" + 0.170*\"des\" + -0.142*\"knowledge\" + 0.121*\"en\" + 0.114*\"la\"'),\n",
       " (15,\n",
       "  '0.336*\"features\" + -0.308*\"question\" + 0.307*\"learning\" + 0.276*\"information\" + -0.230*\"questions\" + -0.188*\"text\" + -0.158*\"et\" + -0.157*\"sentiment\" + -0.151*\"answer\" + 0.151*\"it\"'),\n",
       " (16,\n",
       "  '-0.580*\"de\" + 0.370*\"et\" + 0.232*\"al\" + -0.193*\"d\" + 0.179*\"les\" + 0.156*\"e\" + -0.126*\"learning\" + -0.125*\"semantic\" + 0.123*\"nmt\" + 0.110*\"domain\"'),\n",
       " (17,\n",
       "  '-0.473*\"annotation\" + 0.260*\"features\" + -0.242*\"information\" + 0.230*\"words\" + -0.188*\"learning\" + -0.150*\"sentiment\" + 0.145*\"domain\" + -0.145*\"system\" + 0.130*\"have\" + -0.124*\"tasks\"'),\n",
       " (18,\n",
       "  '-0.381*\"features\" + 0.366*\"domain\" + -0.245*\"knowledge\" + 0.224*\"it\" + 0.187*\"words\" + 0.181*\"system\" + -0.166*\"using\" + 0.126*\"parsing\" + -0.125*\"annotation\" + 0.108*\"information\"'),\n",
       " (19,\n",
       "  '0.428*\"annotation\" + 0.263*\"domain\" + -0.242*\"corpus\" + -0.222*\"method\" + -0.221*\"semantic\" + 0.214*\"features\" + -0.194*\"learning\" + -0.174*\"using\" + -0.167*\"sentiment\" + 0.152*\"de\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36da2234-e893-4f90-82e3-f45cae8734df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('features', -0.3812280153671461),\n",
       " ('domain', 0.3664241332007311),\n",
       " ('knowledge', -0.24539420945523566),\n",
       " ('it', 0.22406247594077064),\n",
       " ('words', 0.18685416476785296),\n",
       " ('system', 0.1808808794074729),\n",
       " ('using', -0.16605763582043478),\n",
       " ('parsing', 0.1259840150474611),\n",
       " ('annotation', -0.12515906329679294),\n",
       " ('information', 0.1075151959073353)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(18, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6ba013f-0207-4346-a922-8f83271b4a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1d72a800358>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e86a9f8-69a5-4484-953b-a5374b71eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_representation = lsi.projection.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27085349-f8e1-412d-b6c1-d662b59af280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37582, 20)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44c4a76b-9554-4484-977e-94498c31dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = torch.FloatTensor(topic_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa74f5-3f63-414f-bc5f-c96ed8b909ca",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ae756-fc0a-4749-8d5e-2ff2974d7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(topic_representation)\n",
    "trans_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06385ae2-3efb-41cf-addd-c05801390ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "pca_topics = pca.fit_transform(trans_topics)\n",
    "pca_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa179f-85a3-44bb-96da-4cad18b1861a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407451da-3962-4e32-951a-8ddf01129eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics_reshaped = pca_trans.reshape(2, 32, 256)\n",
    "type(pca_topics_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5c04b-471d-4557-ba19-badf9bad12d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(100, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.ones(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "b8b2143a-22ba-4535-801d-37fb06ec47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "batch_size = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0ad5be56-300f-4513-9a3d-61e6281d095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)\n",
    "trans_topics = np.transpose(lsi.projection.u)\n",
    "\n",
    "# PCA\n",
    "pca_topics = PCA(n_components=n_layers * batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionedLSTM(\n",
      "  (emb_layer): Embedding(37613, 100)\n",
      "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=37613, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "model = ConditionedLSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "be4497a4-357f-47eb-9466-5e3001770e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to CPU\n",
    "    model.cpu()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # initialize hidden state\n",
    "            h = model.init_hidden(batch_size)\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cpu(), targets.cpu()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                    \"Step: {} -\".format(counter),\n",
    "                    \"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "path = 'weights/cond_lstm.pt'\n",
    "loss = 0.2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16d0cf0d-d128-4eb5-b5b9-8209f006a0ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 - Step: 1 - Loss: 11.459970474243164\n",
      "Epoch: 1/1 - Step: 2 - Loss: 11.45486831665039\n",
      "Epoch: 1/1 - Step: 3 - Loss: 11.443846702575684\n",
      "Epoch: 1/1 - Step: 4 - Loss: 11.432527542114258\n",
      "Epoch: 1/1 - Step: 5 - Loss: 11.419405937194824\n",
      "Epoch: 1/1 - Step: 6 - Loss: 11.404845237731934\n",
      "Epoch: 1/1 - Step: 7 - Loss: 11.381332397460938\n",
      "Epoch: 1/1 - Step: 8 - Loss: 11.355045318603516\n",
      "Epoch: 1/1 - Step: 9 - Loss: 11.334622383117676\n",
      "Epoch: 1/1 - Step: 10 - Loss: 11.306285858154297\n",
      "Epoch: 1/1 - Step: 11 - Loss: 11.26911449432373\n",
      "Epoch: 1/1 - Step: 12 - Loss: 11.223613739013672\n",
      "Epoch: 1/1 - Step: 13 - Loss: 11.174215316772461\n",
      "Epoch: 1/1 - Step: 14 - Loss: 11.122401237487793\n",
      "Epoch: 1/1 - Step: 15 - Loss: 11.057987213134766\n",
      "Epoch: 1/1 - Step: 16 - Loss: 10.976066589355469\n",
      "Epoch: 1/1 - Step: 17 - Loss: 10.886181831359863\n",
      "Epoch: 1/1 - Step: 18 - Loss: 10.78087329864502\n",
      "Epoch: 1/1 - Step: 19 - Loss: 10.678793907165527\n",
      "Epoch: 1/1 - Step: 20 - Loss: 10.543740272521973\n",
      "Epoch: 1/1 - Step: 21 - Loss: 10.412931442260742\n",
      "Epoch: 1/1 - Step: 22 - Loss: 10.298727035522461\n",
      "Epoch: 1/1 - Step: 23 - Loss: 10.151000022888184\n",
      "Epoch: 1/1 - Step: 24 - Loss: 10.677639961242676\n",
      "Epoch: 1/1 - Step: 25 - Loss: 9.990452766418457\n",
      "Epoch: 1/1 - Step: 26 - Loss: 9.794413566589355\n",
      "Epoch: 1/1 - Step: 27 - Loss: 9.630768775939941\n",
      "Epoch: 1/1 - Step: 28 - Loss: 9.504512786865234\n",
      "Epoch: 1/1 - Step: 29 - Loss: 9.3618745803833\n",
      "Epoch: 1/1 - Step: 30 - Loss: 9.228504180908203\n",
      "Epoch: 1/1 - Step: 31 - Loss: 9.127995491027832\n",
      "Epoch: 1/1 - Step: 32 - Loss: 8.98542308807373\n",
      "Epoch: 1/1 - Step: 33 - Loss: 8.87710952758789\n",
      "Epoch: 1/1 - Step: 34 - Loss: 8.832830429077148\n",
      "Epoch: 1/1 - Step: 35 - Loss: 8.677703857421875\n",
      "Epoch: 1/1 - Step: 36 - Loss: 8.677204132080078\n",
      "Epoch: 1/1 - Step: 37 - Loss: 8.591363906860352\n",
      "Epoch: 1/1 - Step: 38 - Loss: 8.371576309204102\n",
      "Epoch: 1/1 - Step: 39 - Loss: 8.344127655029297\n",
      "Epoch: 1/1 - Step: 40 - Loss: 8.185490608215332\n",
      "Epoch: 1/1 - Step: 41 - Loss: 8.091048240661621\n",
      "Epoch: 1/1 - Step: 42 - Loss: 8.04611587524414\n",
      "Epoch: 1/1 - Step: 43 - Loss: 7.979475021362305\n",
      "Epoch: 1/1 - Step: 44 - Loss: 7.954693794250488\n",
      "Epoch: 1/1 - Step: 45 - Loss: 7.876893043518066\n",
      "Epoch: 1/1 - Step: 46 - Loss: 7.8461174964904785\n",
      "Epoch: 1/1 - Step: 47 - Loss: 7.781689167022705\n",
      "Epoch: 1/1 - Step: 48 - Loss: 7.674427509307861\n",
      "Epoch: 1/1 - Step: 49 - Loss: 7.6945648193359375\n",
      "Epoch: 1/1 - Step: 50 - Loss: 7.626260757446289\n",
      "Epoch: 1/1 - Step: 51 - Loss: 7.585330009460449\n",
      "Epoch: 1/1 - Step: 52 - Loss: 7.528258800506592\n",
      "Epoch: 1/1 - Step: 53 - Loss: 7.554337501525879\n",
      "Epoch: 1/1 - Step: 54 - Loss: 7.634463787078857\n",
      "Epoch: 1/1 - Step: 55 - Loss: 7.440093517303467\n",
      "Epoch: 1/1 - Step: 56 - Loss: 7.361660480499268\n",
      "Epoch: 1/1 - Step: 57 - Loss: 7.276959419250488\n",
      "Epoch: 1/1 - Step: 58 - Loss: 7.407865524291992\n",
      "Epoch: 1/1 - Step: 59 - Loss: 7.23048734664917\n",
      "Epoch: 1/1 - Step: 60 - Loss: 7.4108710289001465\n",
      "Epoch: 1/1 - Step: 61 - Loss: 7.2348127365112305\n",
      "Epoch: 1/1 - Step: 62 - Loss: 7.249192714691162\n",
      "Epoch: 1/1 - Step: 63 - Loss: 7.254180908203125\n",
      "Epoch: 1/1 - Step: 64 - Loss: 7.326072692871094\n",
      "Epoch: 1/1 - Step: 65 - Loss: 7.545042514801025\n",
      "Epoch: 1/1 - Step: 66 - Loss: 7.257096290588379\n",
      "Epoch: 1/1 - Step: 67 - Loss: 6.99489688873291\n",
      "Epoch: 1/1 - Step: 68 - Loss: 7.0664262771606445\n",
      "Epoch: 1/1 - Step: 69 - Loss: 6.936731815338135\n",
      "Epoch: 1/1 - Step: 70 - Loss: 6.919395446777344\n",
      "Epoch: 1/1 - Step: 71 - Loss: 7.096215724945068\n",
      "Epoch: 1/1 - Step: 72 - Loss: 7.194657802581787\n",
      "Epoch: 1/1 - Step: 73 - Loss: 7.27852201461792\n",
      "Epoch: 1/1 - Step: 74 - Loss: 7.240736484527588\n",
      "Epoch: 1/1 - Step: 75 - Loss: 7.258378505706787\n",
      "Epoch: 1/1 - Step: 76 - Loss: 7.004136562347412\n",
      "Epoch: 1/1 - Step: 77 - Loss: 7.154111862182617\n",
      "Epoch: 1/1 - Step: 78 - Loss: 7.191157341003418\n",
      "Epoch: 1/1 - Step: 79 - Loss: 7.2117838859558105\n",
      "Epoch: 1/1 - Step: 80 - Loss: 7.2093634605407715\n",
      "Epoch: 1/1 - Step: 81 - Loss: 7.039770126342773\n",
      "Epoch: 1/1 - Step: 82 - Loss: 7.033267974853516\n",
      "Epoch: 1/1 - Step: 83 - Loss: 7.089503765106201\n",
      "Epoch: 1/1 - Step: 84 - Loss: 7.139357089996338\n",
      "Epoch: 1/1 - Step: 85 - Loss: 7.22405481338501\n",
      "Epoch: 1/1 - Step: 86 - Loss: 6.9957427978515625\n",
      "Epoch: 1/1 - Step: 87 - Loss: 7.105498790740967\n",
      "Epoch: 1/1 - Step: 88 - Loss: 6.999964237213135\n",
      "Epoch: 1/1 - Step: 89 - Loss: 7.048961162567139\n",
      "Epoch: 1/1 - Step: 90 - Loss: 6.954586982727051\n",
      "Epoch: 1/1 - Step: 91 - Loss: 7.047149181365967\n",
      "Epoch: 1/1 - Step: 92 - Loss: 7.098365783691406\n",
      "Epoch: 1/1 - Step: 93 - Loss: 7.103520393371582\n",
      "Epoch: 1/1 - Step: 94 - Loss: 7.027928829193115\n",
      "Epoch: 1/1 - Step: 95 - Loss: 7.662619590759277\n",
      "Epoch: 1/1 - Step: 96 - Loss: 10.36642837524414\n",
      "Epoch: 1/1 - Step: 97 - Loss: 10.77554702758789\n",
      "Epoch: 1/1 - Step: 98 - Loss: 9.096009254455566\n",
      "Epoch: 1/1 - Step: 99 - Loss: 7.09705114364624\n",
      "Epoch: 1/1 - Step: 100 - Loss: 7.000644207000732\n",
      "Epoch: 1/1 - Step: 101 - Loss: 7.000943660736084\n",
      "Epoch: 1/1 - Step: 102 - Loss: 7.048354625701904\n",
      "Epoch: 1/1 - Step: 103 - Loss: 7.109907627105713\n",
      "Epoch: 1/1 - Step: 104 - Loss: 7.023257255554199\n",
      "Epoch: 1/1 - Step: 105 - Loss: 7.066898345947266\n",
      "Epoch: 1/1 - Step: 106 - Loss: 7.119380950927734\n",
      "Epoch: 1/1 - Step: 107 - Loss: 7.087214946746826\n",
      "Epoch: 1/1 - Step: 108 - Loss: 7.058599472045898\n",
      "Epoch: 1/1 - Step: 109 - Loss: 7.077847957611084\n",
      "Epoch: 1/1 - Step: 110 - Loss: 7.048978805541992\n",
      "Epoch: 1/1 - Step: 111 - Loss: 7.082947731018066\n",
      "Epoch: 1/1 - Step: 112 - Loss: 6.90334415435791\n",
      "Epoch: 1/1 - Step: 113 - Loss: 7.090301513671875\n",
      "Epoch: 1/1 - Step: 114 - Loss: 7.10110330581665\n",
      "Epoch: 1/1 - Step: 115 - Loss: 7.072311878204346\n",
      "Epoch: 1/1 - Step: 116 - Loss: 7.016221046447754\n",
      "Epoch: 1/1 - Step: 117 - Loss: 7.01404333114624\n",
      "Epoch: 1/1 - Step: 118 - Loss: 7.0092973709106445\n",
      "Epoch: 1/1 - Step: 119 - Loss: 7.039072036743164\n",
      "Epoch: 1/1 - Step: 120 - Loss: 7.02944803237915\n",
      "Epoch: 1/1 - Step: 121 - Loss: 7.0555524826049805\n",
      "Epoch: 1/1 - Step: 122 - Loss: 7.014928340911865\n",
      "Epoch: 1/1 - Step: 123 - Loss: 6.95071268081665\n",
      "Epoch: 1/1 - Step: 124 - Loss: 7.084184646606445\n",
      "Epoch: 1/1 - Step: 125 - Loss: 6.934330940246582\n",
      "Epoch: 1/1 - Step: 126 - Loss: 7.2061381340026855\n",
      "Epoch: 1/1 - Step: 127 - Loss: 7.066728591918945\n",
      "Epoch: 1/1 - Step: 128 - Loss: 7.058518886566162\n",
      "Epoch: 1/1 - Step: 129 - Loss: 6.982628345489502\n",
      "Epoch: 1/1 - Step: 130 - Loss: 6.911971569061279\n",
      "Epoch: 1/1 - Step: 131 - Loss: 6.891573905944824\n",
      "Epoch: 1/1 - Step: 132 - Loss: 6.927075386047363\n",
      "Epoch: 1/1 - Step: 133 - Loss: 6.8847198486328125\n",
      "Epoch: 1/1 - Step: 134 - Loss: 6.881794452667236\n",
      "Epoch: 1/1 - Step: 135 - Loss: 6.954366207122803\n",
      "Epoch: 1/1 - Step: 136 - Loss: 7.001890659332275\n",
      "Epoch: 1/1 - Step: 137 - Loss: 6.8819355964660645\n",
      "Epoch: 1/1 - Step: 138 - Loss: 6.899958610534668\n",
      "Epoch: 1/1 - Step: 139 - Loss: 6.915788173675537\n",
      "Epoch: 1/1 - Step: 140 - Loss: 6.959719657897949\n",
      "Epoch: 1/1 - Step: 141 - Loss: 7.109541893005371\n",
      "Epoch: 1/1 - Step: 142 - Loss: 7.119516372680664\n",
      "Epoch: 1/1 - Step: 143 - Loss: 7.016725063323975\n",
      "Epoch: 1/1 - Step: 144 - Loss: 6.959285736083984\n",
      "Epoch: 1/1 - Step: 145 - Loss: 6.971037864685059\n",
      "Epoch: 1/1 - Step: 146 - Loss: 7.129267692565918\n",
      "Epoch: 1/1 - Step: 147 - Loss: 6.955091953277588\n",
      "Epoch: 1/1 - Step: 148 - Loss: 6.961817264556885\n",
      "Epoch: 1/1 - Step: 149 - Loss: 6.917952060699463\n",
      "Epoch: 1/1 - Step: 150 - Loss: 6.967790603637695\n",
      "Epoch: 1/1 - Step: 151 - Loss: 6.970884799957275\n",
      "Epoch: 1/1 - Step: 152 - Loss: 6.965341091156006\n",
      "Epoch: 1/1 - Step: 153 - Loss: 6.886343479156494\n",
      "Epoch: 1/1 - Step: 154 - Loss: 7.013902187347412\n",
      "Epoch: 1/1 - Step: 155 - Loss: 7.083578109741211\n",
      "Epoch: 1/1 - Step: 156 - Loss: 6.913426876068115\n",
      "Epoch: 1/1 - Step: 157 - Loss: 6.912065029144287\n",
      "Epoch: 1/1 - Step: 158 - Loss: 6.93356990814209\n",
      "Epoch: 1/1 - Step: 159 - Loss: 6.908444881439209\n",
      "Epoch: 1/1 - Step: 160 - Loss: 6.98653507232666\n",
      "Epoch: 1/1 - Step: 161 - Loss: 7.131585121154785\n",
      "Epoch: 1/1 - Step: 162 - Loss: 6.8609232902526855\n",
      "Epoch: 1/1 - Step: 163 - Loss: 7.0831685066223145\n",
      "Epoch: 1/1 - Step: 164 - Loss: 6.892364025115967\n",
      "Epoch: 1/1 - Step: 165 - Loss: 7.004077911376953\n",
      "Epoch: 1/1 - Step: 166 - Loss: 6.923161029815674\n",
      "Epoch: 1/1 - Step: 167 - Loss: 6.771792411804199\n",
      "Epoch: 1/1 - Step: 168 - Loss: 6.963028907775879\n",
      "Epoch: 1/1 - Step: 169 - Loss: 6.929847240447998\n",
      "Epoch: 1/1 - Step: 170 - Loss: 7.061622619628906\n",
      "Epoch: 1/1 - Step: 171 - Loss: 6.87025785446167\n",
      "Epoch: 1/1 - Step: 172 - Loss: 6.918567657470703\n",
      "Epoch: 1/1 - Step: 173 - Loss: 6.913487434387207\n",
      "Epoch: 1/1 - Step: 174 - Loss: 7.101480007171631\n",
      "Epoch: 1/1 - Step: 175 - Loss: 7.063811779022217\n",
      "Epoch: 1/1 - Step: 176 - Loss: 7.273281574249268\n",
      "Epoch: 1/1 - Step: 177 - Loss: 7.086886405944824\n",
      "Epoch: 1/1 - Step: 178 - Loss: 7.000097751617432\n",
      "Epoch: 1/1 - Step: 179 - Loss: 6.978206634521484\n",
      "Epoch: 1/1 - Step: 180 - Loss: 6.897518634796143\n",
      "Epoch: 1/1 - Step: 181 - Loss: 6.853534698486328\n",
      "Epoch: 1/1 - Step: 182 - Loss: 6.826253890991211\n",
      "Epoch: 1/1 - Step: 183 - Loss: 6.969621658325195\n",
      "Epoch: 1/1 - Step: 184 - Loss: 6.836864948272705\n",
      "Epoch: 1/1 - Step: 185 - Loss: 6.810409069061279\n",
      "Epoch: 1/1 - Step: 186 - Loss: 6.884420394897461\n",
      "Epoch: 1/1 - Step: 187 - Loss: 7.147679328918457\n",
      "Epoch: 1/1 - Step: 188 - Loss: 6.849830150604248\n",
      "Epoch: 1/1 - Step: 189 - Loss: 6.901618480682373\n",
      "Epoch: 1/1 - Step: 190 - Loss: 6.971312999725342\n",
      "Epoch: 1/1 - Step: 191 - Loss: 6.968591213226318\n",
      "Epoch: 1/1 - Step: 192 - Loss: 6.968206405639648\n",
      "Epoch: 1/1 - Step: 193 - Loss: 6.891511917114258\n",
      "Epoch: 1/1 - Step: 194 - Loss: 6.918716907501221\n",
      "Epoch: 1/1 - Step: 195 - Loss: 6.9397382736206055\n",
      "Epoch: 1/1 - Step: 196 - Loss: 6.936758041381836\n",
      "Epoch: 1/1 - Step: 197 - Loss: 6.925171852111816\n",
      "Epoch: 1/1 - Step: 198 - Loss: 6.876123428344727\n",
      "Epoch: 1/1 - Step: 199 - Loss: 7.014253616333008\n",
      "Epoch: 1/1 - Step: 200 - Loss: 7.007014751434326\n",
      "Epoch: 1/1 - Step: 201 - Loss: 6.966381072998047\n",
      "Epoch: 1/1 - Step: 202 - Loss: 6.9587531089782715\n",
      "Epoch: 1/1 - Step: 203 - Loss: 6.924504280090332\n",
      "Epoch: 1/1 - Step: 204 - Loss: 6.894530773162842\n",
      "Epoch: 1/1 - Step: 205 - Loss: 6.85919713973999\n",
      "Epoch: 1/1 - Step: 206 - Loss: 6.8513946533203125\n",
      "Epoch: 1/1 - Step: 207 - Loss: 6.8786115646362305\n",
      "Epoch: 1/1 - Step: 208 - Loss: 6.86676549911499\n",
      "Epoch: 1/1 - Step: 209 - Loss: 6.859118938446045\n",
      "Epoch: 1/1 - Step: 210 - Loss: 6.887251377105713\n",
      "Epoch: 1/1 - Step: 211 - Loss: 6.821497440338135\n",
      "Epoch: 1/1 - Step: 212 - Loss: 6.7733001708984375\n",
      "Epoch: 1/1 - Step: 213 - Loss: 6.866979122161865\n",
      "Epoch: 1/1 - Step: 214 - Loss: 6.88520622253418\n",
      "Epoch: 1/1 - Step: 215 - Loss: 7.055181980133057\n",
      "Epoch: 1/1 - Step: 216 - Loss: 7.138817310333252\n",
      "Epoch: 1/1 - Step: 217 - Loss: 6.883678913116455\n",
      "Epoch: 1/1 - Step: 218 - Loss: 6.898997783660889\n",
      "Epoch: 1/1 - Step: 219 - Loss: 6.862514972686768\n",
      "Epoch: 1/1 - Step: 220 - Loss: 6.9270195960998535\n",
      "Epoch: 1/1 - Step: 221 - Loss: 6.957643032073975\n",
      "Epoch: 1/1 - Step: 222 - Loss: 6.810085773468018\n",
      "Epoch: 1/1 - Step: 223 - Loss: 6.886484146118164\n",
      "Epoch: 1/1 - Step: 224 - Loss: 6.9354753494262695\n",
      "Epoch: 1/1 - Step: 225 - Loss: 6.912859916687012\n",
      "Epoch: 1/1 - Step: 226 - Loss: 6.8475494384765625\n",
      "Epoch: 1/1 - Step: 227 - Loss: 6.9071574211120605\n",
      "Epoch: 1/1 - Step: 228 - Loss: 8.950785636901855\n",
      "Epoch: 1/1 - Step: 229 - Loss: 8.91287899017334\n",
      "Epoch: 1/1 - Step: 230 - Loss: 7.011681079864502\n",
      "Epoch: 1/1 - Step: 231 - Loss: 6.82970666885376\n",
      "Epoch: 1/1 - Step: 232 - Loss: 6.738011837005615\n",
      "Epoch: 1/1 - Step: 233 - Loss: 6.900947093963623\n",
      "Epoch: 1/1 - Step: 234 - Loss: 7.105902194976807\n",
      "Epoch: 1/1 - Step: 235 - Loss: 6.8784074783325195\n",
      "Epoch: 1/1 - Step: 236 - Loss: 6.890860557556152\n",
      "Epoch: 1/1 - Step: 237 - Loss: 6.898493766784668\n",
      "Epoch: 1/1 - Step: 238 - Loss: 7.14456844329834\n",
      "Epoch: 1/1 - Step: 239 - Loss: 7.0880818367004395\n",
      "Epoch: 1/1 - Step: 240 - Loss: 7.037671089172363\n",
      "Epoch: 1/1 - Step: 241 - Loss: 6.864660263061523\n",
      "Epoch: 1/1 - Step: 242 - Loss: 6.79647159576416\n",
      "Epoch: 1/1 - Step: 243 - Loss: 6.949185848236084\n",
      "Epoch: 1/1 - Step: 244 - Loss: 7.009121417999268\n",
      "Epoch: 1/1 - Step: 245 - Loss: 6.948022365570068\n",
      "Epoch: 1/1 - Step: 246 - Loss: 6.833758354187012\n",
      "Epoch: 1/1 - Step: 247 - Loss: 6.691831111907959\n",
      "Epoch: 1/1 - Step: 248 - Loss: 6.737096309661865\n",
      "Epoch: 1/1 - Step: 249 - Loss: 7.064283847808838\n",
      "Epoch: 1/1 - Step: 250 - Loss: 6.850384712219238\n",
      "Epoch: 1/1 - Step: 251 - Loss: 6.848431587219238\n",
      "Epoch: 1/1 - Step: 252 - Loss: 6.979099750518799\n",
      "Epoch: 1/1 - Step: 253 - Loss: 6.95198392868042\n",
      "Epoch: 1/1 - Step: 254 - Loss: 6.902029037475586\n",
      "Epoch: 1/1 - Step: 255 - Loss: 6.921367645263672\n",
      "Epoch: 1/1 - Step: 256 - Loss: 6.976243019104004\n",
      "Epoch: 1/1 - Step: 257 - Loss: 6.858356952667236\n",
      "Epoch: 1/1 - Step: 258 - Loss: 6.921128273010254\n",
      "Epoch: 1/1 - Step: 259 - Loss: 6.869513988494873\n",
      "Epoch: 1/1 - Step: 260 - Loss: 6.858165740966797\n",
      "Epoch: 1/1 - Step: 261 - Loss: 6.960450649261475\n",
      "Epoch: 1/1 - Step: 262 - Loss: 6.921109199523926\n",
      "Epoch: 1/1 - Step: 263 - Loss: 7.111289978027344\n",
      "Epoch: 1/1 - Step: 264 - Loss: 6.832249164581299\n",
      "Epoch: 1/1 - Step: 265 - Loss: 6.840885639190674\n",
      "Epoch: 1/1 - Step: 266 - Loss: 6.90015983581543\n",
      "Epoch: 1/1 - Step: 267 - Loss: 6.910015106201172\n",
      "Epoch: 1/1 - Step: 268 - Loss: 6.854632377624512\n",
      "Epoch: 1/1 - Step: 269 - Loss: 6.928370475769043\n",
      "Epoch: 1/1 - Step: 270 - Loss: 6.8018574714660645\n",
      "Epoch: 1/1 - Step: 271 - Loss: 6.941991329193115\n",
      "Epoch: 1/1 - Step: 272 - Loss: 6.9688801765441895\n",
      "Epoch: 1/1 - Step: 273 - Loss: 6.795634746551514\n",
      "Epoch: 1/1 - Step: 274 - Loss: 6.752923011779785\n",
      "Epoch: 1/1 - Step: 275 - Loss: 6.814010143280029\n",
      "Epoch: 1/1 - Step: 276 - Loss: 6.866889476776123\n",
      "Epoch: 1/1 - Step: 277 - Loss: 6.81313943862915\n",
      "Epoch: 1/1 - Step: 278 - Loss: 8.896186828613281\n",
      "Epoch: 1/1 - Step: 279 - Loss: 8.565058708190918\n",
      "Epoch: 1/1 - Step: 280 - Loss: 7.021106243133545\n",
      "Epoch: 1/1 - Step: 281 - Loss: 6.9702467918396\n",
      "Epoch: 1/1 - Step: 282 - Loss: 6.949080944061279\n",
      "Epoch: 1/1 - Step: 283 - Loss: 6.841716766357422\n",
      "Epoch: 1/1 - Step: 284 - Loss: 6.901073455810547\n",
      "Epoch: 1/1 - Step: 285 - Loss: 6.874527454376221\n",
      "Epoch: 1/1 - Step: 286 - Loss: 7.058850288391113\n",
      "Epoch: 1/1 - Step: 287 - Loss: 6.754133224487305\n",
      "Epoch: 1/1 - Step: 288 - Loss: 7.011227607727051\n",
      "Epoch: 1/1 - Step: 289 - Loss: 6.97381067276001\n",
      "Epoch: 1/1 - Step: 290 - Loss: 6.929965972900391\n",
      "Epoch: 1/1 - Step: 291 - Loss: 6.823137283325195\n",
      "Epoch: 1/1 - Step: 292 - Loss: 6.750227451324463\n",
      "Epoch: 1/1 - Step: 293 - Loss: 6.876285552978516\n",
      "Epoch: 1/1 - Step: 294 - Loss: 6.8560895919799805\n",
      "Epoch: 1/1 - Step: 295 - Loss: 6.9160614013671875\n",
      "Epoch: 1/1 - Step: 296 - Loss: 6.866463661193848\n",
      "Epoch: 1/1 - Step: 297 - Loss: 6.9109063148498535\n",
      "Epoch: 1/1 - Step: 298 - Loss: 6.907713413238525\n",
      "Epoch: 1/1 - Step: 299 - Loss: 6.795322418212891\n",
      "Epoch: 1/1 - Step: 300 - Loss: 6.805948257446289\n",
      "Epoch: 1/1 - Step: 301 - Loss: 6.913090229034424\n",
      "Epoch: 1/1 - Step: 302 - Loss: 6.886027812957764\n",
      "Epoch: 1/1 - Step: 303 - Loss: 6.779256343841553\n",
      "Epoch: 1/1 - Step: 304 - Loss: 6.948215007781982\n",
      "Epoch: 1/1 - Step: 305 - Loss: 6.9518351554870605\n",
      "Epoch: 1/1 - Step: 306 - Loss: 6.803732872009277\n",
      "Epoch: 1/1 - Step: 307 - Loss: 6.807299613952637\n",
      "Epoch: 1/1 - Step: 308 - Loss: 7.008373737335205\n",
      "Epoch: 1/1 - Step: 309 - Loss: 6.83456563949585\n",
      "Epoch: 1/1 - Step: 310 - Loss: 6.846120357513428\n",
      "Epoch: 1/1 - Step: 311 - Loss: 6.820254802703857\n",
      "Epoch: 1/1 - Step: 312 - Loss: 6.9097065925598145\n",
      "Epoch: 1/1 - Step: 313 - Loss: 8.562381744384766\n",
      "Epoch: 1/1 - Step: 314 - Loss: 7.949306011199951\n",
      "Epoch: 1/1 - Step: 315 - Loss: 6.888023853302002\n",
      "Epoch: 1/1 - Step: 316 - Loss: 6.811279773712158\n",
      "Epoch: 1/1 - Step: 317 - Loss: 7.029067039489746\n",
      "Epoch: 1/1 - Step: 318 - Loss: 7.049145698547363\n",
      "Epoch: 1/1 - Step: 319 - Loss: 6.968602657318115\n",
      "Epoch: 1/1 - Step: 320 - Loss: 6.961694240570068\n",
      "Epoch: 1/1 - Step: 321 - Loss: 7.004903793334961\n",
      "Epoch: 1/1 - Step: 322 - Loss: 7.128469467163086\n",
      "Epoch: 1/1 - Step: 323 - Loss: 7.065162181854248\n",
      "Epoch: 1/1 - Step: 324 - Loss: 6.941257476806641\n",
      "Epoch: 1/1 - Step: 325 - Loss: 7.058610916137695\n",
      "Epoch: 1/1 - Step: 326 - Loss: 7.1106085777282715\n",
      "Epoch: 1/1 - Step: 327 - Loss: 6.970444202423096\n",
      "Epoch: 1/1 - Step: 328 - Loss: 7.0152788162231445\n",
      "Epoch: 1/1 - Step: 329 - Loss: 7.0138654708862305\n",
      "Epoch: 1/1 - Step: 330 - Loss: 7.088650703430176\n",
      "Epoch: 1/1 - Step: 331 - Loss: 6.9860992431640625\n",
      "Epoch: 1/1 - Step: 332 - Loss: 6.979841232299805\n",
      "Epoch: 1/1 - Step: 333 - Loss: 6.998648643493652\n",
      "Epoch: 1/1 - Step: 334 - Loss: 6.8842082023620605\n",
      "Epoch: 1/1 - Step: 335 - Loss: 6.837790012359619\n",
      "Epoch: 1/1 - Step: 336 - Loss: 6.861116886138916\n",
      "Epoch: 1/1 - Step: 337 - Loss: 6.9067583084106445\n",
      "Epoch: 1/1 - Step: 338 - Loss: 6.849933624267578\n",
      "Epoch: 1/1 - Step: 339 - Loss: 9.25027084350586\n",
      "Epoch: 1/1 - Step: 340 - Loss: 9.492446899414062\n",
      "Epoch: 1/1 - Step: 341 - Loss: 9.492680549621582\n",
      "Epoch: 1/1 - Step: 342 - Loss: 7.042007923126221\n",
      "Epoch: 1/1 - Step: 343 - Loss: 7.0146284103393555\n",
      "Epoch: 1/1 - Step: 344 - Loss: 6.96052360534668\n",
      "Epoch: 1/1 - Step: 345 - Loss: 7.050965785980225\n",
      "Epoch: 1/1 - Step: 346 - Loss: 7.081006050109863\n",
      "Epoch: 1/1 - Step: 347 - Loss: 7.036479473114014\n",
      "Epoch: 1/1 - Step: 348 - Loss: 7.083626747131348\n",
      "Epoch: 1/1 - Step: 349 - Loss: 6.80075740814209\n",
      "Epoch: 1/1 - Step: 350 - Loss: 6.710334300994873\n",
      "Epoch: 1/1 - Step: 351 - Loss: 6.872408866882324\n",
      "Epoch: 1/1 - Step: 352 - Loss: 6.958913803100586\n",
      "Epoch: 1/1 - Step: 353 - Loss: 7.04902982711792\n",
      "Epoch: 1/1 - Step: 354 - Loss: 6.916999340057373\n",
      "Epoch: 1/1 - Step: 355 - Loss: 7.06894588470459\n",
      "Epoch: 1/1 - Step: 356 - Loss: 7.10998010635376\n",
      "Epoch: 1/1 - Step: 357 - Loss: 7.069475173950195\n",
      "Epoch: 1/1 - Step: 358 - Loss: 7.054784297943115\n",
      "Epoch: 1/1 - Step: 359 - Loss: 7.091903209686279\n",
      "Epoch: 1/1 - Step: 360 - Loss: 7.053440093994141\n",
      "Epoch: 1/1 - Step: 361 - Loss: 7.072916507720947\n",
      "Epoch: 1/1 - Step: 362 - Loss: 7.060504913330078\n",
      "Epoch: 1/1 - Step: 363 - Loss: 7.052378177642822\n",
      "Epoch: 1/1 - Step: 364 - Loss: 6.889228343963623\n",
      "Epoch: 1/1 - Step: 365 - Loss: 6.983901500701904\n",
      "Epoch: 1/1 - Step: 366 - Loss: 8.359614372253418\n",
      "Epoch: 1/1 - Step: 367 - Loss: 6.898664474487305\n",
      "Epoch: 1/1 - Step: 368 - Loss: 7.001204013824463\n",
      "Epoch: 1/1 - Step: 369 - Loss: 6.962008953094482\n",
      "Epoch: 1/1 - Step: 370 - Loss: 6.995028018951416\n",
      "Epoch: 1/1 - Step: 371 - Loss: 6.882989883422852\n",
      "Epoch: 1/1 - Step: 372 - Loss: 6.923826217651367\n",
      "Epoch: 1/1 - Step: 373 - Loss: 6.979931831359863\n",
      "Epoch: 1/1 - Step: 374 - Loss: 6.933437824249268\n",
      "Epoch: 1/1 - Step: 375 - Loss: 6.964001178741455\n",
      "Epoch: 1/1 - Step: 376 - Loss: 6.96708869934082\n",
      "Epoch: 1/1 - Step: 377 - Loss: 6.915095329284668\n",
      "Epoch: 1/1 - Step: 378 - Loss: 7.164944648742676\n",
      "Epoch: 1/1 - Step: 379 - Loss: 7.057703971862793\n",
      "Epoch: 1/1 - Step: 380 - Loss: 6.8557538986206055\n",
      "Epoch: 1/1 - Step: 381 - Loss: 6.97023344039917\n",
      "Epoch: 1/1 - Step: 382 - Loss: 6.81072473526001\n",
      "Epoch: 1/1 - Step: 383 - Loss: 6.968257427215576\n",
      "Epoch: 1/1 - Step: 384 - Loss: 6.876547336578369\n",
      "Epoch: 1/1 - Step: 385 - Loss: 6.96988582611084\n",
      "Epoch: 1/1 - Step: 386 - Loss: 7.007035255432129\n",
      "Epoch: 1/1 - Step: 387 - Loss: 6.936164855957031\n",
      "Epoch: 1/1 - Step: 388 - Loss: 7.020412921905518\n",
      "Epoch: 1/1 - Step: 389 - Loss: 6.942351818084717\n",
      "Epoch: 1/1 - Step: 390 - Loss: 6.945703983306885\n",
      "Epoch: 1/1 - Step: 391 - Loss: 6.970431327819824\n",
      "Epoch: 1/1 - Step: 392 - Loss: 6.9896955490112305\n",
      "Epoch: 1/1 - Step: 393 - Loss: 6.996150493621826\n",
      "Epoch: 1/1 - Step: 394 - Loss: 6.917125701904297\n",
      "Epoch: 1/1 - Step: 395 - Loss: 7.060379505157471\n",
      "Epoch: 1/1 - Step: 396 - Loss: 7.218331813812256\n",
      "Epoch: 1/1 - Step: 397 - Loss: 6.817619323730469\n",
      "Epoch: 1/1 - Step: 398 - Loss: 6.7706074714660645\n",
      "Epoch: 1/1 - Step: 399 - Loss: 6.740713119506836\n",
      "Epoch: 1/1 - Step: 400 - Loss: 6.88839864730835\n",
      "Epoch: 1/1 - Step: 401 - Loss: 6.796656608581543\n",
      "Epoch: 1/1 - Step: 402 - Loss: 6.986058712005615\n",
      "Epoch: 1/1 - Step: 403 - Loss: 6.8967084884643555\n",
      "Epoch: 1/1 - Step: 404 - Loss: 6.932356357574463\n",
      "Epoch: 1/1 - Step: 405 - Loss: 6.921738624572754\n",
      "Epoch: 1/1 - Step: 406 - Loss: 7.098161220550537\n",
      "Epoch: 1/1 - Step: 407 - Loss: 7.116441249847412\n",
      "Epoch: 1/1 - Step: 408 - Loss: 6.977575302124023\n",
      "Epoch: 1/1 - Step: 409 - Loss: 6.66098165512085\n",
      "Epoch: 1/1 - Step: 410 - Loss: 6.749462127685547\n",
      "Epoch: 1/1 - Step: 411 - Loss: 6.653496742248535\n",
      "Epoch: 1/1 - Step: 412 - Loss: 6.755751132965088\n",
      "Epoch: 1/1 - Step: 413 - Loss: 6.858512878417969\n",
      "Epoch: 1/1 - Step: 414 - Loss: 7.065510272979736\n",
      "Epoch: 1/1 - Step: 415 - Loss: 7.0141215324401855\n",
      "Epoch: 1/1 - Step: 416 - Loss: 7.131600379943848\n",
      "Epoch: 1/1 - Step: 417 - Loss: 7.028425216674805\n",
      "Epoch: 1/1 - Step: 418 - Loss: 6.87933874130249\n",
      "Epoch: 1/1 - Step: 419 - Loss: 7.023054599761963\n",
      "Epoch: 1/1 - Step: 420 - Loss: 7.021417617797852\n",
      "Epoch: 1/1 - Step: 421 - Loss: 7.080476760864258\n",
      "Epoch: 1/1 - Step: 422 - Loss: 7.031616687774658\n",
      "Epoch: 1/1 - Step: 423 - Loss: 6.9221062660217285\n",
      "Epoch: 1/1 - Step: 424 - Loss: 6.911267280578613\n",
      "Epoch: 1/1 - Step: 425 - Loss: 6.987875938415527\n",
      "Epoch: 1/1 - Step: 426 - Loss: 7.0377678871154785\n",
      "Epoch: 1/1 - Step: 427 - Loss: 7.02599573135376\n",
      "Epoch: 1/1 - Step: 428 - Loss: 6.921002388000488\n",
      "Epoch: 1/1 - Step: 429 - Loss: 7.0165839195251465\n",
      "Epoch: 1/1 - Step: 430 - Loss: 6.843069076538086\n",
      "Epoch: 1/1 - Step: 431 - Loss: 6.931645393371582\n",
      "Epoch: 1/1 - Step: 432 - Loss: 6.9066033363342285\n",
      "Epoch: 1/1 - Step: 433 - Loss: 6.950833320617676\n",
      "Epoch: 1/1 - Step: 434 - Loss: 6.982944965362549\n",
      "Epoch: 1/1 - Step: 435 - Loss: 7.065324306488037\n",
      "Epoch: 1/1 - Step: 436 - Loss: 6.8753252029418945\n",
      "Epoch: 1/1 - Step: 437 - Loss: 7.898748874664307\n",
      "Epoch: 1/1 - Step: 438 - Loss: 9.259549140930176\n",
      "Epoch: 1/1 - Step: 439 - Loss: 9.61707592010498\n",
      "Epoch: 1/1 - Step: 440 - Loss: 7.791934967041016\n",
      "Epoch: 1/1 - Step: 441 - Loss: 6.984755516052246\n",
      "Epoch: 1/1 - Step: 442 - Loss: 6.95613956451416\n",
      "Epoch: 1/1 - Step: 443 - Loss: 6.880195617675781\n",
      "Epoch: 1/1 - Step: 444 - Loss: 6.993424892425537\n",
      "Epoch: 1/1 - Step: 445 - Loss: 6.972822666168213\n",
      "Epoch: 1/1 - Step: 446 - Loss: 6.97024393081665\n",
      "Epoch: 1/1 - Step: 447 - Loss: 6.9855427742004395\n",
      "Epoch: 1/1 - Step: 448 - Loss: 7.037213325500488\n",
      "Epoch: 1/1 - Step: 449 - Loss: 7.002920150756836\n",
      "Epoch: 1/1 - Step: 450 - Loss: 6.958885669708252\n",
      "Epoch: 1/1 - Step: 451 - Loss: 6.982710838317871\n",
      "Epoch: 1/1 - Step: 452 - Loss: 7.002150535583496\n",
      "Epoch: 1/1 - Step: 453 - Loss: 6.986998081207275\n",
      "Epoch: 1/1 - Step: 454 - Loss: 6.88499116897583\n",
      "Epoch: 1/1 - Step: 455 - Loss: 7.064748764038086\n",
      "Epoch: 1/1 - Step: 456 - Loss: 7.014246463775635\n",
      "Epoch: 1/1 - Step: 457 - Loss: 7.0011773109436035\n",
      "Epoch: 1/1 - Step: 458 - Loss: 6.897211074829102\n",
      "Epoch: 1/1 - Step: 459 - Loss: 6.947227954864502\n",
      "Epoch: 1/1 - Step: 460 - Loss: 6.958132743835449\n",
      "Epoch: 1/1 - Step: 461 - Loss: 6.971035003662109\n",
      "Epoch: 1/1 - Step: 462 - Loss: 6.985474109649658\n",
      "Epoch: 1/1 - Step: 463 - Loss: 6.9529128074646\n",
      "Epoch: 1/1 - Step: 464 - Loss: 6.912662982940674\n",
      "Epoch: 1/1 - Step: 465 - Loss: 6.941675662994385\n",
      "Epoch: 1/1 - Step: 466 - Loss: 6.976380348205566\n",
      "Epoch: 1/1 - Step: 467 - Loss: 6.903120040893555\n",
      "Epoch: 1/1 - Step: 468 - Loss: 7.162574291229248\n",
      "Epoch: 1/1 - Step: 469 - Loss: 6.966883659362793\n",
      "Epoch: 1/1 - Step: 470 - Loss: 6.989845275878906\n",
      "Epoch: 1/1 - Step: 471 - Loss: 6.887176990509033\n",
      "Epoch: 1/1 - Step: 472 - Loss: 6.832885265350342\n",
      "Epoch: 1/1 - Step: 473 - Loss: 6.861373424530029\n",
      "Epoch: 1/1 - Step: 474 - Loss: 6.857918739318848\n",
      "Epoch: 1/1 - Step: 475 - Loss: 6.7841057777404785\n",
      "Epoch: 1/1 - Step: 476 - Loss: 6.84151554107666\n",
      "Epoch: 1/1 - Step: 477 - Loss: 6.946381568908691\n",
      "Epoch: 1/1 - Step: 478 - Loss: 6.859147071838379\n",
      "Epoch: 1/1 - Step: 479 - Loss: 6.839123249053955\n",
      "Epoch: 1/1 - Step: 480 - Loss: 6.827944755554199\n",
      "Epoch: 1/1 - Step: 481 - Loss: 6.818335056304932\n",
      "Epoch: 1/1 - Step: 482 - Loss: 6.958718776702881\n",
      "Epoch: 1/1 - Step: 483 - Loss: 7.07802677154541\n",
      "Epoch: 1/1 - Step: 484 - Loss: 6.989020347595215\n",
      "Epoch: 1/1 - Step: 485 - Loss: 6.881507873535156\n",
      "Epoch: 1/1 - Step: 486 - Loss: 6.944772720336914\n",
      "Epoch: 1/1 - Step: 487 - Loss: 6.947964191436768\n",
      "Epoch: 1/1 - Step: 488 - Loss: 6.980967044830322\n",
      "Epoch: 1/1 - Step: 489 - Loss: 6.896586894989014\n",
      "Epoch: 1/1 - Step: 490 - Loss: 6.8855180740356445\n",
      "Epoch: 1/1 - Step: 491 - Loss: 6.835980415344238\n",
      "Epoch: 1/1 - Step: 492 - Loss: 6.94303035736084\n",
      "Epoch: 1/1 - Step: 493 - Loss: 6.941259384155273\n",
      "Epoch: 1/1 - Step: 494 - Loss: 6.855952739715576\n",
      "Epoch: 1/1 - Step: 495 - Loss: 6.831262588500977\n",
      "Epoch: 1/1 - Step: 496 - Loss: 6.9638190269470215\n",
      "Epoch: 1/1 - Step: 497 - Loss: 7.005434036254883\n",
      "Epoch: 1/1 - Step: 498 - Loss: 6.82372522354126\n",
      "Epoch: 1/1 - Step: 499 - Loss: 6.878234386444092\n",
      "Epoch: 1/1 - Step: 500 - Loss: 6.844669818878174\n",
      "Epoch: 1/1 - Step: 501 - Loss: 6.887248992919922\n",
      "Epoch: 1/1 - Step: 502 - Loss: 6.959606647491455\n",
      "Epoch: 1/1 - Step: 503 - Loss: 6.981674671173096\n",
      "Epoch: 1/1 - Step: 504 - Loss: 6.797287464141846\n",
      "Epoch: 1/1 - Step: 505 - Loss: 7.0093231201171875\n",
      "Epoch: 1/1 - Step: 506 - Loss: 6.787208557128906\n",
      "Epoch: 1/1 - Step: 507 - Loss: 6.971996307373047\n",
      "Epoch: 1/1 - Step: 508 - Loss: 6.805374622344971\n",
      "Epoch: 1/1 - Step: 509 - Loss: 6.723110198974609\n",
      "Epoch: 1/1 - Step: 510 - Loss: 6.88989782333374\n",
      "Epoch: 1/1 - Step: 511 - Loss: 6.9608001708984375\n",
      "Epoch: 1/1 - Step: 512 - Loss: 6.914773464202881\n",
      "Epoch: 1/1 - Step: 513 - Loss: 6.772237300872803\n",
      "Epoch: 1/1 - Step: 514 - Loss: 6.886801242828369\n",
      "Epoch: 1/1 - Step: 515 - Loss: 6.854186534881592\n",
      "Epoch: 1/1 - Step: 516 - Loss: 7.077944755554199\n",
      "Epoch: 1/1 - Step: 517 - Loss: 7.04617166519165\n",
      "Epoch: 1/1 - Step: 518 - Loss: 7.070407390594482\n",
      "Epoch: 1/1 - Step: 519 - Loss: 7.059285640716553\n",
      "Epoch: 1/1 - Step: 520 - Loss: 6.920216083526611\n",
      "Epoch: 1/1 - Step: 521 - Loss: 6.89785099029541\n",
      "Epoch: 1/1 - Step: 522 - Loss: 6.784216403961182\n",
      "Epoch: 1/1 - Step: 523 - Loss: 6.746599197387695\n",
      "Epoch: 1/1 - Step: 524 - Loss: 6.807788372039795\n",
      "Epoch: 1/1 - Step: 525 - Loss: 6.882952690124512\n",
      "Epoch: 1/1 - Step: 526 - Loss: 6.742084980010986\n",
      "Epoch: 1/1 - Step: 527 - Loss: 6.714053153991699\n",
      "Epoch: 1/1 - Step: 528 - Loss: 6.883364677429199\n",
      "Epoch: 1/1 - Step: 529 - Loss: 7.0683369636535645\n",
      "Epoch: 1/1 - Step: 530 - Loss: 6.7329230308532715\n",
      "Epoch: 1/1 - Step: 531 - Loss: 6.87466287612915\n",
      "Epoch: 1/1 - Step: 532 - Loss: 6.873244285583496\n",
      "Epoch: 1/1 - Step: 533 - Loss: 6.902957916259766\n",
      "Epoch: 1/1 - Step: 534 - Loss: 6.912435531616211\n",
      "Epoch: 1/1 - Step: 535 - Loss: 6.787377834320068\n",
      "Epoch: 1/1 - Step: 536 - Loss: 6.8993706703186035\n",
      "Epoch: 1/1 - Step: 537 - Loss: 6.850071430206299\n",
      "Epoch: 1/1 - Step: 538 - Loss: 6.871264934539795\n",
      "Epoch: 1/1 - Step: 539 - Loss: 6.825989723205566\n",
      "Epoch: 1/1 - Step: 540 - Loss: 6.820058345794678\n",
      "Epoch: 1/1 - Step: 541 - Loss: 6.9583353996276855\n",
      "Epoch: 1/1 - Step: 542 - Loss: 6.960453033447266\n",
      "Epoch: 1/1 - Step: 543 - Loss: 6.837268352508545\n",
      "Epoch: 1/1 - Step: 544 - Loss: 6.880180358886719\n",
      "Epoch: 1/1 - Step: 545 - Loss: 6.84702205657959\n",
      "Epoch: 1/1 - Step: 546 - Loss: 6.809656620025635\n",
      "Epoch: 1/1 - Step: 547 - Loss: 6.7671332359313965\n",
      "Epoch: 1/1 - Step: 548 - Loss: 6.77148962020874\n",
      "Epoch: 1/1 - Step: 549 - Loss: 6.863022804260254\n",
      "Epoch: 1/1 - Step: 550 - Loss: 6.772172451019287\n",
      "Epoch: 1/1 - Step: 551 - Loss: 6.809401512145996\n",
      "Epoch: 1/1 - Step: 552 - Loss: 6.803008556365967\n",
      "Epoch: 1/1 - Step: 553 - Loss: 6.7420220375061035\n",
      "Epoch: 1/1 - Step: 554 - Loss: 6.713129997253418\n",
      "Epoch: 1/1 - Step: 555 - Loss: 6.790866851806641\n",
      "Epoch: 1/1 - Step: 556 - Loss: 6.8457136154174805\n",
      "Epoch: 1/1 - Step: 557 - Loss: 7.0672221183776855\n",
      "Epoch: 1/1 - Step: 558 - Loss: 6.986254692077637\n",
      "Epoch: 1/1 - Step: 559 - Loss: 6.79637336730957\n",
      "Epoch: 1/1 - Step: 560 - Loss: 6.807648181915283\n",
      "Epoch: 1/1 - Step: 561 - Loss: 6.842859745025635\n",
      "Epoch: 1/1 - Step: 562 - Loss: 6.89392614364624\n",
      "Epoch: 1/1 - Step: 563 - Loss: 6.816775798797607\n",
      "Epoch: 1/1 - Step: 564 - Loss: 6.768621444702148\n",
      "Epoch: 1/1 - Step: 565 - Loss: 6.830860614776611\n",
      "Epoch: 1/1 - Step: 566 - Loss: 6.840323448181152\n",
      "Epoch: 1/1 - Step: 567 - Loss: 6.849339962005615\n",
      "Epoch: 1/1 - Step: 568 - Loss: 6.782182693481445\n",
      "Epoch: 1/1 - Step: 569 - Loss: 6.854057312011719\n",
      "Epoch: 1/1 - Step: 570 - Loss: 9.129758834838867\n",
      "Epoch: 1/1 - Step: 571 - Loss: 7.964099407196045\n",
      "Epoch: 1/1 - Step: 572 - Loss: 6.922844886779785\n",
      "Epoch: 1/1 - Step: 573 - Loss: 6.693248271942139\n",
      "Epoch: 1/1 - Step: 574 - Loss: 6.714899063110352\n",
      "Epoch: 1/1 - Step: 575 - Loss: 6.835608959197998\n",
      "Epoch: 1/1 - Step: 576 - Loss: 7.003822326660156\n",
      "Epoch: 1/1 - Step: 577 - Loss: 6.834004878997803\n",
      "Epoch: 1/1 - Step: 578 - Loss: 6.821761131286621\n",
      "Epoch: 1/1 - Step: 579 - Loss: 6.914228916168213\n",
      "Epoch: 1/1 - Step: 580 - Loss: 6.978487968444824\n",
      "Epoch: 1/1 - Step: 581 - Loss: 7.040536403656006\n",
      "Epoch: 1/1 - Step: 582 - Loss: 6.910878658294678\n",
      "Epoch: 1/1 - Step: 583 - Loss: 6.759945869445801\n",
      "Epoch: 1/1 - Step: 584 - Loss: 6.745037078857422\n",
      "Epoch: 1/1 - Step: 585 - Loss: 6.907653331756592\n",
      "Epoch: 1/1 - Step: 586 - Loss: 7.0055317878723145\n",
      "Epoch: 1/1 - Step: 587 - Loss: 6.8172688484191895\n",
      "Epoch: 1/1 - Step: 588 - Loss: 6.69396448135376\n",
      "Epoch: 1/1 - Step: 589 - Loss: 6.629591464996338\n",
      "Epoch: 1/1 - Step: 590 - Loss: 6.712400913238525\n",
      "Epoch: 1/1 - Step: 591 - Loss: 6.977031230926514\n",
      "Epoch: 1/1 - Step: 592 - Loss: 6.745092391967773\n",
      "Epoch: 1/1 - Step: 593 - Loss: 6.81958532333374\n",
      "Epoch: 1/1 - Step: 594 - Loss: 6.857963562011719\n",
      "Epoch: 1/1 - Step: 595 - Loss: 6.894818305969238\n",
      "Epoch: 1/1 - Step: 596 - Loss: 6.795336723327637\n",
      "Epoch: 1/1 - Step: 597 - Loss: 6.912169933319092\n",
      "Epoch: 1/1 - Step: 598 - Loss: 6.84691858291626\n",
      "Epoch: 1/1 - Step: 599 - Loss: 6.773382663726807\n",
      "Epoch: 1/1 - Step: 600 - Loss: 6.849388599395752\n",
      "Epoch: 1/1 - Step: 601 - Loss: 6.8334574699401855\n",
      "Epoch: 1/1 - Step: 602 - Loss: 6.796417713165283\n",
      "Epoch: 1/1 - Step: 603 - Loss: 6.87489652633667\n",
      "Epoch: 1/1 - Step: 604 - Loss: 6.846118450164795\n",
      "Epoch: 1/1 - Step: 605 - Loss: 7.035303115844727\n",
      "Epoch: 1/1 - Step: 606 - Loss: 6.713908672332764\n",
      "Epoch: 1/1 - Step: 607 - Loss: 6.809627532958984\n",
      "Epoch: 1/1 - Step: 608 - Loss: 6.819375991821289\n",
      "Epoch: 1/1 - Step: 609 - Loss: 6.812746047973633\n",
      "Epoch: 1/1 - Step: 610 - Loss: 6.790030002593994\n",
      "Epoch: 1/1 - Step: 611 - Loss: 6.8828277587890625\n",
      "Epoch: 1/1 - Step: 612 - Loss: 6.713112831115723\n",
      "Epoch: 1/1 - Step: 613 - Loss: 6.851833343505859\n",
      "Epoch: 1/1 - Step: 614 - Loss: 6.891480922698975\n",
      "Epoch: 1/1 - Step: 615 - Loss: 6.712531566619873\n",
      "Epoch: 1/1 - Step: 616 - Loss: 6.68724250793457\n",
      "Epoch: 1/1 - Step: 617 - Loss: 6.761834144592285\n",
      "Epoch: 1/1 - Step: 618 - Loss: 6.800267696380615\n",
      "Epoch: 1/1 - Step: 619 - Loss: 6.737181663513184\n",
      "Epoch: 1/1 - Step: 620 - Loss: 9.10307788848877\n",
      "Epoch: 1/1 - Step: 621 - Loss: 7.842819690704346\n",
      "Epoch: 1/1 - Step: 622 - Loss: 6.955781936645508\n",
      "Epoch: 1/1 - Step: 623 - Loss: 6.909176826477051\n",
      "Epoch: 1/1 - Step: 624 - Loss: 6.845920085906982\n",
      "Epoch: 1/1 - Step: 625 - Loss: 6.786560535430908\n",
      "Epoch: 1/1 - Step: 626 - Loss: 6.842219829559326\n",
      "Epoch: 1/1 - Step: 627 - Loss: 6.785925388336182\n",
      "Epoch: 1/1 - Step: 628 - Loss: 6.963767051696777\n",
      "Epoch: 1/1 - Step: 629 - Loss: 6.744207382202148\n",
      "Epoch: 1/1 - Step: 630 - Loss: 6.939402103424072\n",
      "Epoch: 1/1 - Step: 631 - Loss: 6.891180992126465\n",
      "Epoch: 1/1 - Step: 632 - Loss: 6.855259895324707\n",
      "Epoch: 1/1 - Step: 633 - Loss: 6.688649654388428\n",
      "Epoch: 1/1 - Step: 634 - Loss: 6.683945655822754\n",
      "Epoch: 1/1 - Step: 635 - Loss: 6.783201694488525\n",
      "Epoch: 1/1 - Step: 636 - Loss: 6.807253837585449\n",
      "Epoch: 1/1 - Step: 637 - Loss: 6.840864181518555\n",
      "Epoch: 1/1 - Step: 638 - Loss: 6.813738822937012\n",
      "Epoch: 1/1 - Step: 639 - Loss: 6.837307453155518\n",
      "Epoch: 1/1 - Step: 640 - Loss: 6.798776626586914\n",
      "Epoch: 1/1 - Step: 641 - Loss: 6.729898452758789\n",
      "Epoch: 1/1 - Step: 642 - Loss: 6.76344633102417\n",
      "Epoch: 1/1 - Step: 643 - Loss: 6.8193359375\n",
      "Epoch: 1/1 - Step: 644 - Loss: 6.750659465789795\n",
      "Epoch: 1/1 - Step: 645 - Loss: 6.717341423034668\n",
      "Epoch: 1/1 - Step: 646 - Loss: 6.964877605438232\n",
      "Epoch: 1/1 - Step: 647 - Loss: 6.8148908615112305\n",
      "Epoch: 1/1 - Step: 648 - Loss: 6.695885181427002\n",
      "Epoch: 1/1 - Step: 649 - Loss: 6.730728626251221\n",
      "Epoch: 1/1 - Step: 650 - Loss: 6.946218967437744\n",
      "Epoch: 1/1 - Step: 651 - Loss: 6.774385452270508\n",
      "Epoch: 1/1 - Step: 652 - Loss: 6.74628210067749\n",
      "Epoch: 1/1 - Step: 653 - Loss: 6.762221813201904\n",
      "Epoch: 1/1 - Step: 654 - Loss: 6.806393146514893\n",
      "Epoch: 1/1 - Step: 655 - Loss: 8.93644905090332\n",
      "Epoch: 1/1 - Step: 656 - Loss: 7.365255832672119\n",
      "Epoch: 1/1 - Step: 657 - Loss: 6.774569034576416\n",
      "Epoch: 1/1 - Step: 658 - Loss: 6.793455123901367\n",
      "Epoch: 1/1 - Step: 659 - Loss: 6.9734578132629395\n",
      "Epoch: 1/1 - Step: 660 - Loss: 6.974953651428223\n",
      "Epoch: 1/1 - Step: 661 - Loss: 6.8864946365356445\n",
      "Epoch: 1/1 - Step: 662 - Loss: 6.826705455780029\n",
      "Epoch: 1/1 - Step: 663 - Loss: 6.972297191619873\n",
      "Epoch: 1/1 - Step: 664 - Loss: 7.0463385581970215\n",
      "Epoch: 1/1 - Step: 665 - Loss: 6.946659564971924\n",
      "Epoch: 1/1 - Step: 666 - Loss: 6.827375411987305\n",
      "Epoch: 1/1 - Step: 667 - Loss: 7.012537002563477\n",
      "Epoch: 1/1 - Step: 668 - Loss: 7.060778617858887\n",
      "Epoch: 1/1 - Step: 669 - Loss: 6.8528900146484375\n",
      "Epoch: 1/1 - Step: 670 - Loss: 6.940429210662842\n",
      "Epoch: 1/1 - Step: 671 - Loss: 6.954302787780762\n",
      "Epoch: 1/1 - Step: 672 - Loss: 6.967185020446777\n",
      "Epoch: 1/1 - Step: 673 - Loss: 6.9090118408203125\n",
      "Epoch: 1/1 - Step: 674 - Loss: 6.95664119720459\n",
      "Epoch: 1/1 - Step: 675 - Loss: 6.8558430671691895\n",
      "Epoch: 1/1 - Step: 676 - Loss: 6.7984771728515625\n",
      "Epoch: 1/1 - Step: 677 - Loss: 6.7398762702941895\n",
      "Epoch: 1/1 - Step: 678 - Loss: 6.786450386047363\n",
      "Epoch: 1/1 - Step: 679 - Loss: 6.802524566650391\n",
      "Epoch: 1/1 - Step: 680 - Loss: 7.269327640533447\n",
      "Epoch: 1/1 - Step: 681 - Loss: 9.194862365722656\n",
      "Epoch: 1/1 - Step: 682 - Loss: 9.360137939453125\n",
      "Epoch: 1/1 - Step: 683 - Loss: 8.726771354675293\n",
      "Epoch: 1/1 - Step: 684 - Loss: 6.965685844421387\n",
      "Epoch: 1/1 - Step: 685 - Loss: 6.940802097320557\n",
      "Epoch: 1/1 - Step: 686 - Loss: 6.875455379486084\n",
      "Epoch: 1/1 - Step: 687 - Loss: 7.02421760559082\n",
      "Epoch: 1/1 - Step: 688 - Loss: 7.01183557510376\n",
      "Epoch: 1/1 - Step: 689 - Loss: 7.000994682312012\n",
      "Epoch: 1/1 - Step: 690 - Loss: 6.948201656341553\n",
      "Epoch: 1/1 - Step: 691 - Loss: 6.6964921951293945\n",
      "Epoch: 1/1 - Step: 692 - Loss: 6.655280113220215\n",
      "Epoch: 1/1 - Step: 693 - Loss: 6.848928451538086\n",
      "Epoch: 1/1 - Step: 694 - Loss: 6.9152984619140625\n",
      "Epoch: 1/1 - Step: 695 - Loss: 6.97172737121582\n",
      "Epoch: 1/1 - Step: 696 - Loss: 6.875450134277344\n",
      "Epoch: 1/1 - Step: 697 - Loss: 7.036594867706299\n",
      "Epoch: 1/1 - Step: 698 - Loss: 7.025735855102539\n",
      "Epoch: 1/1 - Step: 699 - Loss: 7.0494771003723145\n",
      "Epoch: 1/1 - Step: 700 - Loss: 6.954075336456299\n",
      "Epoch: 1/1 - Step: 701 - Loss: 7.017547607421875\n",
      "Epoch: 1/1 - Step: 702 - Loss: 7.0061469078063965\n",
      "Epoch: 1/1 - Step: 703 - Loss: 7.006088733673096\n",
      "Epoch: 1/1 - Step: 704 - Loss: 6.975784778594971\n",
      "Epoch: 1/1 - Step: 705 - Loss: 6.972633361816406\n",
      "Epoch: 1/1 - Step: 706 - Loss: 6.847395420074463\n",
      "Epoch: 1/1 - Step: 707 - Loss: 7.269323348999023\n",
      "Epoch: 1/1 - Step: 708 - Loss: 7.927366256713867\n",
      "Epoch: 1/1 - Step: 709 - Loss: 6.904181480407715\n",
      "Epoch: 1/1 - Step: 710 - Loss: 6.919376850128174\n",
      "Epoch: 1/1 - Step: 711 - Loss: 6.890915393829346\n",
      "Epoch: 1/1 - Step: 712 - Loss: 6.909605026245117\n",
      "Epoch: 1/1 - Step: 713 - Loss: 6.824565887451172\n",
      "Epoch: 1/1 - Step: 714 - Loss: 6.850854396820068\n",
      "Epoch: 1/1 - Step: 715 - Loss: 6.949203968048096\n",
      "Epoch: 1/1 - Step: 716 - Loss: 6.839889049530029\n",
      "Epoch: 1/1 - Step: 717 - Loss: 6.950258731842041\n",
      "Epoch: 1/1 - Step: 718 - Loss: 6.884953022003174\n",
      "Epoch: 1/1 - Step: 719 - Loss: 6.838096618652344\n",
      "Epoch: 1/1 - Step: 720 - Loss: 7.178782939910889\n",
      "Epoch: 1/1 - Step: 721 - Loss: 6.914486408233643\n",
      "Epoch: 1/1 - Step: 722 - Loss: 6.795504570007324\n",
      "Epoch: 1/1 - Step: 723 - Loss: 6.88196325302124\n",
      "Epoch: 1/1 - Step: 724 - Loss: 6.81138277053833\n",
      "Epoch: 1/1 - Step: 725 - Loss: 6.879836559295654\n",
      "Epoch: 1/1 - Step: 726 - Loss: 6.843560218811035\n",
      "Epoch: 1/1 - Step: 727 - Loss: 6.914715766906738\n",
      "Epoch: 1/1 - Step: 728 - Loss: 6.905046463012695\n",
      "Epoch: 1/1 - Step: 729 - Loss: 6.902110576629639\n",
      "Epoch: 1/1 - Step: 730 - Loss: 6.9414544105529785\n",
      "Epoch: 1/1 - Step: 731 - Loss: 6.878592491149902\n",
      "Epoch: 1/1 - Step: 732 - Loss: 6.901500701904297\n",
      "Epoch: 1/1 - Step: 733 - Loss: 6.87676477432251\n",
      "Epoch: 1/1 - Step: 734 - Loss: 6.953708648681641\n",
      "Epoch: 1/1 - Step: 735 - Loss: 6.934352874755859\n",
      "Epoch: 1/1 - Step: 736 - Loss: 6.865140914916992\n",
      "Epoch: 1/1 - Step: 737 - Loss: 7.010645866394043\n",
      "Epoch: 1/1 - Step: 738 - Loss: 7.059446811676025\n",
      "Epoch: 1/1 - Step: 739 - Loss: 6.7396159172058105\n",
      "Epoch: 1/1 - Step: 740 - Loss: 6.725028038024902\n",
      "Epoch: 1/1 - Step: 741 - Loss: 6.698940277099609\n",
      "Epoch: 1/1 - Step: 742 - Loss: 6.774086952209473\n",
      "Epoch: 1/1 - Step: 743 - Loss: 6.752455711364746\n",
      "Epoch: 1/1 - Step: 744 - Loss: 6.892634391784668\n",
      "Epoch: 1/1 - Step: 745 - Loss: 6.838085651397705\n",
      "Epoch: 1/1 - Step: 746 - Loss: 6.885438442230225\n",
      "Epoch: 1/1 - Step: 747 - Loss: 6.897852897644043\n",
      "Epoch: 1/1 - Step: 748 - Loss: 7.072585105895996\n",
      "Epoch: 1/1 - Step: 749 - Loss: 6.9844136238098145\n",
      "Epoch: 1/1 - Step: 750 - Loss: 6.843002796173096\n",
      "Epoch: 1/1 - Step: 751 - Loss: 6.5792131423950195\n",
      "Epoch: 1/1 - Step: 752 - Loss: 6.659473419189453\n",
      "Epoch: 1/1 - Step: 753 - Loss: 6.565976142883301\n",
      "Epoch: 1/1 - Step: 754 - Loss: 6.7073564529418945\n",
      "Epoch: 1/1 - Step: 755 - Loss: 6.817738056182861\n",
      "Epoch: 1/1 - Step: 756 - Loss: 7.066929817199707\n",
      "Epoch: 1/1 - Step: 757 - Loss: 6.8481903076171875\n",
      "Epoch: 1/1 - Step: 758 - Loss: 7.128663539886475\n",
      "Epoch: 1/1 - Step: 759 - Loss: 6.90976619720459\n",
      "Epoch: 1/1 - Step: 760 - Loss: 6.836672782897949\n",
      "Epoch: 1/1 - Step: 761 - Loss: 6.923529148101807\n",
      "Epoch: 1/1 - Step: 762 - Loss: 6.994091510772705\n",
      "Epoch: 1/1 - Step: 763 - Loss: 7.049192905426025\n",
      "Epoch: 1/1 - Step: 764 - Loss: 6.865848064422607\n",
      "Epoch: 1/1 - Step: 765 - Loss: 6.88109827041626\n",
      "Epoch: 1/1 - Step: 766 - Loss: 6.858060359954834\n",
      "Epoch: 1/1 - Step: 767 - Loss: 6.91779088973999\n",
      "Epoch: 1/1 - Step: 768 - Loss: 6.964114665985107\n",
      "Epoch: 1/1 - Step: 769 - Loss: 6.907858848571777\n",
      "Epoch: 1/1 - Step: 770 - Loss: 6.862551689147949\n",
      "Epoch: 1/1 - Step: 771 - Loss: 6.956509113311768\n",
      "Epoch: 1/1 - Step: 772 - Loss: 6.7599029541015625\n",
      "Epoch: 1/1 - Step: 773 - Loss: 6.832731246948242\n",
      "Epoch: 1/1 - Step: 774 - Loss: 6.888577938079834\n",
      "Epoch: 1/1 - Step: 775 - Loss: 6.9032440185546875\n",
      "Epoch: 1/1 - Step: 776 - Loss: 6.871631622314453\n",
      "Epoch: 1/1 - Step: 777 - Loss: 7.001174449920654\n",
      "Epoch: 1/1 - Step: 778 - Loss: 6.8523945808410645\n",
      "Epoch: 1/1 - Step: 779 - Loss: 8.220126152038574\n",
      "Epoch: 1/1 - Step: 780 - Loss: 9.138651847839355\n",
      "Epoch: 1/1 - Step: 781 - Loss: 9.375140190124512\n",
      "Epoch: 1/1 - Step: 782 - Loss: 7.171628475189209\n",
      "Epoch: 1/1 - Step: 783 - Loss: 6.8995585441589355\n",
      "Epoch: 1/1 - Step: 784 - Loss: 6.83615255355835\n",
      "Epoch: 1/1 - Step: 785 - Loss: 6.845876216888428\n",
      "Epoch: 1/1 - Step: 786 - Loss: 6.9391865730285645\n",
      "Epoch: 1/1 - Step: 787 - Loss: 6.894115924835205\n",
      "Epoch: 1/1 - Step: 788 - Loss: 6.933403491973877\n",
      "Epoch: 1/1 - Step: 789 - Loss: 6.910646915435791\n",
      "Epoch: 1/1 - Step: 790 - Loss: 6.9670915603637695\n",
      "Epoch: 1/1 - Step: 791 - Loss: 6.950976371765137\n",
      "Epoch: 1/1 - Step: 792 - Loss: 6.826879501342773\n",
      "Epoch: 1/1 - Step: 793 - Loss: 6.974781036376953\n",
      "Epoch: 1/1 - Step: 794 - Loss: 6.909286022186279\n",
      "Epoch: 1/1 - Step: 795 - Loss: 6.877874851226807\n",
      "Epoch: 1/1 - Step: 796 - Loss: 6.85764741897583\n",
      "Epoch: 1/1 - Step: 797 - Loss: 7.022129535675049\n",
      "Epoch: 1/1 - Step: 798 - Loss: 6.90448522567749\n",
      "Epoch: 1/1 - Step: 799 - Loss: 6.939453601837158\n",
      "Epoch: 1/1 - Step: 800 - Loss: 6.8046417236328125\n",
      "Epoch: 1/1 - Step: 801 - Loss: 6.865850925445557\n",
      "Epoch: 1/1 - Step: 802 - Loss: 6.93051290512085\n",
      "Epoch: 1/1 - Step: 803 - Loss: 6.891812324523926\n",
      "Epoch: 1/1 - Step: 804 - Loss: 6.963232040405273\n",
      "Epoch: 1/1 - Step: 805 - Loss: 6.877288818359375\n",
      "Epoch: 1/1 - Step: 806 - Loss: 6.824043273925781\n",
      "Epoch: 1/1 - Step: 807 - Loss: 6.858005523681641\n",
      "Epoch: 1/1 - Step: 808 - Loss: 6.93338680267334\n",
      "Epoch: 1/1 - Step: 809 - Loss: 6.854434967041016\n",
      "Epoch: 1/1 - Step: 810 - Loss: 7.0234527587890625\n",
      "Epoch: 1/1 - Step: 811 - Loss: 6.9159440994262695\n",
      "Epoch: 1/1 - Step: 812 - Loss: 6.953327655792236\n",
      "Epoch: 1/1 - Step: 813 - Loss: 6.773611068725586\n",
      "Epoch: 1/1 - Step: 814 - Loss: 6.764330863952637\n",
      "Epoch: 1/1 - Step: 815 - Loss: 6.7717766761779785\n",
      "Epoch: 1/1 - Step: 816 - Loss: 6.752038955688477\n",
      "Epoch: 1/1 - Step: 817 - Loss: 6.734463214874268\n",
      "Epoch: 1/1 - Step: 818 - Loss: 6.775969982147217\n",
      "Epoch: 1/1 - Step: 819 - Loss: 6.8453521728515625\n",
      "Epoch: 1/1 - Step: 820 - Loss: 6.802669048309326\n",
      "Epoch: 1/1 - Step: 821 - Loss: 6.7231831550598145\n",
      "Epoch: 1/1 - Step: 822 - Loss: 6.771107196807861\n",
      "Epoch: 1/1 - Step: 823 - Loss: 6.7270283699035645\n",
      "Epoch: 1/1 - Step: 824 - Loss: 6.897248268127441\n",
      "Epoch: 1/1 - Step: 825 - Loss: 7.017210483551025\n",
      "Epoch: 1/1 - Step: 826 - Loss: 6.972445487976074\n",
      "Epoch: 1/1 - Step: 827 - Loss: 6.729701042175293\n",
      "Epoch: 1/1 - Step: 828 - Loss: 6.876366138458252\n",
      "Epoch: 1/1 - Step: 829 - Loss: 6.905309200286865\n",
      "Epoch: 1/1 - Step: 830 - Loss: 6.8891706466674805\n",
      "Epoch: 1/1 - Step: 831 - Loss: 6.789633274078369\n",
      "Epoch: 1/1 - Step: 832 - Loss: 6.835551738739014\n",
      "Epoch: 1/1 - Step: 833 - Loss: 6.77191686630249\n",
      "Epoch: 1/1 - Step: 834 - Loss: 6.842006683349609\n",
      "Epoch: 1/1 - Step: 835 - Loss: 6.881777763366699\n",
      "Epoch: 1/1 - Step: 836 - Loss: 6.7422404289245605\n",
      "Epoch: 1/1 - Step: 837 - Loss: 6.771484851837158\n",
      "Epoch: 1/1 - Step: 838 - Loss: 6.899821758270264\n",
      "Epoch: 1/1 - Step: 839 - Loss: 6.926262378692627\n",
      "Epoch: 1/1 - Step: 840 - Loss: 6.741756439208984\n",
      "Epoch: 1/1 - Step: 841 - Loss: 6.781989574432373\n",
      "Epoch: 1/1 - Step: 842 - Loss: 6.785302639007568\n",
      "Epoch: 1/1 - Step: 843 - Loss: 6.7662672996521\n",
      "Epoch: 1/1 - Step: 844 - Loss: 7.0307159423828125\n",
      "Epoch: 1/1 - Step: 845 - Loss: 6.773083686828613\n",
      "Epoch: 1/1 - Step: 846 - Loss: 6.766589641571045\n",
      "Epoch: 1/1 - Step: 847 - Loss: 6.89768648147583\n",
      "Epoch: 1/1 - Step: 848 - Loss: 6.715860366821289\n",
      "Epoch: 1/1 - Step: 849 - Loss: 6.87697696685791\n",
      "Epoch: 1/1 - Step: 850 - Loss: 6.721505165100098\n",
      "Epoch: 1/1 - Step: 851 - Loss: 6.741775989532471\n",
      "Epoch: 1/1 - Step: 852 - Loss: 6.7197723388671875\n",
      "Epoch: 1/1 - Step: 853 - Loss: 6.944425582885742\n",
      "Epoch: 1/1 - Step: 854 - Loss: 6.778987407684326\n",
      "Epoch: 1/1 - Step: 855 - Loss: 6.679030895233154\n",
      "Epoch: 1/1 - Step: 856 - Loss: 6.853179931640625\n",
      "Epoch: 1/1 - Step: 857 - Loss: 6.787971019744873\n",
      "Epoch: 1/1 - Step: 858 - Loss: 6.992791175842285\n",
      "Epoch: 1/1 - Step: 859 - Loss: 7.086124420166016\n",
      "Epoch: 1/1 - Step: 860 - Loss: 6.921294689178467\n",
      "Epoch: 1/1 - Step: 861 - Loss: 6.910240650177002\n",
      "Epoch: 1/1 - Step: 862 - Loss: 6.877875804901123\n",
      "Epoch: 1/1 - Step: 863 - Loss: 6.785068988800049\n",
      "Epoch: 1/1 - Step: 864 - Loss: 6.697212219238281\n",
      "Epoch: 1/1 - Step: 865 - Loss: 6.686027526855469\n",
      "Epoch: 1/1 - Step: 866 - Loss: 6.710525035858154\n",
      "Epoch: 1/1 - Step: 867 - Loss: 6.79442024230957\n",
      "Epoch: 1/1 - Step: 868 - Loss: 6.671713829040527\n",
      "Epoch: 1/1 - Step: 869 - Loss: 6.642905235290527\n",
      "Epoch: 1/1 - Step: 870 - Loss: 6.836973667144775\n",
      "Epoch: 1/1 - Step: 871 - Loss: 6.994643211364746\n",
      "Epoch: 1/1 - Step: 872 - Loss: 6.687130451202393\n",
      "Epoch: 1/1 - Step: 873 - Loss: 6.8087921142578125\n",
      "Epoch: 1/1 - Step: 874 - Loss: 6.808435916900635\n",
      "Epoch: 1/1 - Step: 875 - Loss: 6.763670444488525\n",
      "Epoch: 1/1 - Step: 876 - Loss: 6.836030960083008\n",
      "Epoch: 1/1 - Step: 877 - Loss: 6.711461544036865\n",
      "Epoch: 1/1 - Step: 878 - Loss: 6.809590816497803\n",
      "Epoch: 1/1 - Step: 879 - Loss: 6.78469705581665\n",
      "Epoch: 1/1 - Step: 880 - Loss: 6.778205871582031\n",
      "Epoch: 1/1 - Step: 881 - Loss: 6.7422194480896\n",
      "Epoch: 1/1 - Step: 882 - Loss: 6.7324395179748535\n",
      "Epoch: 1/1 - Step: 883 - Loss: 6.916684150695801\n",
      "Epoch: 1/1 - Step: 884 - Loss: 6.82036828994751\n",
      "Epoch: 1/1 - Step: 885 - Loss: 6.78736686706543\n",
      "Epoch: 1/1 - Step: 886 - Loss: 6.803382396697998\n",
      "Epoch: 1/1 - Step: 887 - Loss: 6.80269193649292\n",
      "Epoch: 1/1 - Step: 888 - Loss: 6.694675445556641\n",
      "Epoch: 1/1 - Step: 889 - Loss: 6.701136589050293\n",
      "Epoch: 1/1 - Step: 890 - Loss: 6.677668571472168\n",
      "Epoch: 1/1 - Step: 891 - Loss: 6.789919853210449\n",
      "Epoch: 1/1 - Step: 892 - Loss: 6.693887233734131\n",
      "Epoch: 1/1 - Step: 893 - Loss: 6.736427307128906\n",
      "Epoch: 1/1 - Step: 894 - Loss: 6.7056708335876465\n",
      "Epoch: 1/1 - Step: 895 - Loss: 6.6403727531433105\n",
      "Epoch: 1/1 - Step: 896 - Loss: 6.62681770324707\n",
      "Epoch: 1/1 - Step: 897 - Loss: 6.76283597946167\n",
      "Epoch: 1/1 - Step: 898 - Loss: 6.768992900848389\n",
      "Epoch: 1/1 - Step: 899 - Loss: 7.041948318481445\n",
      "Epoch: 1/1 - Step: 900 - Loss: 6.817868709564209\n",
      "Epoch: 1/1 - Step: 901 - Loss: 6.723000526428223\n",
      "Epoch: 1/1 - Step: 902 - Loss: 6.71705961227417\n",
      "Epoch: 1/1 - Step: 903 - Loss: 6.797138214111328\n",
      "Epoch: 1/1 - Step: 904 - Loss: 6.816473007202148\n",
      "Epoch: 1/1 - Step: 905 - Loss: 6.693583965301514\n",
      "Epoch: 1/1 - Step: 906 - Loss: 6.706589221954346\n",
      "Epoch: 1/1 - Step: 907 - Loss: 6.761289596557617\n",
      "Epoch: 1/1 - Step: 908 - Loss: 6.782828330993652\n",
      "Epoch: 1/1 - Step: 909 - Loss: 6.694369316101074\n",
      "Epoch: 1/1 - Step: 910 - Loss: 6.718021869659424\n",
      "Epoch: 1/1 - Step: 911 - Loss: 7.0178070068359375\n",
      "Epoch: 1/1 - Step: 912 - Loss: 9.313774108886719\n",
      "Epoch: 1/1 - Step: 913 - Loss: 7.343331813812256\n",
      "Epoch: 1/1 - Step: 914 - Loss: 6.822432994842529\n",
      "Epoch: 1/1 - Step: 915 - Loss: 6.587935447692871\n",
      "Epoch: 1/1 - Step: 916 - Loss: 6.673899173736572\n",
      "Epoch: 1/1 - Step: 917 - Loss: 6.8070878982543945\n",
      "Epoch: 1/1 - Step: 918 - Loss: 6.890739917755127\n",
      "Epoch: 1/1 - Step: 919 - Loss: 6.721611976623535\n",
      "Epoch: 1/1 - Step: 920 - Loss: 6.769229888916016\n",
      "Epoch: 1/1 - Step: 921 - Loss: 6.853584289550781\n",
      "Epoch: 1/1 - Step: 922 - Loss: 6.946799278259277\n",
      "Epoch: 1/1 - Step: 923 - Loss: 6.93583869934082\n",
      "Epoch: 1/1 - Step: 924 - Loss: 6.763409614562988\n",
      "Epoch: 1/1 - Step: 925 - Loss: 6.668216228485107\n",
      "Epoch: 1/1 - Step: 926 - Loss: 6.729007244110107\n",
      "Epoch: 1/1 - Step: 927 - Loss: 6.858806610107422\n",
      "Epoch: 1/1 - Step: 928 - Loss: 6.909531593322754\n",
      "Epoch: 1/1 - Step: 929 - Loss: 6.7064595222473145\n",
      "Epoch: 1/1 - Step: 930 - Loss: 6.6081438064575195\n",
      "Epoch: 1/1 - Step: 931 - Loss: 6.5389628410339355\n",
      "Epoch: 1/1 - Step: 932 - Loss: 6.680950164794922\n",
      "Epoch: 1/1 - Step: 933 - Loss: 6.891133785247803\n",
      "Epoch: 1/1 - Step: 934 - Loss: 6.649210453033447\n",
      "Epoch: 1/1 - Step: 935 - Loss: 6.742692470550537\n",
      "Epoch: 1/1 - Step: 936 - Loss: 6.796822547912598\n",
      "Epoch: 1/1 - Step: 937 - Loss: 6.76982307434082\n",
      "Epoch: 1/1 - Step: 938 - Loss: 6.698574066162109\n",
      "Epoch: 1/1 - Step: 939 - Loss: 6.851964950561523\n",
      "Epoch: 1/1 - Step: 940 - Loss: 6.808027267456055\n",
      "Epoch: 1/1 - Step: 941 - Loss: 6.703891277313232\n",
      "Epoch: 1/1 - Step: 942 - Loss: 6.710909366607666\n",
      "Epoch: 1/1 - Step: 943 - Loss: 6.73076868057251\n",
      "Epoch: 1/1 - Step: 944 - Loss: 6.7958245277404785\n",
      "Epoch: 1/1 - Step: 945 - Loss: 6.741372585296631\n",
      "Epoch: 1/1 - Step: 946 - Loss: 6.819398403167725\n",
      "Epoch: 1/1 - Step: 947 - Loss: 6.918989658355713\n",
      "Epoch: 1/1 - Step: 948 - Loss: 6.657434463500977\n",
      "Epoch: 1/1 - Step: 949 - Loss: 6.7188286781311035\n",
      "Epoch: 1/1 - Step: 950 - Loss: 6.701957702636719\n",
      "Epoch: 1/1 - Step: 951 - Loss: 6.736030578613281\n",
      "Epoch: 1/1 - Step: 952 - Loss: 6.710064888000488\n",
      "Epoch: 1/1 - Step: 953 - Loss: 6.78471565246582\n",
      "Epoch: 1/1 - Step: 954 - Loss: 6.699212551116943\n",
      "Epoch: 1/1 - Step: 955 - Loss: 6.712330341339111\n",
      "Epoch: 1/1 - Step: 956 - Loss: 6.800754547119141\n",
      "Epoch: 1/1 - Step: 957 - Loss: 6.616293907165527\n",
      "Epoch: 1/1 - Step: 958 - Loss: 6.595933437347412\n",
      "Epoch: 1/1 - Step: 959 - Loss: 6.716860294342041\n",
      "Epoch: 1/1 - Step: 960 - Loss: 6.661472797393799\n",
      "Epoch: 1/1 - Step: 961 - Loss: 7.096356391906738\n",
      "Epoch: 1/1 - Step: 962 - Loss: 9.12475872039795\n",
      "Epoch: 1/1 - Step: 963 - Loss: 7.342578887939453\n",
      "Epoch: 1/1 - Step: 964 - Loss: 6.86743688583374\n",
      "Epoch: 1/1 - Step: 965 - Loss: 6.783760070800781\n",
      "Epoch: 1/1 - Step: 966 - Loss: 6.765288352966309\n",
      "Epoch: 1/1 - Step: 967 - Loss: 6.701347827911377\n",
      "Epoch: 1/1 - Step: 968 - Loss: 6.791939735412598\n",
      "Epoch: 1/1 - Step: 969 - Loss: 6.79303503036499\n",
      "Epoch: 1/1 - Step: 970 - Loss: 6.7808403968811035\n",
      "Epoch: 1/1 - Step: 971 - Loss: 6.705556392669678\n",
      "Epoch: 1/1 - Step: 972 - Loss: 6.889158248901367\n",
      "Epoch: 1/1 - Step: 973 - Loss: 6.808437824249268\n",
      "Epoch: 1/1 - Step: 974 - Loss: 6.719685077667236\n",
      "Epoch: 1/1 - Step: 975 - Loss: 6.576404571533203\n",
      "Epoch: 1/1 - Step: 976 - Loss: 6.665896892547607\n",
      "Epoch: 1/1 - Step: 977 - Loss: 6.7072553634643555\n",
      "Epoch: 1/1 - Step: 978 - Loss: 6.742994785308838\n",
      "Epoch: 1/1 - Step: 979 - Loss: 6.752483367919922\n",
      "Epoch: 1/1 - Step: 980 - Loss: 6.703650951385498\n",
      "Epoch: 1/1 - Step: 981 - Loss: 6.773299217224121\n",
      "Epoch: 1/1 - Step: 982 - Loss: 6.6877851486206055\n",
      "Epoch: 1/1 - Step: 983 - Loss: 6.673800468444824\n",
      "Epoch: 1/1 - Step: 984 - Loss: 6.665140628814697\n",
      "Epoch: 1/1 - Step: 985 - Loss: 6.707422733306885\n",
      "Epoch: 1/1 - Step: 986 - Loss: 6.665079593658447\n",
      "Epoch: 1/1 - Step: 987 - Loss: 6.662225723266602\n",
      "Epoch: 1/1 - Step: 988 - Loss: 6.910079479217529\n",
      "Epoch: 1/1 - Step: 989 - Loss: 6.711635589599609\n",
      "Epoch: 1/1 - Step: 990 - Loss: 6.606227874755859\n",
      "Epoch: 1/1 - Step: 991 - Loss: 6.720151901245117\n",
      "Epoch: 1/1 - Step: 992 - Loss: 6.833584308624268\n",
      "Epoch: 1/1 - Step: 993 - Loss: 6.68866491317749\n",
      "Epoch: 1/1 - Step: 994 - Loss: 6.6329498291015625\n",
      "Epoch: 1/1 - Step: 995 - Loss: 6.704448223114014\n",
      "Epoch: 1/1 - Step: 996 - Loss: 6.894505023956299\n",
      "Epoch: 1/1 - Step: 997 - Loss: 9.148812294006348\n",
      "Epoch: 1/1 - Step: 998 - Loss: 6.841139316558838\n",
      "Epoch: 1/1 - Step: 999 - Loss: 6.669916152954102\n",
      "Epoch: 1/1 - Step: 1000 - Loss: 6.7661895751953125\n",
      "Epoch: 1/1 - Step: 1001 - Loss: 6.928626537322998\n",
      "Epoch: 1/1 - Step: 1002 - Loss: 6.85144567489624\n",
      "Epoch: 1/1 - Step: 1003 - Loss: 6.805220127105713\n",
      "Epoch: 1/1 - Step: 1004 - Loss: 6.7918548583984375\n",
      "Epoch: 1/1 - Step: 1005 - Loss: 6.910823822021484\n",
      "Epoch: 1/1 - Step: 1006 - Loss: 6.914226531982422\n",
      "Epoch: 1/1 - Step: 1007 - Loss: 6.915091514587402\n",
      "Epoch: 1/1 - Step: 1008 - Loss: 6.7817230224609375\n",
      "Epoch: 1/1 - Step: 1009 - Loss: 6.891862869262695\n",
      "Epoch: 1/1 - Step: 1010 - Loss: 6.95262336730957\n",
      "Epoch: 1/1 - Step: 1011 - Loss: 6.824018478393555\n",
      "Epoch: 1/1 - Step: 1012 - Loss: 6.850121021270752\n",
      "Epoch: 1/1 - Step: 1013 - Loss: 6.914871692657471\n",
      "Epoch: 1/1 - Step: 1014 - Loss: 6.868904113769531\n",
      "Epoch: 1/1 - Step: 1015 - Loss: 6.81987190246582\n",
      "Epoch: 1/1 - Step: 1016 - Loss: 6.9244208335876465\n",
      "Epoch: 1/1 - Step: 1017 - Loss: 6.692368030548096\n",
      "Epoch: 1/1 - Step: 1018 - Loss: 6.676538944244385\n",
      "Epoch: 1/1 - Step: 1019 - Loss: 6.695336818695068\n",
      "Epoch: 1/1 - Step: 1020 - Loss: 6.706544876098633\n",
      "Epoch: 1/1 - Step: 1021 - Loss: 6.69309139251709\n",
      "Epoch: 1/1 - Step: 1022 - Loss: 7.715978622436523\n",
      "Epoch: 1/1 - Step: 1023 - Loss: 9.198857307434082\n",
      "Epoch: 1/1 - Step: 1024 - Loss: 9.410911560058594\n",
      "Epoch: 1/1 - Step: 1025 - Loss: 8.079413414001465\n",
      "Epoch: 1/1 - Step: 1026 - Loss: 6.847414016723633\n",
      "Epoch: 1/1 - Step: 1027 - Loss: 6.901947021484375\n",
      "Epoch: 1/1 - Step: 1028 - Loss: 6.77377986907959\n",
      "Epoch: 1/1 - Step: 1029 - Loss: 6.975656032562256\n",
      "Epoch: 1/1 - Step: 1030 - Loss: 6.897517204284668\n",
      "Epoch: 1/1 - Step: 1031 - Loss: 6.963017463684082\n",
      "Epoch: 1/1 - Step: 1032 - Loss: 6.812117099761963\n",
      "Epoch: 1/1 - Step: 1033 - Loss: 6.595344066619873\n",
      "Epoch: 1/1 - Step: 1034 - Loss: 6.621232509613037\n",
      "Epoch: 1/1 - Step: 1035 - Loss: 6.827263355255127\n",
      "Epoch: 1/1 - Step: 1036 - Loss: 6.819920063018799\n",
      "Epoch: 1/1 - Step: 1037 - Loss: 6.890233516693115\n",
      "Epoch: 1/1 - Step: 1038 - Loss: 6.846777439117432\n",
      "Epoch: 1/1 - Step: 1039 - Loss: 6.967667102813721\n",
      "Epoch: 1/1 - Step: 1040 - Loss: 6.932218074798584\n",
      "Epoch: 1/1 - Step: 1041 - Loss: 6.935183048248291\n",
      "Epoch: 1/1 - Step: 1042 - Loss: 6.934499740600586\n",
      "Epoch: 1/1 - Step: 1043 - Loss: 6.908904075622559\n",
      "Epoch: 1/1 - Step: 1044 - Loss: 6.943926811218262\n",
      "Epoch: 1/1 - Step: 1045 - Loss: 6.9615797996521\n",
      "Epoch: 1/1 - Step: 1046 - Loss: 6.875744819641113\n",
      "Epoch: 1/1 - Step: 1047 - Loss: 6.843046188354492\n",
      "Epoch: 1/1 - Step: 1048 - Loss: 6.79607629776001\n",
      "Epoch: 1/1 - Step: 1049 - Loss: 7.613961696624756\n",
      "Epoch: 1/1 - Step: 1050 - Loss: 7.532476425170898\n",
      "Epoch: 1/1 - Step: 1051 - Loss: 6.849892616271973\n",
      "Epoch: 1/1 - Step: 1052 - Loss: 6.832671642303467\n",
      "Epoch: 1/1 - Step: 1053 - Loss: 6.812793254852295\n",
      "Epoch: 1/1 - Step: 1054 - Loss: 6.810638427734375\n",
      "Epoch: 1/1 - Step: 1055 - Loss: 6.7536821365356445\n",
      "Epoch: 1/1 - Step: 1056 - Loss: 6.813629627227783\n",
      "Epoch: 1/1 - Step: 1057 - Loss: 6.849239349365234\n",
      "Epoch: 1/1 - Step: 1058 - Loss: 6.7795729637146\n",
      "Epoch: 1/1 - Step: 1059 - Loss: 6.88232421875\n",
      "Epoch: 1/1 - Step: 1060 - Loss: 6.765832901000977\n",
      "Epoch: 1/1 - Step: 1061 - Loss: 6.874837875366211\n",
      "Epoch: 1/1 - Step: 1062 - Loss: 7.104373931884766\n",
      "Epoch: 1/1 - Step: 1063 - Loss: 6.76004695892334\n",
      "Epoch: 1/1 - Step: 1064 - Loss: 6.715641975402832\n",
      "Epoch: 1/1 - Step: 1065 - Loss: 6.793462753295898\n",
      "Epoch: 1/1 - Step: 1066 - Loss: 6.757962226867676\n",
      "Epoch: 1/1 - Step: 1067 - Loss: 6.794217109680176\n",
      "Epoch: 1/1 - Step: 1068 - Loss: 6.77426290512085\n",
      "Epoch: 1/1 - Step: 1069 - Loss: 6.870002746582031\n",
      "Epoch: 1/1 - Step: 1070 - Loss: 6.808053970336914\n",
      "Epoch: 1/1 - Step: 1071 - Loss: 6.870539665222168\n",
      "Epoch: 1/1 - Step: 1072 - Loss: 6.828494548797607\n",
      "Epoch: 1/1 - Step: 1073 - Loss: 6.777757167816162\n",
      "Epoch: 1/1 - Step: 1074 - Loss: 6.876692771911621\n",
      "Epoch: 1/1 - Step: 1075 - Loss: 6.793991565704346\n",
      "Epoch: 1/1 - Step: 1076 - Loss: 6.868995189666748\n",
      "Epoch: 1/1 - Step: 1077 - Loss: 6.81486177444458\n",
      "Epoch: 1/1 - Step: 1078 - Loss: 6.889926910400391\n",
      "Epoch: 1/1 - Step: 1079 - Loss: 6.921023368835449\n",
      "Epoch: 1/1 - Step: 1080 - Loss: 6.948604106903076\n",
      "Epoch: 1/1 - Step: 1081 - Loss: 6.623519420623779\n",
      "Epoch: 1/1 - Step: 1082 - Loss: 6.6232218742370605\n",
      "Epoch: 1/1 - Step: 1083 - Loss: 6.6923699378967285\n",
      "Epoch: 1/1 - Step: 1084 - Loss: 6.6529717445373535\n",
      "Epoch: 1/1 - Step: 1085 - Loss: 6.694971561431885\n",
      "Epoch: 1/1 - Step: 1086 - Loss: 6.8177032470703125\n",
      "Epoch: 1/1 - Step: 1087 - Loss: 6.763154029846191\n",
      "Epoch: 1/1 - Step: 1088 - Loss: 6.8241167068481445\n",
      "Epoch: 1/1 - Step: 1089 - Loss: 6.850336074829102\n",
      "Epoch: 1/1 - Step: 1090 - Loss: 7.0508036613464355\n",
      "Epoch: 1/1 - Step: 1091 - Loss: 6.861532688140869\n",
      "Epoch: 1/1 - Step: 1092 - Loss: 6.691101551055908\n",
      "Epoch: 1/1 - Step: 1093 - Loss: 6.4965434074401855\n",
      "Epoch: 1/1 - Step: 1094 - Loss: 6.573780059814453\n",
      "Epoch: 1/1 - Step: 1095 - Loss: 6.477003574371338\n",
      "Epoch: 1/1 - Step: 1096 - Loss: 6.704082012176514\n",
      "Epoch: 1/1 - Step: 1097 - Loss: 6.785577297210693\n",
      "Epoch: 1/1 - Step: 1098 - Loss: 6.9565043449401855\n",
      "Epoch: 1/1 - Step: 1099 - Loss: 6.868180751800537\n",
      "Epoch: 1/1 - Step: 1100 - Loss: 7.029902935028076\n",
      "Epoch: 1/1 - Step: 1101 - Loss: 6.800653457641602\n",
      "Epoch: 1/1 - Step: 1102 - Loss: 6.752094268798828\n",
      "Epoch: 1/1 - Step: 1103 - Loss: 6.885123252868652\n",
      "Epoch: 1/1 - Step: 1104 - Loss: 6.914936542510986\n",
      "Epoch: 1/1 - Step: 1105 - Loss: 6.948733329772949\n",
      "Epoch: 1/1 - Step: 1106 - Loss: 6.751911163330078\n",
      "Epoch: 1/1 - Step: 1107 - Loss: 6.784590244293213\n",
      "Epoch: 1/1 - Step: 1108 - Loss: 6.793055057525635\n",
      "Epoch: 1/1 - Step: 1109 - Loss: 6.8876051902771\n",
      "Epoch: 1/1 - Step: 1110 - Loss: 6.941413879394531\n",
      "Epoch: 1/1 - Step: 1111 - Loss: 6.772204399108887\n",
      "Epoch: 1/1 - Step: 1112 - Loss: 6.799641132354736\n",
      "Epoch: 1/1 - Step: 1113 - Loss: 6.8395867347717285\n",
      "Epoch: 1/1 - Step: 1114 - Loss: 6.727473258972168\n",
      "Epoch: 1/1 - Step: 1115 - Loss: 6.769571781158447\n",
      "Epoch: 1/1 - Step: 1116 - Loss: 6.793182373046875\n",
      "Epoch: 1/1 - Step: 1117 - Loss: 6.8727803230285645\n",
      "Epoch: 1/1 - Step: 1118 - Loss: 6.791708469390869\n",
      "Epoch: 1/1 - Step: 1119 - Loss: 6.835888862609863\n",
      "Epoch: 1/1 - Step: 1120 - Loss: 6.850733757019043\n",
      "Epoch: 1/1 - Step: 1121 - Loss: 8.617069244384766\n",
      "Epoch: 1/1 - Step: 1122 - Loss: 9.124088287353516\n",
      "Epoch: 1/1 - Step: 1123 - Loss: 9.069257736206055\n",
      "Epoch: 1/1 - Step: 1124 - Loss: 6.805037498474121\n",
      "Epoch: 1/1 - Step: 1125 - Loss: 6.787642002105713\n",
      "Epoch: 1/1 - Step: 1126 - Loss: 6.7707109451293945\n",
      "Epoch: 1/1 - Step: 1127 - Loss: 6.783648490905762\n",
      "Epoch: 1/1 - Step: 1128 - Loss: 6.9248762130737305\n",
      "Epoch: 1/1 - Step: 1129 - Loss: 6.789069652557373\n",
      "Epoch: 1/1 - Step: 1130 - Loss: 6.8549628257751465\n",
      "Epoch: 1/1 - Step: 1131 - Loss: 6.849515914916992\n",
      "Epoch: 1/1 - Step: 1132 - Loss: 6.887095928192139\n",
      "Epoch: 1/1 - Step: 1133 - Loss: 6.857424259185791\n",
      "Epoch: 1/1 - Step: 1134 - Loss: 6.7978105545043945\n",
      "Epoch: 1/1 - Step: 1135 - Loss: 6.8528947830200195\n",
      "Epoch: 1/1 - Step: 1136 - Loss: 6.85065221786499\n",
      "Epoch: 1/1 - Step: 1137 - Loss: 6.709993839263916\n",
      "Epoch: 1/1 - Step: 1138 - Loss: 6.867222309112549\n",
      "Epoch: 1/1 - Step: 1139 - Loss: 6.9066619873046875\n",
      "Epoch: 1/1 - Step: 1140 - Loss: 6.814277172088623\n",
      "Epoch: 1/1 - Step: 1141 - Loss: 6.862329006195068\n",
      "Epoch: 1/1 - Step: 1142 - Loss: 6.753674507141113\n",
      "Epoch: 1/1 - Step: 1143 - Loss: 6.763814449310303\n",
      "Epoch: 1/1 - Step: 1144 - Loss: 6.890186309814453\n",
      "Epoch: 1/1 - Step: 1145 - Loss: 6.795127868652344\n",
      "Epoch: 1/1 - Step: 1146 - Loss: 6.866943359375\n",
      "Epoch: 1/1 - Step: 1147 - Loss: 6.813195705413818\n",
      "Epoch: 1/1 - Step: 1148 - Loss: 6.763667106628418\n",
      "Epoch: 1/1 - Step: 1149 - Loss: 6.808497428894043\n",
      "Epoch: 1/1 - Step: 1150 - Loss: 6.849552631378174\n",
      "Epoch: 1/1 - Step: 1151 - Loss: 6.8729987144470215\n",
      "Epoch: 1/1 - Step: 1152 - Loss: 6.8494720458984375\n",
      "Epoch: 1/1 - Step: 1153 - Loss: 6.880226135253906\n",
      "Epoch: 1/1 - Step: 1154 - Loss: 6.82611608505249\n",
      "Epoch: 1/1 - Step: 1155 - Loss: 6.705898284912109\n",
      "Epoch: 1/1 - Step: 1156 - Loss: 6.662106037139893\n",
      "Epoch: 1/1 - Step: 1157 - Loss: 6.719336986541748\n",
      "Epoch: 1/1 - Step: 1158 - Loss: 6.640626430511475\n",
      "Epoch: 1/1 - Step: 1159 - Loss: 6.687488555908203\n",
      "Epoch: 1/1 - Step: 1160 - Loss: 6.69096040725708\n",
      "Epoch: 1/1 - Step: 1161 - Loss: 6.7684197425842285\n",
      "Epoch: 1/1 - Step: 1162 - Loss: 6.703573226928711\n",
      "Epoch: 1/1 - Step: 1163 - Loss: 6.628510475158691\n",
      "Epoch: 1/1 - Step: 1164 - Loss: 6.724267482757568\n",
      "Epoch: 1/1 - Step: 1165 - Loss: 6.6258649826049805\n",
      "Epoch: 1/1 - Step: 1166 - Loss: 6.864013195037842\n",
      "Epoch: 1/1 - Step: 1167 - Loss: 6.950281143188477\n",
      "Epoch: 1/1 - Step: 1168 - Loss: 6.857942581176758\n",
      "Epoch: 1/1 - Step: 1169 - Loss: 6.668856620788574\n",
      "Epoch: 1/1 - Step: 1170 - Loss: 6.772278308868408\n",
      "Epoch: 1/1 - Step: 1171 - Loss: 6.8444743156433105\n",
      "Epoch: 1/1 - Step: 1172 - Loss: 6.816627502441406\n",
      "Epoch: 1/1 - Step: 1173 - Loss: 6.735605716705322\n",
      "Epoch: 1/1 - Step: 1174 - Loss: 6.73372220993042\n",
      "Epoch: 1/1 - Step: 1175 - Loss: 6.675014019012451\n",
      "Epoch: 1/1 - Step: 1176 - Loss: 6.760809421539307\n",
      "Epoch: 1/1 - Step: 1177 - Loss: 6.78452730178833\n",
      "Epoch: 1/1 - Step: 1178 - Loss: 6.687057018280029\n",
      "Epoch: 1/1 - Step: 1179 - Loss: 6.736738204956055\n",
      "Epoch: 1/1 - Step: 1180 - Loss: 6.825719833374023\n",
      "Epoch: 1/1 - Step: 1181 - Loss: 6.830233573913574\n",
      "Epoch: 1/1 - Step: 1182 - Loss: 6.664597034454346\n",
      "Epoch: 1/1 - Step: 1183 - Loss: 6.684851169586182\n",
      "Epoch: 1/1 - Step: 1184 - Loss: 6.7552947998046875\n",
      "Epoch: 1/1 - Step: 1185 - Loss: 6.64933443069458\n",
      "Epoch: 1/1 - Step: 1186 - Loss: 7.0195722579956055\n",
      "Epoch: 1/1 - Step: 1187 - Loss: 6.586769104003906\n",
      "Epoch: 1/1 - Step: 1188 - Loss: 6.788748741149902\n",
      "Epoch: 1/1 - Step: 1189 - Loss: 6.756021976470947\n",
      "Epoch: 1/1 - Step: 1190 - Loss: 6.614511966705322\n",
      "Epoch: 1/1 - Step: 1191 - Loss: 6.882744312286377\n",
      "Epoch: 1/1 - Step: 1192 - Loss: 6.562349796295166\n",
      "Epoch: 1/1 - Step: 1193 - Loss: 6.702545642852783\n",
      "Epoch: 1/1 - Step: 1194 - Loss: 6.621633052825928\n",
      "Epoch: 1/1 - Step: 1195 - Loss: 6.89404821395874\n",
      "Epoch: 1/1 - Step: 1196 - Loss: 6.672259330749512\n",
      "Epoch: 1/1 - Step: 1197 - Loss: 6.600229263305664\n",
      "Epoch: 1/1 - Step: 1198 - Loss: 6.808608531951904\n",
      "Epoch: 1/1 - Step: 1199 - Loss: 6.781726837158203\n",
      "Epoch: 1/1 - Step: 1200 - Loss: 6.894537448883057\n",
      "Epoch: 1/1 - Step: 1201 - Loss: 7.02814245223999\n",
      "Epoch: 1/1 - Step: 1202 - Loss: 6.8773274421691895\n",
      "Epoch: 1/1 - Step: 1203 - Loss: 6.785008430480957\n",
      "Epoch: 1/1 - Step: 1204 - Loss: 6.809386730194092\n",
      "Epoch: 1/1 - Step: 1205 - Loss: 6.697781085968018\n",
      "Epoch: 1/1 - Step: 1206 - Loss: 6.579824924468994\n",
      "Epoch: 1/1 - Step: 1207 - Loss: 6.62197208404541\n",
      "Epoch: 1/1 - Step: 1208 - Loss: 6.686816215515137\n",
      "Epoch: 1/1 - Step: 1209 - Loss: 6.637243747711182\n",
      "Epoch: 1/1 - Step: 1210 - Loss: 6.611150741577148\n",
      "Epoch: 1/1 - Step: 1211 - Loss: 6.569010257720947\n",
      "Epoch: 1/1 - Step: 1212 - Loss: 6.804924964904785\n",
      "Epoch: 1/1 - Step: 1213 - Loss: 6.823938846588135\n",
      "Epoch: 1/1 - Step: 1214 - Loss: 6.640638828277588\n",
      "Epoch: 1/1 - Step: 1215 - Loss: 6.73822021484375\n",
      "Epoch: 1/1 - Step: 1216 - Loss: 6.743213176727295\n",
      "Epoch: 1/1 - Step: 1217 - Loss: 6.6398138999938965\n",
      "Epoch: 1/1 - Step: 1218 - Loss: 6.77360725402832\n",
      "Epoch: 1/1 - Step: 1219 - Loss: 6.630418300628662\n",
      "Epoch: 1/1 - Step: 1220 - Loss: 6.7257304191589355\n",
      "Epoch: 1/1 - Step: 1221 - Loss: 6.712912082672119\n",
      "Epoch: 1/1 - Step: 1222 - Loss: 6.677903652191162\n",
      "Epoch: 1/1 - Step: 1223 - Loss: 6.646109580993652\n",
      "Epoch: 1/1 - Step: 1224 - Loss: 6.707859516143799\n",
      "Epoch: 1/1 - Step: 1225 - Loss: 6.812451362609863\n",
      "Epoch: 1/1 - Step: 1226 - Loss: 6.741298675537109\n",
      "Epoch: 1/1 - Step: 1227 - Loss: 6.697803020477295\n",
      "Epoch: 1/1 - Step: 1228 - Loss: 6.748018264770508\n",
      "Epoch: 1/1 - Step: 1229 - Loss: 6.68781852722168\n",
      "Epoch: 1/1 - Step: 1230 - Loss: 6.611868381500244\n",
      "Epoch: 1/1 - Step: 1231 - Loss: 6.616467475891113\n",
      "Epoch: 1/1 - Step: 1232 - Loss: 6.611344814300537\n",
      "Epoch: 1/1 - Step: 1233 - Loss: 6.715665817260742\n",
      "Epoch: 1/1 - Step: 1234 - Loss: 6.627182960510254\n",
      "Epoch: 1/1 - Step: 1235 - Loss: 6.616947650909424\n",
      "Epoch: 1/1 - Step: 1236 - Loss: 6.616090774536133\n",
      "Epoch: 1/1 - Step: 1237 - Loss: 6.551889896392822\n",
      "Epoch: 1/1 - Step: 1238 - Loss: 6.6077561378479\n",
      "Epoch: 1/1 - Step: 1239 - Loss: 6.653735160827637\n",
      "Epoch: 1/1 - Step: 1240 - Loss: 6.760308742523193\n",
      "Epoch: 1/1 - Step: 1241 - Loss: 6.969937324523926\n",
      "Epoch: 1/1 - Step: 1242 - Loss: 6.669186115264893\n",
      "Epoch: 1/1 - Step: 1243 - Loss: 6.6643452644348145\n",
      "Epoch: 1/1 - Step: 1244 - Loss: 6.673161029815674\n",
      "Epoch: 1/1 - Step: 1245 - Loss: 6.655597686767578\n",
      "Epoch: 1/1 - Step: 1246 - Loss: 6.751930236816406\n",
      "Epoch: 1/1 - Step: 1247 - Loss: 6.60146951675415\n",
      "Epoch: 1/1 - Step: 1248 - Loss: 6.625408172607422\n",
      "Epoch: 1/1 - Step: 1249 - Loss: 6.688858985900879\n",
      "Epoch: 1/1 - Step: 1250 - Loss: 6.712010860443115\n",
      "Epoch: 1/1 - Step: 1251 - Loss: 6.5973100662231445\n",
      "Epoch: 1/1 - Step: 1252 - Loss: 6.642119884490967\n",
      "Epoch: 1/1 - Step: 1253 - Loss: 7.485307216644287\n",
      "Epoch: 1/1 - Step: 1254 - Loss: 9.222635269165039\n",
      "Epoch: 1/1 - Step: 1255 - Loss: 6.736031532287598\n",
      "Epoch: 1/1 - Step: 1256 - Loss: 6.68412971496582\n",
      "Epoch: 1/1 - Step: 1257 - Loss: 6.50511360168457\n",
      "Epoch: 1/1 - Step: 1258 - Loss: 6.600427627563477\n",
      "Epoch: 1/1 - Step: 1259 - Loss: 6.820464134216309\n",
      "Epoch: 1/1 - Step: 1260 - Loss: 6.754854679107666\n",
      "Epoch: 1/1 - Step: 1261 - Loss: 6.646835803985596\n",
      "Epoch: 1/1 - Step: 1262 - Loss: 6.7048020362854\n",
      "Epoch: 1/1 - Step: 1263 - Loss: 6.8224077224731445\n",
      "Epoch: 1/1 - Step: 1264 - Loss: 6.884232997894287\n",
      "Epoch: 1/1 - Step: 1265 - Loss: 6.827658176422119\n",
      "Epoch: 1/1 - Step: 1266 - Loss: 6.666376113891602\n",
      "Epoch: 1/1 - Step: 1267 - Loss: 6.562544822692871\n",
      "Epoch: 1/1 - Step: 1268 - Loss: 6.687036514282227\n",
      "Epoch: 1/1 - Step: 1269 - Loss: 6.7306365966796875\n",
      "Epoch: 1/1 - Step: 1270 - Loss: 6.848214149475098\n",
      "Epoch: 1/1 - Step: 1271 - Loss: 6.6208906173706055\n",
      "Epoch: 1/1 - Step: 1272 - Loss: 6.499285697937012\n",
      "Epoch: 1/1 - Step: 1273 - Loss: 6.4374918937683105\n",
      "Epoch: 1/1 - Step: 1274 - Loss: 6.707228183746338\n",
      "Epoch: 1/1 - Step: 1275 - Loss: 6.774209022521973\n",
      "Epoch: 1/1 - Step: 1276 - Loss: 6.575573921203613\n",
      "Epoch: 1/1 - Step: 1277 - Loss: 6.70042085647583\n",
      "Epoch: 1/1 - Step: 1278 - Loss: 6.6919403076171875\n",
      "Epoch: 1/1 - Step: 1279 - Loss: 6.67426061630249\n",
      "Epoch: 1/1 - Step: 1280 - Loss: 6.598780155181885\n",
      "Epoch: 1/1 - Step: 1281 - Loss: 6.830639839172363\n",
      "Epoch: 1/1 - Step: 1282 - Loss: 6.639510631561279\n",
      "Epoch: 1/1 - Step: 1283 - Loss: 6.667206764221191\n",
      "Epoch: 1/1 - Step: 1284 - Loss: 6.609137535095215\n",
      "Epoch: 1/1 - Step: 1285 - Loss: 6.643214225769043\n",
      "Epoch: 1/1 - Step: 1286 - Loss: 6.735107421875\n",
      "Epoch: 1/1 - Step: 1287 - Loss: 6.688692569732666\n",
      "Epoch: 1/1 - Step: 1288 - Loss: 6.821264266967773\n",
      "Epoch: 1/1 - Step: 1289 - Loss: 6.720951080322266\n",
      "Epoch: 1/1 - Step: 1290 - Loss: 6.5892462730407715\n",
      "Epoch: 1/1 - Step: 1291 - Loss: 6.641401767730713\n",
      "Epoch: 1/1 - Step: 1292 - Loss: 6.650470733642578\n",
      "Epoch: 1/1 - Step: 1293 - Loss: 6.66035795211792\n",
      "Epoch: 1/1 - Step: 1294 - Loss: 6.644935607910156\n",
      "Epoch: 1/1 - Step: 1295 - Loss: 6.624422073364258\n",
      "Epoch: 1/1 - Step: 1296 - Loss: 6.668489933013916\n",
      "Epoch: 1/1 - Step: 1297 - Loss: 6.7022175788879395\n",
      "Epoch: 1/1 - Step: 1298 - Loss: 6.631672382354736\n",
      "Epoch: 1/1 - Step: 1299 - Loss: 6.4938435554504395\n",
      "Epoch: 1/1 - Step: 1300 - Loss: 6.582866668701172\n",
      "Epoch: 1/1 - Step: 1301 - Loss: 6.625607013702393\n",
      "Epoch: 1/1 - Step: 1302 - Loss: 6.536927700042725\n",
      "Epoch: 1/1 - Step: 1303 - Loss: 7.584918975830078\n",
      "Epoch: 1/1 - Step: 1304 - Loss: 9.021408081054688\n",
      "Epoch: 1/1 - Step: 1305 - Loss: 6.799121379852295\n",
      "Epoch: 1/1 - Step: 1306 - Loss: 6.715252876281738\n",
      "Epoch: 1/1 - Step: 1307 - Loss: 6.714192867279053\n",
      "Epoch: 1/1 - Step: 1308 - Loss: 6.686479568481445\n",
      "Epoch: 1/1 - Step: 1309 - Loss: 6.65244197845459\n",
      "Epoch: 1/1 - Step: 1310 - Loss: 6.641992568969727\n",
      "Epoch: 1/1 - Step: 1311 - Loss: 6.796785831451416\n",
      "Epoch: 1/1 - Step: 1312 - Loss: 6.6449995040893555\n",
      "Epoch: 1/1 - Step: 1313 - Loss: 6.688343048095703\n",
      "Epoch: 1/1 - Step: 1314 - Loss: 6.795531272888184\n",
      "Epoch: 1/1 - Step: 1315 - Loss: 6.708474636077881\n",
      "Epoch: 1/1 - Step: 1316 - Loss: 6.603290557861328\n",
      "Epoch: 1/1 - Step: 1317 - Loss: 6.535764217376709\n",
      "Epoch: 1/1 - Step: 1318 - Loss: 6.5965576171875\n",
      "Epoch: 1/1 - Step: 1319 - Loss: 6.612941741943359\n",
      "Epoch: 1/1 - Step: 1320 - Loss: 6.69305944442749\n",
      "Epoch: 1/1 - Step: 1321 - Loss: 6.67380952835083\n",
      "Epoch: 1/1 - Step: 1322 - Loss: 6.638589382171631\n",
      "Epoch: 1/1 - Step: 1323 - Loss: 6.652934551239014\n",
      "Epoch: 1/1 - Step: 1324 - Loss: 6.584524154663086\n",
      "Epoch: 1/1 - Step: 1325 - Loss: 6.549136161804199\n",
      "Epoch: 1/1 - Step: 1326 - Loss: 6.691223621368408\n",
      "Epoch: 1/1 - Step: 1327 - Loss: 6.628148555755615\n",
      "Epoch: 1/1 - Step: 1328 - Loss: 6.5406951904296875\n",
      "Epoch: 1/1 - Step: 1329 - Loss: 6.620054244995117\n",
      "Epoch: 1/1 - Step: 1330 - Loss: 6.788092136383057\n",
      "Epoch: 1/1 - Step: 1331 - Loss: 6.621284484863281\n",
      "Epoch: 1/1 - Step: 1332 - Loss: 6.553118705749512\n",
      "Epoch: 1/1 - Step: 1333 - Loss: 6.701437950134277\n",
      "Epoch: 1/1 - Step: 1334 - Loss: 6.623884201049805\n",
      "Epoch: 1/1 - Step: 1335 - Loss: 6.657100200653076\n",
      "Epoch: 1/1 - Step: 1336 - Loss: 6.5584397315979\n",
      "Epoch: 1/1 - Step: 1337 - Loss: 6.631165504455566\n",
      "Epoch: 1/1 - Step: 1338 - Loss: 7.3716583251953125\n",
      "Epoch: 1/1 - Step: 1339 - Loss: 8.522221565246582\n",
      "Epoch: 1/1 - Step: 1340 - Loss: 6.716771602630615\n",
      "Epoch: 1/1 - Step: 1341 - Loss: 6.639320373535156\n",
      "Epoch: 1/1 - Step: 1342 - Loss: 6.692254543304443\n",
      "Epoch: 1/1 - Step: 1343 - Loss: 6.8592681884765625\n",
      "Epoch: 1/1 - Step: 1344 - Loss: 6.732678413391113\n",
      "Epoch: 1/1 - Step: 1345 - Loss: 6.740161895751953\n",
      "Epoch: 1/1 - Step: 1346 - Loss: 6.840104579925537\n",
      "Epoch: 1/1 - Step: 1347 - Loss: 6.782931804656982\n",
      "Epoch: 1/1 - Step: 1348 - Loss: 6.822092533111572\n",
      "Epoch: 1/1 - Step: 1349 - Loss: 6.835267066955566\n",
      "Epoch: 1/1 - Step: 1350 - Loss: 6.72880744934082\n",
      "Epoch: 1/1 - Step: 1351 - Loss: 6.842335224151611\n",
      "Epoch: 1/1 - Step: 1352 - Loss: 6.867359161376953\n",
      "Epoch: 1/1 - Step: 1353 - Loss: 6.7435526847839355\n",
      "Epoch: 1/1 - Step: 1354 - Loss: 6.81301736831665\n",
      "Epoch: 1/1 - Step: 1355 - Loss: 6.795682907104492\n",
      "Epoch: 1/1 - Step: 1356 - Loss: 6.779027462005615\n",
      "Epoch: 1/1 - Step: 1357 - Loss: 6.7579851150512695\n",
      "Epoch: 1/1 - Step: 1358 - Loss: 6.842691898345947\n",
      "Epoch: 1/1 - Step: 1359 - Loss: 6.62847375869751\n",
      "Epoch: 1/1 - Step: 1360 - Loss: 6.55481481552124\n",
      "Epoch: 1/1 - Step: 1361 - Loss: 6.6111979484558105\n",
      "Epoch: 1/1 - Step: 1362 - Loss: 6.630391597747803\n",
      "Epoch: 1/1 - Step: 1363 - Loss: 6.590381622314453\n",
      "Epoch: 1/1 - Step: 1364 - Loss: 8.151039123535156\n",
      "Epoch: 1/1 - Step: 1365 - Loss: 9.141995429992676\n",
      "Epoch: 1/1 - Step: 1366 - Loss: 9.30145263671875\n",
      "Epoch: 1/1 - Step: 1367 - Loss: 7.4954447746276855\n",
      "Epoch: 1/1 - Step: 1368 - Loss: 6.7504425048828125\n",
      "Epoch: 1/1 - Step: 1369 - Loss: 6.868832111358643\n",
      "Epoch: 1/1 - Step: 1370 - Loss: 6.652879238128662\n",
      "Epoch: 1/1 - Step: 1371 - Loss: 6.895003318786621\n",
      "Epoch: 1/1 - Step: 1372 - Loss: 6.8658013343811035\n",
      "Epoch: 1/1 - Step: 1373 - Loss: 6.927407264709473\n",
      "Epoch: 1/1 - Step: 1374 - Loss: 6.66404390335083\n",
      "Epoch: 1/1 - Step: 1375 - Loss: 6.478276252746582\n",
      "Epoch: 1/1 - Step: 1376 - Loss: 6.591580867767334\n",
      "Epoch: 1/1 - Step: 1377 - Loss: 6.795844078063965\n",
      "Epoch: 1/1 - Step: 1378 - Loss: 6.719453811645508\n",
      "Epoch: 1/1 - Step: 1379 - Loss: 6.809659481048584\n",
      "Epoch: 1/1 - Step: 1380 - Loss: 6.781734466552734\n",
      "Epoch: 1/1 - Step: 1381 - Loss: 6.91277551651001\n",
      "Epoch: 1/1 - Step: 1382 - Loss: 6.828211307525635\n",
      "Epoch: 1/1 - Step: 1383 - Loss: 6.882699966430664\n",
      "Epoch: 1/1 - Step: 1384 - Loss: 6.865644454956055\n",
      "Epoch: 1/1 - Step: 1385 - Loss: 6.838517665863037\n",
      "Epoch: 1/1 - Step: 1386 - Loss: 6.876465320587158\n",
      "Epoch: 1/1 - Step: 1387 - Loss: 6.821835517883301\n",
      "Epoch: 1/1 - Step: 1388 - Loss: 6.817195415496826\n",
      "Epoch: 1/1 - Step: 1389 - Loss: 6.758259296417236\n",
      "Epoch: 1/1 - Step: 1390 - Loss: 6.746703624725342\n",
      "Epoch: 1/1 - Step: 1391 - Loss: 7.9241485595703125\n",
      "Epoch: 1/1 - Step: 1392 - Loss: 7.079432487487793\n",
      "Epoch: 1/1 - Step: 1393 - Loss: 6.819595813751221\n",
      "Epoch: 1/1 - Step: 1394 - Loss: 6.740307807922363\n",
      "Epoch: 1/1 - Step: 1395 - Loss: 6.730836868286133\n",
      "Epoch: 1/1 - Step: 1396 - Loss: 6.694247722625732\n",
      "Epoch: 1/1 - Step: 1397 - Loss: 6.70936393737793\n",
      "Epoch: 1/1 - Step: 1398 - Loss: 6.7527546882629395\n",
      "Epoch: 1/1 - Step: 1399 - Loss: 6.774486064910889\n",
      "Epoch: 1/1 - Step: 1400 - Loss: 6.70542049407959\n",
      "Epoch: 1/1 - Step: 1401 - Loss: 6.825111389160156\n",
      "Epoch: 1/1 - Step: 1402 - Loss: 6.673418998718262\n",
      "Epoch: 1/1 - Step: 1403 - Loss: 6.830738544464111\n",
      "Epoch: 1/1 - Step: 1404 - Loss: 7.0335283279418945\n",
      "Epoch: 1/1 - Step: 1405 - Loss: 6.62993049621582\n",
      "Epoch: 1/1 - Step: 1406 - Loss: 6.677949905395508\n",
      "Epoch: 1/1 - Step: 1407 - Loss: 6.705894947052002\n",
      "Epoch: 1/1 - Step: 1408 - Loss: 6.685433864593506\n",
      "Epoch: 1/1 - Step: 1409 - Loss: 6.728776931762695\n",
      "Epoch: 1/1 - Step: 1410 - Loss: 6.723330497741699\n",
      "Epoch: 1/1 - Step: 1411 - Loss: 6.754209518432617\n",
      "Epoch: 1/1 - Step: 1412 - Loss: 6.760067462921143\n",
      "Epoch: 1/1 - Step: 1413 - Loss: 6.805747032165527\n",
      "Epoch: 1/1 - Step: 1414 - Loss: 6.735043525695801\n",
      "Epoch: 1/1 - Step: 1415 - Loss: 6.709348678588867\n",
      "Epoch: 1/1 - Step: 1416 - Loss: 6.810536861419678\n",
      "Epoch: 1/1 - Step: 1417 - Loss: 6.7319135665893555\n",
      "Epoch: 1/1 - Step: 1418 - Loss: 6.781914710998535\n",
      "Epoch: 1/1 - Step: 1419 - Loss: 6.729696750640869\n",
      "Epoch: 1/1 - Step: 1420 - Loss: 6.81814432144165\n",
      "Epoch: 1/1 - Step: 1421 - Loss: 6.917654037475586\n",
      "Epoch: 1/1 - Step: 1422 - Loss: 6.766078472137451\n",
      "Epoch: 1/1 - Step: 1423 - Loss: 6.571887016296387\n",
      "Epoch: 1/1 - Step: 1424 - Loss: 6.527105808258057\n",
      "Epoch: 1/1 - Step: 1425 - Loss: 6.657118320465088\n",
      "Epoch: 1/1 - Step: 1426 - Loss: 6.553864479064941\n",
      "Epoch: 1/1 - Step: 1427 - Loss: 6.676548004150391\n",
      "Epoch: 1/1 - Step: 1428 - Loss: 6.687286853790283\n",
      "Epoch: 1/1 - Step: 1429 - Loss: 6.6860127449035645\n",
      "Epoch: 1/1 - Step: 1430 - Loss: 6.767952919006348\n",
      "Epoch: 1/1 - Step: 1431 - Loss: 6.7957000732421875\n",
      "Epoch: 1/1 - Step: 1432 - Loss: 7.018678188323975\n",
      "Epoch: 1/1 - Step: 1433 - Loss: 6.759608745574951\n",
      "Epoch: 1/1 - Step: 1434 - Loss: 6.497697830200195\n",
      "Epoch: 1/1 - Step: 1435 - Loss: 6.47269868850708\n",
      "Epoch: 1/1 - Step: 1436 - Loss: 6.438392162322998\n",
      "Epoch: 1/1 - Step: 1437 - Loss: 6.43954610824585\n",
      "Epoch: 1/1 - Step: 1438 - Loss: 6.683350563049316\n",
      "Epoch: 1/1 - Step: 1439 - Loss: 6.737112045288086\n",
      "Epoch: 1/1 - Step: 1440 - Loss: 6.877355575561523\n",
      "Epoch: 1/1 - Step: 1441 - Loss: 6.848841667175293\n",
      "Epoch: 1/1 - Step: 1442 - Loss: 6.927727699279785\n",
      "Epoch: 1/1 - Step: 1443 - Loss: 6.681085109710693\n",
      "Epoch: 1/1 - Step: 1444 - Loss: 6.727842807769775\n",
      "Epoch: 1/1 - Step: 1445 - Loss: 6.80370569229126\n",
      "Epoch: 1/1 - Step: 1446 - Loss: 6.853738784790039\n",
      "Epoch: 1/1 - Step: 1447 - Loss: 6.866588115692139\n",
      "Epoch: 1/1 - Step: 1448 - Loss: 6.685211658477783\n",
      "Epoch: 1/1 - Step: 1449 - Loss: 6.7092204093933105\n",
      "Epoch: 1/1 - Step: 1450 - Loss: 6.725376605987549\n",
      "Epoch: 1/1 - Step: 1451 - Loss: 6.822454929351807\n",
      "Epoch: 1/1 - Step: 1452 - Loss: 6.918737888336182\n",
      "Epoch: 1/1 - Step: 1453 - Loss: 6.6712327003479\n",
      "Epoch: 1/1 - Step: 1454 - Loss: 6.755616188049316\n",
      "Epoch: 1/1 - Step: 1455 - Loss: 6.688331127166748\n",
      "Epoch: 1/1 - Step: 1456 - Loss: 6.749485492706299\n",
      "Epoch: 1/1 - Step: 1457 - Loss: 6.616111755371094\n",
      "Epoch: 1/1 - Step: 1458 - Loss: 6.742487907409668\n",
      "Epoch: 1/1 - Step: 1459 - Loss: 6.778541564941406\n",
      "Epoch: 1/1 - Step: 1460 - Loss: 6.765020370483398\n",
      "Epoch: 1/1 - Step: 1461 - Loss: 6.74368143081665\n",
      "Epoch: 1/1 - Step: 1462 - Loss: 6.896054267883301\n",
      "Epoch: 1/1 - Step: 1463 - Loss: 8.857940673828125\n",
      "Epoch: 1/1 - Step: 1464 - Loss: 9.186877250671387\n",
      "Epoch: 1/1 - Step: 1465 - Loss: 8.38042163848877\n",
      "Epoch: 1/1 - Step: 1466 - Loss: 6.783695697784424\n",
      "Epoch: 1/1 - Step: 1467 - Loss: 6.677618980407715\n",
      "Epoch: 1/1 - Step: 1468 - Loss: 6.71108865737915\n",
      "Epoch: 1/1 - Step: 1469 - Loss: 6.799210071563721\n",
      "Epoch: 1/1 - Step: 1470 - Loss: 6.814478874206543\n",
      "Epoch: 1/1 - Step: 1471 - Loss: 6.692566871643066\n",
      "Epoch: 1/1 - Step: 1472 - Loss: 6.762347221374512\n",
      "Epoch: 1/1 - Step: 1473 - Loss: 6.808576583862305\n",
      "Epoch: 1/1 - Step: 1474 - Loss: 6.802399635314941\n",
      "Epoch: 1/1 - Step: 1475 - Loss: 6.768013954162598\n",
      "Epoch: 1/1 - Step: 1476 - Loss: 6.729698657989502\n",
      "Epoch: 1/1 - Step: 1477 - Loss: 6.769570827484131\n",
      "Epoch: 1/1 - Step: 1478 - Loss: 6.7753376960754395\n",
      "Epoch: 1/1 - Step: 1479 - Loss: 6.61525297164917\n",
      "Epoch: 1/1 - Step: 1480 - Loss: 6.830244064331055\n",
      "Epoch: 1/1 - Step: 1481 - Loss: 6.810386657714844\n",
      "Epoch: 1/1 - Step: 1482 - Loss: 6.761225700378418\n",
      "Epoch: 1/1 - Step: 1483 - Loss: 6.759105205535889\n",
      "Epoch: 1/1 - Step: 1484 - Loss: 6.7326579093933105\n",
      "Epoch: 1/1 - Step: 1485 - Loss: 6.6829071044921875\n",
      "Epoch: 1/1 - Step: 1486 - Loss: 6.804727554321289\n",
      "Epoch: 1/1 - Step: 1487 - Loss: 6.726894378662109\n",
      "Epoch: 1/1 - Step: 1488 - Loss: 6.782816410064697\n",
      "Epoch: 1/1 - Step: 1489 - Loss: 6.730443000793457\n",
      "Epoch: 1/1 - Step: 1490 - Loss: 6.726937294006348\n",
      "Epoch: 1/1 - Step: 1491 - Loss: 6.777299404144287\n",
      "Epoch: 1/1 - Step: 1492 - Loss: 6.676416873931885\n",
      "Epoch: 1/1 - Step: 1493 - Loss: 6.92681360244751\n",
      "Epoch: 1/1 - Step: 1494 - Loss: 6.702195167541504\n",
      "Epoch: 1/1 - Step: 1495 - Loss: 6.7921295166015625\n",
      "Epoch: 1/1 - Step: 1496 - Loss: 6.717695713043213\n",
      "Epoch: 1/1 - Step: 1497 - Loss: 6.6267900466918945\n",
      "Epoch: 1/1 - Step: 1498 - Loss: 6.6114115715026855\n",
      "Epoch: 1/1 - Step: 1499 - Loss: 6.62790060043335\n",
      "Epoch: 1/1 - Step: 1500 - Loss: 6.561933517456055\n",
      "Epoch: 1/1 - Step: 1501 - Loss: 6.5966596603393555\n",
      "Epoch: 1/1 - Step: 1502 - Loss: 6.628077030181885\n",
      "Epoch: 1/1 - Step: 1503 - Loss: 6.694644927978516\n",
      "Epoch: 1/1 - Step: 1504 - Loss: 6.609517574310303\n",
      "Epoch: 1/1 - Step: 1505 - Loss: 6.594728469848633\n",
      "Epoch: 1/1 - Step: 1506 - Loss: 6.597419738769531\n",
      "Epoch: 1/1 - Step: 1507 - Loss: 6.601778507232666\n",
      "Epoch: 1/1 - Step: 1508 - Loss: 6.834022045135498\n",
      "Epoch: 1/1 - Step: 1509 - Loss: 6.857466220855713\n",
      "Epoch: 1/1 - Step: 1510 - Loss: 6.722702503204346\n",
      "Epoch: 1/1 - Step: 1511 - Loss: 6.660728931427002\n",
      "Epoch: 1/1 - Step: 1512 - Loss: 6.696475505828857\n",
      "Epoch: 1/1 - Step: 1513 - Loss: 6.794325828552246\n",
      "Epoch: 1/1 - Step: 1514 - Loss: 6.735067367553711\n",
      "Epoch: 1/1 - Step: 1515 - Loss: 6.616413593292236\n",
      "Epoch: 1/1 - Step: 1516 - Loss: 6.6270527839660645\n",
      "Epoch: 1/1 - Step: 1517 - Loss: 6.682786464691162\n",
      "Epoch: 1/1 - Step: 1518 - Loss: 6.691910743713379\n",
      "Epoch: 1/1 - Step: 1519 - Loss: 6.672280311584473\n",
      "Epoch: 1/1 - Step: 1520 - Loss: 6.599956512451172\n",
      "Epoch: 1/1 - Step: 1521 - Loss: 6.682857990264893\n",
      "Epoch: 1/1 - Step: 1522 - Loss: 6.808472156524658\n",
      "Epoch: 1/1 - Step: 1523 - Loss: 6.645482063293457\n",
      "Epoch: 1/1 - Step: 1524 - Loss: 6.631232738494873\n",
      "Epoch: 1/1 - Step: 1525 - Loss: 6.604214191436768\n",
      "Epoch: 1/1 - Step: 1526 - Loss: 6.629787921905518\n",
      "Epoch: 1/1 - Step: 1527 - Loss: 6.650667667388916\n",
      "Epoch: 1/1 - Step: 1528 - Loss: 6.888849258422852\n",
      "Epoch: 1/1 - Step: 1529 - Loss: 6.568281650543213\n",
      "Epoch: 1/1 - Step: 1530 - Loss: 6.72624397277832\n",
      "Epoch: 1/1 - Step: 1531 - Loss: 6.657029151916504\n",
      "Epoch: 1/1 - Step: 1532 - Loss: 6.5492119789123535\n",
      "Epoch: 1/1 - Step: 1533 - Loss: 6.791513442993164\n",
      "Epoch: 1/1 - Step: 1534 - Loss: 6.457972049713135\n",
      "Epoch: 1/1 - Step: 1535 - Loss: 6.670256614685059\n",
      "Epoch: 1/1 - Step: 1536 - Loss: 6.53578519821167\n",
      "Epoch: 1/1 - Step: 1537 - Loss: 6.79163122177124\n",
      "Epoch: 1/1 - Step: 1538 - Loss: 6.622207164764404\n",
      "Epoch: 1/1 - Step: 1539 - Loss: 6.576174259185791\n",
      "Epoch: 1/1 - Step: 1540 - Loss: 6.665414333343506\n",
      "Epoch: 1/1 - Step: 1541 - Loss: 6.766536712646484\n",
      "Epoch: 1/1 - Step: 1542 - Loss: 6.80722188949585\n",
      "Epoch: 1/1 - Step: 1543 - Loss: 6.97230339050293\n",
      "Epoch: 1/1 - Step: 1544 - Loss: 6.762200355529785\n",
      "Epoch: 1/1 - Step: 1545 - Loss: 6.780325412750244\n",
      "Epoch: 1/1 - Step: 1546 - Loss: 6.664919376373291\n",
      "Epoch: 1/1 - Step: 1547 - Loss: 6.603335380554199\n",
      "Epoch: 1/1 - Step: 1548 - Loss: 6.500540733337402\n",
      "Epoch: 1/1 - Step: 1549 - Loss: 6.539464473724365\n",
      "Epoch: 1/1 - Step: 1550 - Loss: 6.657017707824707\n",
      "Epoch: 1/1 - Step: 1551 - Loss: 6.559053421020508\n",
      "Epoch: 1/1 - Step: 1552 - Loss: 6.509017467498779\n",
      "Epoch: 1/1 - Step: 1553 - Loss: 6.557615756988525\n",
      "Epoch: 1/1 - Step: 1554 - Loss: 6.798027992248535\n",
      "Epoch: 1/1 - Step: 1555 - Loss: 6.597740650177002\n",
      "Epoch: 1/1 - Step: 1556 - Loss: 6.63293981552124\n",
      "Epoch: 1/1 - Step: 1557 - Loss: 6.6288299560546875\n",
      "Epoch: 1/1 - Step: 1558 - Loss: 6.6844258308410645\n",
      "Epoch: 1/1 - Step: 1559 - Loss: 6.600225925445557\n",
      "Epoch: 1/1 - Step: 1560 - Loss: 6.614463806152344\n",
      "Epoch: 1/1 - Step: 1561 - Loss: 6.621999263763428\n",
      "Epoch: 1/1 - Step: 1562 - Loss: 6.59296178817749\n",
      "Epoch: 1/1 - Step: 1563 - Loss: 6.64031457901001\n",
      "Epoch: 1/1 - Step: 1564 - Loss: 6.591524124145508\n",
      "Epoch: 1/1 - Step: 1565 - Loss: 6.575186252593994\n",
      "Epoch: 1/1 - Step: 1566 - Loss: 6.68424654006958\n",
      "Epoch: 1/1 - Step: 1567 - Loss: 6.7122650146484375\n",
      "Epoch: 1/1 - Step: 1568 - Loss: 6.669784069061279\n",
      "Epoch: 1/1 - Step: 1569 - Loss: 6.603861331939697\n",
      "Epoch: 1/1 - Step: 1570 - Loss: 6.683324337005615\n",
      "Epoch: 1/1 - Step: 1571 - Loss: 6.608554363250732\n",
      "Epoch: 1/1 - Step: 1572 - Loss: 6.555761337280273\n",
      "Epoch: 1/1 - Step: 1573 - Loss: 6.521477222442627\n",
      "Epoch: 1/1 - Step: 1574 - Loss: 6.578064441680908\n",
      "Epoch: 1/1 - Step: 1575 - Loss: 6.617067337036133\n",
      "Epoch: 1/1 - Step: 1576 - Loss: 6.548326015472412\n",
      "Epoch: 1/1 - Step: 1577 - Loss: 6.55388879776001\n",
      "Epoch: 1/1 - Step: 1578 - Loss: 6.521861553192139\n",
      "Epoch: 1/1 - Step: 1579 - Loss: 6.47552490234375\n",
      "Epoch: 1/1 - Step: 1580 - Loss: 6.53459358215332\n",
      "Epoch: 1/1 - Step: 1581 - Loss: 6.6197404861450195\n",
      "Epoch: 1/1 - Step: 1582 - Loss: 6.685720443725586\n",
      "Epoch: 1/1 - Step: 1583 - Loss: 6.890777587890625\n",
      "Epoch: 1/1 - Step: 1584 - Loss: 6.5804362297058105\n",
      "Epoch: 1/1 - Step: 1585 - Loss: 6.581539154052734\n",
      "Epoch: 1/1 - Step: 1586 - Loss: 6.583816051483154\n",
      "Epoch: 1/1 - Step: 1587 - Loss: 6.6423115730285645\n",
      "Epoch: 1/1 - Step: 1588 - Loss: 6.661470413208008\n",
      "Epoch: 1/1 - Step: 1589 - Loss: 6.515145778656006\n",
      "Epoch: 1/1 - Step: 1590 - Loss: 6.589178085327148\n",
      "Epoch: 1/1 - Step: 1591 - Loss: 6.595984935760498\n",
      "Epoch: 1/1 - Step: 1592 - Loss: 6.6524248123168945\n",
      "Epoch: 1/1 - Step: 1593 - Loss: 6.523969650268555\n",
      "Epoch: 1/1 - Step: 1594 - Loss: 6.548148155212402\n",
      "Epoch: 1/1 - Step: 1595 - Loss: 7.998661994934082\n",
      "Epoch: 1/1 - Step: 1596 - Loss: 8.569175720214844\n",
      "Epoch: 1/1 - Step: 1597 - Loss: 6.683840751647949\n",
      "Epoch: 1/1 - Step: 1598 - Loss: 6.529200077056885\n",
      "Epoch: 1/1 - Step: 1599 - Loss: 6.426427364349365\n",
      "Epoch: 1/1 - Step: 1600 - Loss: 6.588733196258545\n",
      "Epoch: 1/1 - Step: 1601 - Loss: 6.762515544891357\n",
      "Epoch: 1/1 - Step: 1602 - Loss: 6.609557151794434\n",
      "Epoch: 1/1 - Step: 1603 - Loss: 6.604623794555664\n",
      "Epoch: 1/1 - Step: 1604 - Loss: 6.618594169616699\n",
      "Epoch: 1/1 - Step: 1605 - Loss: 6.8001885414123535\n",
      "Epoch: 1/1 - Step: 1606 - Loss: 6.81593132019043\n",
      "Epoch: 1/1 - Step: 1607 - Loss: 6.72604513168335\n",
      "Epoch: 1/1 - Step: 1608 - Loss: 6.591635227203369\n",
      "Epoch: 1/1 - Step: 1609 - Loss: 6.450376987457275\n",
      "Epoch: 1/1 - Step: 1610 - Loss: 6.650980472564697\n",
      "Epoch: 1/1 - Step: 1611 - Loss: 6.716577053070068\n",
      "Epoch: 1/1 - Step: 1612 - Loss: 6.687244892120361\n",
      "Epoch: 1/1 - Step: 1613 - Loss: 6.547196865081787\n",
      "Epoch: 1/1 - Step: 1614 - Loss: 6.391003131866455\n",
      "Epoch: 1/1 - Step: 1615 - Loss: 6.379646301269531\n",
      "Epoch: 1/1 - Step: 1616 - Loss: 6.734472751617432\n",
      "Epoch: 1/1 - Step: 1617 - Loss: 6.599050998687744\n",
      "Epoch: 1/1 - Step: 1618 - Loss: 6.5182342529296875\n",
      "Epoch: 1/1 - Step: 1619 - Loss: 6.671014785766602\n",
      "Epoch: 1/1 - Step: 1620 - Loss: 6.634590148925781\n",
      "Epoch: 1/1 - Step: 1621 - Loss: 6.551978588104248\n",
      "Epoch: 1/1 - Step: 1622 - Loss: 6.542593955993652\n",
      "Epoch: 1/1 - Step: 1623 - Loss: 6.775838851928711\n",
      "Epoch: 1/1 - Step: 1624 - Loss: 6.542386054992676\n",
      "Epoch: 1/1 - Step: 1625 - Loss: 6.582657337188721\n",
      "Epoch: 1/1 - Step: 1626 - Loss: 6.54896354675293\n",
      "Epoch: 1/1 - Step: 1627 - Loss: 6.560629367828369\n",
      "Epoch: 1/1 - Step: 1628 - Loss: 6.644291400909424\n",
      "Epoch: 1/1 - Step: 1629 - Loss: 6.616004943847656\n",
      "Epoch: 1/1 - Step: 1630 - Loss: 6.808109760284424\n",
      "Epoch: 1/1 - Step: 1631 - Loss: 6.563791751861572\n",
      "Epoch: 1/1 - Step: 1632 - Loss: 6.521937370300293\n",
      "Epoch: 1/1 - Step: 1633 - Loss: 6.570339202880859\n",
      "Epoch: 1/1 - Step: 1634 - Loss: 6.598874092102051\n",
      "Epoch: 1/1 - Step: 1635 - Loss: 6.566659927368164\n",
      "Epoch: 1/1 - Step: 1636 - Loss: 6.5594801902771\n",
      "Epoch: 1/1 - Step: 1637 - Loss: 6.532147407531738\n",
      "Epoch: 1/1 - Step: 1638 - Loss: 6.655980587005615\n",
      "Epoch: 1/1 - Step: 1639 - Loss: 6.6343560218811035\n",
      "Epoch: 1/1 - Step: 1640 - Loss: 6.490000247955322\n",
      "Epoch: 1/1 - Step: 1641 - Loss: 6.403519153594971\n",
      "Epoch: 1/1 - Step: 1642 - Loss: 6.512678146362305\n",
      "Epoch: 1/1 - Step: 1643 - Loss: 6.54447603225708\n",
      "Epoch: 1/1 - Step: 1644 - Loss: 6.470534324645996\n",
      "Epoch: 1/1 - Step: 1645 - Loss: 8.098447799682617\n",
      "Epoch: 1/1 - Step: 1646 - Loss: 8.3385009765625\n",
      "Epoch: 1/1 - Step: 1647 - Loss: 6.734462738037109\n",
      "Epoch: 1/1 - Step: 1648 - Loss: 6.628415107727051\n",
      "Epoch: 1/1 - Step: 1649 - Loss: 6.6742024421691895\n",
      "Epoch: 1/1 - Step: 1650 - Loss: 6.567460060119629\n",
      "Epoch: 1/1 - Step: 1651 - Loss: 6.593231678009033\n",
      "Epoch: 1/1 - Step: 1652 - Loss: 6.5713982582092285\n",
      "Epoch: 1/1 - Step: 1653 - Loss: 6.776895999908447\n",
      "Epoch: 1/1 - Step: 1654 - Loss: 6.4652323722839355\n",
      "Epoch: 1/1 - Step: 1655 - Loss: 6.668832778930664\n",
      "Epoch: 1/1 - Step: 1656 - Loss: 6.69026517868042\n",
      "Epoch: 1/1 - Step: 1657 - Loss: 6.625543594360352\n",
      "Epoch: 1/1 - Step: 1658 - Loss: 6.553515434265137\n",
      "Epoch: 1/1 - Step: 1659 - Loss: 6.46447229385376\n",
      "Epoch: 1/1 - Step: 1660 - Loss: 6.530665874481201\n",
      "Epoch: 1/1 - Step: 1661 - Loss: 6.5523762702941895\n",
      "Epoch: 1/1 - Step: 1662 - Loss: 6.612483024597168\n",
      "Epoch: 1/1 - Step: 1663 - Loss: 6.591627597808838\n",
      "Epoch: 1/1 - Step: 1664 - Loss: 6.559179306030273\n",
      "Epoch: 1/1 - Step: 1665 - Loss: 6.618420124053955\n",
      "Epoch: 1/1 - Step: 1666 - Loss: 6.486222743988037\n",
      "Epoch: 1/1 - Step: 1667 - Loss: 6.495235443115234\n",
      "Epoch: 1/1 - Step: 1668 - Loss: 6.628048896789551\n",
      "Epoch: 1/1 - Step: 1669 - Loss: 6.505836486816406\n",
      "Epoch: 1/1 - Step: 1670 - Loss: 6.467740058898926\n",
      "Epoch: 1/1 - Step: 1671 - Loss: 6.589158535003662\n",
      "Epoch: 1/1 - Step: 1672 - Loss: 6.706191539764404\n",
      "Epoch: 1/1 - Step: 1673 - Loss: 6.481438636779785\n",
      "Epoch: 1/1 - Step: 1674 - Loss: 6.4907073974609375\n",
      "Epoch: 1/1 - Step: 1675 - Loss: 6.738991737365723\n",
      "Epoch: 1/1 - Step: 1676 - Loss: 6.486495494842529\n",
      "Epoch: 1/1 - Step: 1677 - Loss: 6.545834064483643\n",
      "Epoch: 1/1 - Step: 1678 - Loss: 6.4968109130859375\n",
      "Epoch: 1/1 - Step: 1679 - Loss: 6.592837333679199\n",
      "Epoch: 1/1 - Step: 1680 - Loss: 7.795021057128906\n",
      "Epoch: 1/1 - Step: 1681 - Loss: 7.924665451049805\n",
      "Epoch: 1/1 - Step: 1682 - Loss: 6.629789352416992\n",
      "Epoch: 1/1 - Step: 1683 - Loss: 6.52123498916626\n",
      "Epoch: 1/1 - Step: 1684 - Loss: 6.70357084274292\n",
      "Epoch: 1/1 - Step: 1685 - Loss: 6.745415210723877\n",
      "Epoch: 1/1 - Step: 1686 - Loss: 6.704933166503906\n",
      "Epoch: 1/1 - Step: 1687 - Loss: 6.69089937210083\n",
      "Epoch: 1/1 - Step: 1688 - Loss: 6.728574275970459\n",
      "Epoch: 1/1 - Step: 1689 - Loss: 6.797785758972168\n",
      "Epoch: 1/1 - Step: 1690 - Loss: 6.7676496505737305\n",
      "Epoch: 1/1 - Step: 1691 - Loss: 6.6881937980651855\n",
      "Epoch: 1/1 - Step: 1692 - Loss: 6.753342628479004\n",
      "Epoch: 1/1 - Step: 1693 - Loss: 6.822608947753906\n",
      "Epoch: 1/1 - Step: 1694 - Loss: 6.688088417053223\n",
      "Epoch: 1/1 - Step: 1695 - Loss: 6.723555088043213\n",
      "Epoch: 1/1 - Step: 1696 - Loss: 6.685511112213135\n",
      "Epoch: 1/1 - Step: 1697 - Loss: 6.833934783935547\n",
      "Epoch: 1/1 - Step: 1698 - Loss: 6.693840503692627\n",
      "Epoch: 1/1 - Step: 1699 - Loss: 6.679500102996826\n",
      "Epoch: 1/1 - Step: 1700 - Loss: 6.736121654510498\n",
      "Epoch: 1/1 - Step: 1701 - Loss: 6.576198577880859\n",
      "Epoch: 1/1 - Step: 1702 - Loss: 6.464869499206543\n",
      "Epoch: 1/1 - Step: 1703 - Loss: 6.5697832107543945\n",
      "Epoch: 1/1 - Step: 1704 - Loss: 6.544322490692139\n",
      "Epoch: 1/1 - Step: 1705 - Loss: 6.527717113494873\n",
      "Epoch: 1/1 - Step: 1706 - Loss: 8.62920093536377\n",
      "Epoch: 1/1 - Step: 1707 - Loss: 9.091800689697266\n",
      "Epoch: 1/1 - Step: 1708 - Loss: 9.1239652633667\n",
      "Epoch: 1/1 - Step: 1709 - Loss: 6.966671466827393\n",
      "Epoch: 1/1 - Step: 1710 - Loss: 6.735814094543457\n",
      "Epoch: 1/1 - Step: 1711 - Loss: 6.680300712585449\n",
      "Epoch: 1/1 - Step: 1712 - Loss: 6.740790367126465\n",
      "Epoch: 1/1 - Step: 1713 - Loss: 6.8024702072143555\n",
      "Epoch: 1/1 - Step: 1714 - Loss: 6.772151947021484\n",
      "Epoch: 1/1 - Step: 1715 - Loss: 6.851619720458984\n",
      "Epoch: 1/1 - Step: 1716 - Loss: 6.536414623260498\n",
      "Epoch: 1/1 - Step: 1717 - Loss: 6.3831706047058105\n",
      "Epoch: 1/1 - Step: 1718 - Loss: 6.580780029296875\n",
      "Epoch: 1/1 - Step: 1719 - Loss: 6.708963871002197\n",
      "Epoch: 1/1 - Step: 1720 - Loss: 6.6909918785095215\n",
      "Epoch: 1/1 - Step: 1721 - Loss: 6.690913200378418\n",
      "Epoch: 1/1 - Step: 1722 - Loss: 6.791320323944092\n",
      "Epoch: 1/1 - Step: 1723 - Loss: 6.81207799911499\n",
      "Epoch: 1/1 - Step: 1724 - Loss: 6.772949695587158\n",
      "Epoch: 1/1 - Step: 1725 - Loss: 6.792992115020752\n",
      "Epoch: 1/1 - Step: 1726 - Loss: 6.792265892028809\n",
      "Epoch: 1/1 - Step: 1727 - Loss: 6.7803263664245605\n",
      "Epoch: 1/1 - Step: 1728 - Loss: 6.789222717285156\n",
      "Epoch: 1/1 - Step: 1729 - Loss: 6.805222988128662\n",
      "Epoch: 1/1 - Step: 1730 - Loss: 6.7619171142578125\n",
      "Epoch: 1/1 - Step: 1731 - Loss: 6.657361030578613\n",
      "Epoch: 1/1 - Step: 1732 - Loss: 6.692117691040039\n",
      "Epoch: 1/1 - Step: 1733 - Loss: 8.16875171661377\n",
      "Epoch: 1/1 - Step: 1734 - Loss: 6.678084850311279\n",
      "Epoch: 1/1 - Step: 1735 - Loss: 6.7399373054504395\n",
      "Epoch: 1/1 - Step: 1736 - Loss: 6.6660895347595215\n",
      "Epoch: 1/1 - Step: 1737 - Loss: 6.681643009185791\n",
      "Epoch: 1/1 - Step: 1738 - Loss: 6.5954909324646\n",
      "Epoch: 1/1 - Step: 1739 - Loss: 6.651856899261475\n",
      "Epoch: 1/1 - Step: 1740 - Loss: 6.714507579803467\n",
      "Epoch: 1/1 - Step: 1741 - Loss: 6.68958854675293\n",
      "Epoch: 1/1 - Step: 1742 - Loss: 6.635239601135254\n",
      "Epoch: 1/1 - Step: 1743 - Loss: 6.716087818145752\n",
      "Epoch: 1/1 - Step: 1744 - Loss: 6.6511125564575195\n",
      "Epoch: 1/1 - Step: 1745 - Loss: 6.875364303588867\n",
      "Epoch: 1/1 - Step: 1746 - Loss: 6.810887813568115\n",
      "Epoch: 1/1 - Step: 1747 - Loss: 6.5938568115234375\n",
      "Epoch: 1/1 - Step: 1748 - Loss: 6.663178443908691\n",
      "Epoch: 1/1 - Step: 1749 - Loss: 6.574015140533447\n",
      "Epoch: 1/1 - Step: 1750 - Loss: 6.64565896987915\n",
      "Epoch: 1/1 - Step: 1751 - Loss: 6.638300895690918\n",
      "Epoch: 1/1 - Step: 1752 - Loss: 6.6719818115234375\n",
      "Epoch: 1/1 - Step: 1753 - Loss: 6.69049596786499\n",
      "Epoch: 1/1 - Step: 1754 - Loss: 6.705585956573486\n",
      "Epoch: 1/1 - Step: 1755 - Loss: 6.747215270996094\n",
      "Epoch: 1/1 - Step: 1756 - Loss: 6.665000915527344\n",
      "Epoch: 1/1 - Step: 1757 - Loss: 6.65355920791626\n",
      "Epoch: 1/1 - Step: 1758 - Loss: 6.715951442718506\n",
      "Epoch: 1/1 - Step: 1759 - Loss: 6.684316635131836\n",
      "Epoch: 1/1 - Step: 1760 - Loss: 6.7146711349487305\n",
      "Epoch: 1/1 - Step: 1761 - Loss: 6.649714946746826\n",
      "Epoch: 1/1 - Step: 1762 - Loss: 6.785436630249023\n",
      "Epoch: 1/1 - Step: 1763 - Loss: 6.918400764465332\n",
      "Epoch: 1/1 - Step: 1764 - Loss: 6.585582256317139\n",
      "Epoch: 1/1 - Step: 1765 - Loss: 6.469644069671631\n",
      "Epoch: 1/1 - Step: 1766 - Loss: 6.4803032875061035\n",
      "Epoch: 1/1 - Step: 1767 - Loss: 6.629640579223633\n",
      "Epoch: 1/1 - Step: 1768 - Loss: 6.461759090423584\n",
      "Epoch: 1/1 - Step: 1769 - Loss: 6.671728610992432\n",
      "Epoch: 1/1 - Step: 1770 - Loss: 6.584963798522949\n",
      "Epoch: 1/1 - Step: 1771 - Loss: 6.67231559753418\n",
      "Epoch: 1/1 - Step: 1772 - Loss: 6.679518699645996\n",
      "Epoch: 1/1 - Step: 1773 - Loss: 6.7909698486328125\n",
      "Epoch: 1/1 - Step: 1774 - Loss: 6.871114730834961\n",
      "Epoch: 1/1 - Step: 1775 - Loss: 6.699664115905762\n",
      "Epoch: 1/1 - Step: 1776 - Loss: 6.371379375457764\n",
      "Epoch: 1/1 - Step: 1777 - Loss: 6.43802547454834\n",
      "Epoch: 1/1 - Step: 1778 - Loss: 6.332730770111084\n",
      "Epoch: 1/1 - Step: 1779 - Loss: 6.42595100402832\n",
      "Epoch: 1/1 - Step: 1780 - Loss: 6.586492538452148\n",
      "Epoch: 1/1 - Step: 1781 - Loss: 6.747788429260254\n",
      "Epoch: 1/1 - Step: 1782 - Loss: 6.809504985809326\n",
      "Epoch: 1/1 - Step: 1783 - Loss: 6.811129570007324\n",
      "Epoch: 1/1 - Step: 1784 - Loss: 6.8048319816589355\n",
      "Epoch: 1/1 - Step: 1785 - Loss: 6.566093921661377\n",
      "Epoch: 1/1 - Step: 1786 - Loss: 6.710571765899658\n",
      "Epoch: 1/1 - Step: 1787 - Loss: 6.782811641693115\n",
      "Epoch: 1/1 - Step: 1788 - Loss: 6.807413578033447\n",
      "Epoch: 1/1 - Step: 1789 - Loss: 6.770422458648682\n",
      "Epoch: 1/1 - Step: 1790 - Loss: 6.623228549957275\n",
      "Epoch: 1/1 - Step: 1791 - Loss: 6.642396450042725\n",
      "Epoch: 1/1 - Step: 1792 - Loss: 6.709770202636719\n",
      "Epoch: 1/1 - Step: 1793 - Loss: 6.762205600738525\n",
      "Epoch: 1/1 - Step: 1794 - Loss: 6.807999610900879\n",
      "Epoch: 1/1 - Step: 1795 - Loss: 6.5980987548828125\n",
      "Epoch: 1/1 - Step: 1796 - Loss: 6.719615459442139\n",
      "Epoch: 1/1 - Step: 1797 - Loss: 6.585301876068115\n",
      "Epoch: 1/1 - Step: 1798 - Loss: 6.674083232879639\n",
      "Epoch: 1/1 - Step: 1799 - Loss: 6.553967475891113\n",
      "Epoch: 1/1 - Step: 1800 - Loss: 6.6953349113464355\n",
      "Epoch: 1/1 - Step: 1801 - Loss: 6.734872817993164\n",
      "Epoch: 1/1 - Step: 1802 - Loss: 6.764692306518555\n",
      "Epoch: 1/1 - Step: 1803 - Loss: 6.63643741607666\n",
      "Epoch: 1/1 - Step: 1804 - Loss: 7.278229713439941\n",
      "Epoch: 1/1 - Step: 1805 - Loss: 8.806662559509277\n",
      "Epoch: 1/1 - Step: 1806 - Loss: 9.132850646972656\n",
      "Epoch: 1/1 - Step: 1807 - Loss: 7.7566237449646\n",
      "Epoch: 1/1 - Step: 1808 - Loss: 6.72651481628418\n",
      "Epoch: 1/1 - Step: 1809 - Loss: 6.629055976867676\n",
      "Epoch: 1/1 - Step: 1810 - Loss: 6.6300435066223145\n",
      "Epoch: 1/1 - Step: 1811 - Loss: 6.725254058837891\n",
      "Epoch: 1/1 - Step: 1812 - Loss: 6.740599155426025\n",
      "Epoch: 1/1 - Step: 1813 - Loss: 6.678497791290283\n",
      "Epoch: 1/1 - Step: 1814 - Loss: 6.690027236938477\n",
      "Epoch: 1/1 - Step: 1815 - Loss: 6.7512078285217285\n",
      "Epoch: 1/1 - Step: 1816 - Loss: 6.71362829208374\n",
      "Epoch: 1/1 - Step: 1817 - Loss: 6.673003673553467\n",
      "Epoch: 1/1 - Step: 1818 - Loss: 6.673680305480957\n",
      "Epoch: 1/1 - Step: 1819 - Loss: 6.715332508087158\n",
      "Epoch: 1/1 - Step: 1820 - Loss: 6.737066268920898\n",
      "Epoch: 1/1 - Step: 1821 - Loss: 6.5372467041015625\n",
      "Epoch: 1/1 - Step: 1822 - Loss: 6.780825614929199\n",
      "Epoch: 1/1 - Step: 1823 - Loss: 6.726136207580566\n",
      "Epoch: 1/1 - Step: 1824 - Loss: 6.712579250335693\n",
      "Epoch: 1/1 - Step: 1825 - Loss: 6.617795944213867\n",
      "Epoch: 1/1 - Step: 1826 - Loss: 6.672959804534912\n",
      "Epoch: 1/1 - Step: 1827 - Loss: 6.647653102874756\n",
      "Epoch: 1/1 - Step: 1828 - Loss: 6.71409273147583\n",
      "Epoch: 1/1 - Step: 1829 - Loss: 6.697963237762451\n",
      "Epoch: 1/1 - Step: 1830 - Loss: 6.67365026473999\n",
      "Epoch: 1/1 - Step: 1831 - Loss: 6.675801753997803\n",
      "Epoch: 1/1 - Step: 1832 - Loss: 6.641254425048828\n",
      "Epoch: 1/1 - Step: 1833 - Loss: 6.7098588943481445\n",
      "Epoch: 1/1 - Step: 1834 - Loss: 6.61829137802124\n",
      "Epoch: 1/1 - Step: 1835 - Loss: 6.845125675201416\n",
      "Epoch: 1/1 - Step: 1836 - Loss: 6.7124247550964355\n",
      "Epoch: 1/1 - Step: 1837 - Loss: 6.684893608093262\n",
      "Epoch: 1/1 - Step: 1838 - Loss: 6.6063642501831055\n",
      "Epoch: 1/1 - Step: 1839 - Loss: 6.5329108238220215\n",
      "Epoch: 1/1 - Step: 1840 - Loss: 6.541841506958008\n",
      "Epoch: 1/1 - Step: 1841 - Loss: 6.593244552612305\n",
      "Epoch: 1/1 - Step: 1842 - Loss: 6.487725734710693\n",
      "Epoch: 1/1 - Step: 1843 - Loss: 6.520448684692383\n",
      "Epoch: 1/1 - Step: 1844 - Loss: 6.600705623626709\n",
      "Epoch: 1/1 - Step: 1845 - Loss: 6.616509914398193\n",
      "Epoch: 1/1 - Step: 1846 - Loss: 6.5359954833984375\n",
      "Epoch: 1/1 - Step: 1847 - Loss: 6.514034748077393\n",
      "Epoch: 1/1 - Step: 1848 - Loss: 6.522945404052734\n",
      "Epoch: 1/1 - Step: 1849 - Loss: 6.611266613006592\n",
      "Epoch: 1/1 - Step: 1850 - Loss: 6.742443084716797\n",
      "Epoch: 1/1 - Step: 1851 - Loss: 6.785646915435791\n",
      "Epoch: 1/1 - Step: 1852 - Loss: 6.612674236297607\n",
      "Epoch: 1/1 - Step: 1853 - Loss: 6.6528496742248535\n",
      "Epoch: 1/1 - Step: 1854 - Loss: 6.650180339813232\n",
      "Epoch: 1/1 - Step: 1855 - Loss: 6.745640754699707\n",
      "Epoch: 1/1 - Step: 1856 - Loss: 6.588197708129883\n",
      "Epoch: 1/1 - Step: 1857 - Loss: 6.5824503898620605\n",
      "Epoch: 1/1 - Step: 1858 - Loss: 6.521037578582764\n",
      "Epoch: 1/1 - Step: 1859 - Loss: 6.63017463684082\n",
      "Epoch: 1/1 - Step: 1860 - Loss: 6.620204925537109\n",
      "Epoch: 1/1 - Step: 1861 - Loss: 6.594079971313477\n",
      "Epoch: 1/1 - Step: 1862 - Loss: 6.53641414642334\n",
      "Epoch: 1/1 - Step: 1863 - Loss: 6.669637680053711\n",
      "Epoch: 1/1 - Step: 1864 - Loss: 6.7418742179870605\n",
      "Epoch: 1/1 - Step: 1865 - Loss: 6.544286251068115\n",
      "Epoch: 1/1 - Step: 1866 - Loss: 6.537994384765625\n",
      "Epoch: 1/1 - Step: 1867 - Loss: 6.583421230316162\n",
      "Epoch: 1/1 - Step: 1868 - Loss: 6.586643218994141\n",
      "Epoch: 1/1 - Step: 1869 - Loss: 6.653234481811523\n",
      "Epoch: 1/1 - Step: 1870 - Loss: 6.731200218200684\n",
      "Epoch: 1/1 - Step: 1871 - Loss: 6.481760025024414\n",
      "Epoch: 1/1 - Step: 1872 - Loss: 6.767624378204346\n",
      "Epoch: 1/1 - Step: 1873 - Loss: 6.493553161621094\n",
      "Epoch: 1/1 - Step: 1874 - Loss: 6.654483795166016\n",
      "Epoch: 1/1 - Step: 1875 - Loss: 6.530451774597168\n",
      "Epoch: 1/1 - Step: 1876 - Loss: 6.434401512145996\n",
      "Epoch: 1/1 - Step: 1877 - Loss: 6.605536937713623\n",
      "Epoch: 1/1 - Step: 1878 - Loss: 6.6042399406433105\n",
      "Epoch: 1/1 - Step: 1879 - Loss: 6.60808801651001\n",
      "Epoch: 1/1 - Step: 1880 - Loss: 6.5231709480285645\n",
      "Epoch: 1/1 - Step: 1881 - Loss: 6.538583278656006\n",
      "Epoch: 1/1 - Step: 1882 - Loss: 6.564296722412109\n",
      "Epoch: 1/1 - Step: 1883 - Loss: 6.798288822174072\n",
      "Epoch: 1/1 - Step: 1884 - Loss: 6.735439777374268\n",
      "Epoch: 1/1 - Step: 1885 - Loss: 6.8863372802734375\n",
      "Epoch: 1/1 - Step: 1886 - Loss: 6.785965442657471\n",
      "Epoch: 1/1 - Step: 1887 - Loss: 6.591163158416748\n",
      "Epoch: 1/1 - Step: 1888 - Loss: 6.638725757598877\n",
      "Epoch: 1/1 - Step: 1889 - Loss: 6.480846881866455\n",
      "Epoch: 1/1 - Step: 1890 - Loss: 6.455373764038086\n",
      "Epoch: 1/1 - Step: 1891 - Loss: 6.463161468505859\n",
      "Epoch: 1/1 - Step: 1892 - Loss: 6.639131546020508\n",
      "Epoch: 1/1 - Step: 1893 - Loss: 6.430364608764648\n",
      "Epoch: 1/1 - Step: 1894 - Loss: 6.442113399505615\n",
      "Epoch: 1/1 - Step: 1895 - Loss: 6.533632278442383\n",
      "Epoch: 1/1 - Step: 1896 - Loss: 6.7989044189453125\n",
      "Epoch: 1/1 - Step: 1897 - Loss: 6.453939914703369\n",
      "Epoch: 1/1 - Step: 1898 - Loss: 6.561490058898926\n",
      "Epoch: 1/1 - Step: 1899 - Loss: 6.565910816192627\n",
      "Epoch: 1/1 - Step: 1900 - Loss: 6.614015579223633\n",
      "Epoch: 1/1 - Step: 1901 - Loss: 6.5559282302856445\n",
      "Epoch: 1/1 - Step: 1902 - Loss: 6.51624059677124\n",
      "Epoch: 1/1 - Step: 1903 - Loss: 6.5807576179504395\n",
      "Epoch: 1/1 - Step: 1904 - Loss: 6.54063606262207\n",
      "Epoch: 1/1 - Step: 1905 - Loss: 6.562806606292725\n",
      "Epoch: 1/1 - Step: 1906 - Loss: 6.551415920257568\n",
      "Epoch: 1/1 - Step: 1907 - Loss: 6.48168420791626\n",
      "Epoch: 1/1 - Step: 1908 - Loss: 6.6383538246154785\n",
      "Epoch: 1/1 - Step: 1909 - Loss: 6.67586612701416\n",
      "Epoch: 1/1 - Step: 1910 - Loss: 6.556471347808838\n",
      "Epoch: 1/1 - Step: 1911 - Loss: 6.583576679229736\n",
      "Epoch: 1/1 - Step: 1912 - Loss: 6.544031143188477\n",
      "Epoch: 1/1 - Step: 1913 - Loss: 6.543637275695801\n",
      "Epoch: 1/1 - Step: 1914 - Loss: 6.474627494812012\n",
      "Epoch: 1/1 - Step: 1915 - Loss: 6.47786283493042\n",
      "Epoch: 1/1 - Step: 1916 - Loss: 6.552854061126709\n",
      "Epoch: 1/1 - Step: 1917 - Loss: 6.473227500915527\n",
      "Epoch: 1/1 - Step: 1918 - Loss: 6.483149528503418\n",
      "Epoch: 1/1 - Step: 1919 - Loss: 6.5391950607299805\n",
      "Epoch: 1/1 - Step: 1920 - Loss: 6.426305294036865\n",
      "Epoch: 1/1 - Step: 1921 - Loss: 6.391729831695557\n",
      "Epoch: 1/1 - Step: 1922 - Loss: 6.535351753234863\n",
      "Epoch: 1/1 - Step: 1923 - Loss: 6.530645847320557\n",
      "Epoch: 1/1 - Step: 1924 - Loss: 6.675354480743408\n",
      "Epoch: 1/1 - Step: 1925 - Loss: 6.733591079711914\n",
      "Epoch: 1/1 - Step: 1926 - Loss: 6.523247718811035\n",
      "Epoch: 1/1 - Step: 1927 - Loss: 6.486114025115967\n",
      "Epoch: 1/1 - Step: 1928 - Loss: 6.547338485717773\n",
      "Epoch: 1/1 - Step: 1929 - Loss: 6.565869331359863\n",
      "Epoch: 1/1 - Step: 1930 - Loss: 6.576381683349609\n",
      "Epoch: 1/1 - Step: 1931 - Loss: 6.459491729736328\n",
      "Epoch: 1/1 - Step: 1932 - Loss: 6.545364856719971\n",
      "Epoch: 1/1 - Step: 1933 - Loss: 6.554482936859131\n",
      "Epoch: 1/1 - Step: 1934 - Loss: 6.566635608673096\n",
      "Epoch: 1/1 - Step: 1935 - Loss: 6.4616899490356445\n",
      "Epoch: 1/1 - Step: 1936 - Loss: 6.489264488220215\n",
      "Epoch: 1/1 - Step: 1937 - Loss: 8.440058708190918\n",
      "Epoch: 1/1 - Step: 1938 - Loss: 7.971364498138428\n",
      "Epoch: 1/1 - Step: 1939 - Loss: 6.61379861831665\n",
      "Epoch: 1/1 - Step: 1940 - Loss: 6.431382656097412\n",
      "Epoch: 1/1 - Step: 1941 - Loss: 6.413132667541504\n",
      "Epoch: 1/1 - Step: 1942 - Loss: 6.539551258087158\n",
      "Epoch: 1/1 - Step: 1943 - Loss: 6.735251426696777\n",
      "Epoch: 1/1 - Step: 1944 - Loss: 6.508701324462891\n",
      "Epoch: 1/1 - Step: 1945 - Loss: 6.530149459838867\n",
      "Epoch: 1/1 - Step: 1946 - Loss: 6.615586757659912\n",
      "Epoch: 1/1 - Step: 1947 - Loss: 6.713271617889404\n",
      "Epoch: 1/1 - Step: 1948 - Loss: 6.776768684387207\n",
      "Epoch: 1/1 - Step: 1949 - Loss: 6.633384704589844\n",
      "Epoch: 1/1 - Step: 1950 - Loss: 6.47451639175415\n",
      "Epoch: 1/1 - Step: 1951 - Loss: 6.446685314178467\n",
      "Epoch: 1/1 - Step: 1952 - Loss: 6.594385623931885\n",
      "Epoch: 1/1 - Step: 1953 - Loss: 6.7181396484375\n",
      "Epoch: 1/1 - Step: 1954 - Loss: 6.5547380447387695\n",
      "Epoch: 1/1 - Step: 1955 - Loss: 6.437992572784424\n",
      "Epoch: 1/1 - Step: 1956 - Loss: 6.2852783203125\n",
      "Epoch: 1/1 - Step: 1957 - Loss: 6.4037885665893555\n",
      "Epoch: 1/1 - Step: 1958 - Loss: 6.7271599769592285\n",
      "Epoch: 1/1 - Step: 1959 - Loss: 6.440262794494629\n",
      "Epoch: 1/1 - Step: 1960 - Loss: 6.473679065704346\n",
      "Epoch: 1/1 - Step: 1961 - Loss: 6.623147964477539\n",
      "Epoch: 1/1 - Step: 1962 - Loss: 6.540721893310547\n",
      "Epoch: 1/1 - Step: 1963 - Loss: 6.504462242126465\n",
      "Epoch: 1/1 - Step: 1964 - Loss: 6.570705413818359\n",
      "Epoch: 1/1 - Step: 1965 - Loss: 6.568567752838135\n",
      "Epoch: 1/1 - Step: 1966 - Loss: 6.469521522521973\n",
      "Epoch: 1/1 - Step: 1967 - Loss: 6.535195350646973\n",
      "Epoch: 1/1 - Step: 1968 - Loss: 6.50553560256958\n",
      "Epoch: 1/1 - Step: 1969 - Loss: 6.482549667358398\n",
      "Epoch: 1/1 - Step: 1970 - Loss: 6.5982584953308105\n",
      "Epoch: 1/1 - Step: 1971 - Loss: 6.526782989501953\n",
      "Epoch: 1/1 - Step: 1972 - Loss: 6.773970603942871\n",
      "Epoch: 1/1 - Step: 1973 - Loss: 6.432876110076904\n",
      "Epoch: 1/1 - Step: 1974 - Loss: 6.458301544189453\n",
      "Epoch: 1/1 - Step: 1975 - Loss: 6.540036678314209\n",
      "Epoch: 1/1 - Step: 1976 - Loss: 6.531986713409424\n",
      "Epoch: 1/1 - Step: 1977 - Loss: 6.4988694190979\n",
      "Epoch: 1/1 - Step: 1978 - Loss: 6.5557098388671875\n",
      "Epoch: 1/1 - Step: 1979 - Loss: 6.403690338134766\n",
      "Epoch: 1/1 - Step: 1980 - Loss: 6.553267955780029\n",
      "Epoch: 1/1 - Step: 1981 - Loss: 6.628746509552002\n",
      "Epoch: 1/1 - Step: 1982 - Loss: 6.41632604598999\n",
      "Epoch: 1/1 - Step: 1983 - Loss: 6.371025562286377\n",
      "Epoch: 1/1 - Step: 1984 - Loss: 6.42087459564209\n",
      "Epoch: 1/1 - Step: 1985 - Loss: 6.501712799072266\n",
      "Epoch: 1/1 - Step: 1986 - Loss: 6.412050724029541\n",
      "Epoch: 1/1 - Step: 1987 - Loss: 8.470722198486328\n",
      "Epoch: 1/1 - Step: 1988 - Loss: 7.811944484710693\n",
      "Epoch: 1/1 - Step: 1989 - Loss: 6.687750339508057\n",
      "Epoch: 1/1 - Step: 1990 - Loss: 6.59600305557251\n",
      "Epoch: 1/1 - Step: 1991 - Loss: 6.587626934051514\n",
      "Epoch: 1/1 - Step: 1992 - Loss: 6.501591682434082\n",
      "Epoch: 1/1 - Step: 1993 - Loss: 6.538903713226318\n",
      "Epoch: 1/1 - Step: 1994 - Loss: 6.514159679412842\n",
      "Epoch: 1/1 - Step: 1995 - Loss: 6.719719409942627\n",
      "Epoch: 1/1 - Step: 1996 - Loss: 6.376211643218994\n",
      "Epoch: 1/1 - Step: 1997 - Loss: 6.665188789367676\n",
      "Epoch: 1/1 - Step: 1998 - Loss: 6.597433090209961\n",
      "Epoch: 1/1 - Step: 1999 - Loss: 6.6144819259643555\n",
      "Epoch: 1/1 - Step: 2000 - Loss: 6.417191982269287\n",
      "Epoch: 1/1 - Step: 2001 - Loss: 6.372366905212402\n",
      "Epoch: 1/1 - Step: 2002 - Loss: 6.511760711669922\n",
      "Epoch: 1/1 - Step: 2003 - Loss: 6.4569411277771\n",
      "Epoch: 1/1 - Step: 2004 - Loss: 6.607396602630615\n",
      "Epoch: 1/1 - Step: 2005 - Loss: 6.484133720397949\n",
      "Epoch: 1/1 - Step: 2006 - Loss: 6.533433437347412\n",
      "Epoch: 1/1 - Step: 2007 - Loss: 6.48730993270874\n",
      "Epoch: 1/1 - Step: 2008 - Loss: 6.4442925453186035\n",
      "Epoch: 1/1 - Step: 2009 - Loss: 6.419783115386963\n",
      "Epoch: 1/1 - Step: 2010 - Loss: 6.571060657501221\n",
      "Epoch: 1/1 - Step: 2011 - Loss: 6.4654927253723145\n",
      "Epoch: 1/1 - Step: 2012 - Loss: 6.3748955726623535\n",
      "Epoch: 1/1 - Step: 2013 - Loss: 6.642498016357422\n",
      "Epoch: 1/1 - Step: 2014 - Loss: 6.579748153686523\n",
      "Epoch: 1/1 - Step: 2015 - Loss: 6.381870746612549\n",
      "Epoch: 1/1 - Step: 2016 - Loss: 6.415070056915283\n",
      "Epoch: 1/1 - Step: 2017 - Loss: 6.6616926193237305\n",
      "Epoch: 1/1 - Step: 2018 - Loss: 6.449412822723389\n",
      "Epoch: 1/1 - Step: 2019 - Loss: 6.462255477905273\n",
      "Epoch: 1/1 - Step: 2020 - Loss: 6.450571537017822\n",
      "Epoch: 1/1 - Step: 2021 - Loss: 6.5006422996521\n",
      "Epoch: 1/1 - Step: 2022 - Loss: 8.340754508972168\n",
      "Epoch: 1/1 - Step: 2023 - Loss: 7.3729567527771\n",
      "Epoch: 1/1 - Step: 2024 - Loss: 6.524015426635742\n",
      "Epoch: 1/1 - Step: 2025 - Loss: 6.456592559814453\n",
      "Epoch: 1/1 - Step: 2026 - Loss: 6.692756652832031\n",
      "Epoch: 1/1 - Step: 2027 - Loss: 6.7195892333984375\n",
      "Epoch: 1/1 - Step: 2028 - Loss: 6.605208873748779\n",
      "Epoch: 1/1 - Step: 2029 - Loss: 6.599979877471924\n",
      "Epoch: 1/1 - Step: 2030 - Loss: 6.656884670257568\n",
      "Epoch: 1/1 - Step: 2031 - Loss: 6.81856107711792\n",
      "Epoch: 1/1 - Step: 2032 - Loss: 6.713963985443115\n",
      "Epoch: 1/1 - Step: 2033 - Loss: 6.587385177612305\n",
      "Epoch: 1/1 - Step: 2034 - Loss: 6.699166774749756\n",
      "Epoch: 1/1 - Step: 2035 - Loss: 6.80596399307251\n",
      "Epoch: 1/1 - Step: 2036 - Loss: 6.630824565887451\n",
      "Epoch: 1/1 - Step: 2037 - Loss: 6.63504695892334\n",
      "Epoch: 1/1 - Step: 2038 - Loss: 6.718243598937988\n",
      "Epoch: 1/1 - Step: 2039 - Loss: 6.724590301513672\n",
      "Epoch: 1/1 - Step: 2040 - Loss: 6.653903484344482\n",
      "Epoch: 1/1 - Step: 2041 - Loss: 6.702075481414795\n",
      "Epoch: 1/1 - Step: 2042 - Loss: 6.589147090911865\n",
      "Epoch: 1/1 - Step: 2043 - Loss: 6.493998050689697\n",
      "Epoch: 1/1 - Step: 2044 - Loss: 6.411105155944824\n",
      "Epoch: 1/1 - Step: 2045 - Loss: 6.461061000823975\n",
      "Epoch: 1/1 - Step: 2046 - Loss: 6.486871719360352\n",
      "Epoch: 1/1 - Step: 2047 - Loss: 6.638567924499512\n",
      "Epoch: 1/1 - Step: 2048 - Loss: 8.933571815490723\n",
      "Epoch: 1/1 - Step: 2049 - Loss: 9.043774604797363\n",
      "Epoch: 1/1 - Step: 2050 - Loss: 8.805349349975586\n",
      "Epoch: 1/1 - Step: 2051 - Loss: 6.729796886444092\n",
      "Epoch: 1/1 - Step: 2052 - Loss: 6.639524459838867\n",
      "Epoch: 1/1 - Step: 2053 - Loss: 6.607180118560791\n",
      "Epoch: 1/1 - Step: 2054 - Loss: 6.745079040527344\n",
      "Epoch: 1/1 - Step: 2055 - Loss: 6.708629608154297\n",
      "Epoch: 1/1 - Step: 2056 - Loss: 6.73870325088501\n",
      "Epoch: 1/1 - Step: 2057 - Loss: 6.722822189331055\n",
      "Epoch: 1/1 - Step: 2058 - Loss: 6.444919109344482\n",
      "Epoch: 1/1 - Step: 2059 - Loss: 6.327293872833252\n",
      "Epoch: 1/1 - Step: 2060 - Loss: 6.548883438110352\n",
      "Epoch: 1/1 - Step: 2061 - Loss: 6.608320713043213\n",
      "Epoch: 1/1 - Step: 2062 - Loss: 6.738813877105713\n",
      "Epoch: 1/1 - Step: 2063 - Loss: 6.557563304901123\n",
      "Epoch: 1/1 - Step: 2064 - Loss: 6.743157386779785\n",
      "Epoch: 1/1 - Step: 2065 - Loss: 6.763800144195557\n",
      "Epoch: 1/1 - Step: 2066 - Loss: 6.7393999099731445\n",
      "Epoch: 1/1 - Step: 2067 - Loss: 6.703050136566162\n",
      "Epoch: 1/1 - Step: 2068 - Loss: 6.76281213760376\n",
      "Epoch: 1/1 - Step: 2069 - Loss: 6.7011542320251465\n",
      "Epoch: 1/1 - Step: 2070 - Loss: 6.710930347442627\n",
      "Epoch: 1/1 - Step: 2071 - Loss: 6.711424350738525\n",
      "Epoch: 1/1 - Step: 2072 - Loss: 6.708120822906494\n",
      "Epoch: 1/1 - Step: 2073 - Loss: 6.576659202575684\n",
      "Epoch: 1/1 - Step: 2074 - Loss: 6.789484024047852\n",
      "Epoch: 1/1 - Step: 2075 - Loss: 8.013211250305176\n",
      "Epoch: 1/1 - Step: 2076 - Loss: 6.571797847747803\n",
      "Epoch: 1/1 - Step: 2077 - Loss: 6.669252872467041\n",
      "Epoch: 1/1 - Step: 2078 - Loss: 6.602414608001709\n",
      "Epoch: 1/1 - Step: 2079 - Loss: 6.65092134475708\n",
      "Epoch: 1/1 - Step: 2080 - Loss: 6.514218330383301\n",
      "Epoch: 1/1 - Step: 2081 - Loss: 6.577316761016846\n",
      "Epoch: 1/1 - Step: 2082 - Loss: 6.6839752197265625\n",
      "Epoch: 1/1 - Step: 2083 - Loss: 6.58673620223999\n",
      "Epoch: 1/1 - Step: 2084 - Loss: 6.658329010009766\n",
      "Epoch: 1/1 - Step: 2085 - Loss: 6.6070709228515625\n",
      "Epoch: 1/1 - Step: 2086 - Loss: 6.610433578491211\n",
      "Epoch: 1/1 - Step: 2087 - Loss: 6.857810974121094\n",
      "Epoch: 1/1 - Step: 2088 - Loss: 6.686132431030273\n",
      "Epoch: 1/1 - Step: 2089 - Loss: 6.530169486999512\n",
      "Epoch: 1/1 - Step: 2090 - Loss: 6.607518196105957\n",
      "Epoch: 1/1 - Step: 2091 - Loss: 6.530979156494141\n",
      "Epoch: 1/1 - Step: 2092 - Loss: 6.605978012084961\n",
      "Epoch: 1/1 - Step: 2093 - Loss: 6.559563159942627\n",
      "Epoch: 1/1 - Step: 2094 - Loss: 6.617424011230469\n",
      "Epoch: 1/1 - Step: 2095 - Loss: 6.655350685119629\n",
      "Epoch: 1/1 - Step: 2096 - Loss: 6.60911226272583\n",
      "Epoch: 1/1 - Step: 2097 - Loss: 6.702169895172119\n",
      "Epoch: 1/1 - Step: 2098 - Loss: 6.610475540161133\n",
      "Epoch: 1/1 - Step: 2099 - Loss: 6.624805450439453\n",
      "Epoch: 1/1 - Step: 2100 - Loss: 6.6030120849609375\n",
      "Epoch: 1/1 - Step: 2101 - Loss: 6.632429599761963\n",
      "Epoch: 1/1 - Step: 2102 - Loss: 6.677128314971924\n",
      "Epoch: 1/1 - Step: 2103 - Loss: 6.576983451843262\n",
      "Epoch: 1/1 - Step: 2104 - Loss: 6.722557067871094\n",
      "Epoch: 1/1 - Step: 2105 - Loss: 6.877508163452148\n",
      "Epoch: 1/1 - Step: 2106 - Loss: 6.447934627532959\n",
      "Epoch: 1/1 - Step: 2107 - Loss: 6.436277389526367\n",
      "Epoch: 1/1 - Step: 2108 - Loss: 6.3988776206970215\n",
      "Epoch: 1/1 - Step: 2109 - Loss: 6.5211358070373535\n",
      "Epoch: 1/1 - Step: 2110 - Loss: 6.446507930755615\n",
      "Epoch: 1/1 - Step: 2111 - Loss: 6.615307807922363\n",
      "Epoch: 1/1 - Step: 2112 - Loss: 6.559383392333984\n",
      "Epoch: 1/1 - Step: 2113 - Loss: 6.603183269500732\n",
      "Epoch: 1/1 - Step: 2114 - Loss: 6.611562252044678\n",
      "Epoch: 1/1 - Step: 2115 - Loss: 6.790858268737793\n",
      "Epoch: 1/1 - Step: 2116 - Loss: 6.756757736206055\n",
      "Epoch: 1/1 - Step: 2117 - Loss: 6.6128926277160645\n",
      "Epoch: 1/1 - Step: 2118 - Loss: 6.290978908538818\n",
      "Epoch: 1/1 - Step: 2119 - Loss: 6.361141204833984\n",
      "Epoch: 1/1 - Step: 2120 - Loss: 6.2756733894348145\n",
      "Epoch: 1/1 - Step: 2121 - Loss: 6.452093124389648\n",
      "Epoch: 1/1 - Step: 2122 - Loss: 6.507407188415527\n",
      "Epoch: 1/1 - Step: 2123 - Loss: 6.770557403564453\n",
      "Epoch: 1/1 - Step: 2124 - Loss: 6.642333030700684\n",
      "Epoch: 1/1 - Step: 2125 - Loss: 6.824862480163574\n",
      "Epoch: 1/1 - Step: 2126 - Loss: 6.683653354644775\n",
      "Epoch: 1/1 - Step: 2127 - Loss: 6.5453200340271\n",
      "Epoch: 1/1 - Step: 2128 - Loss: 6.664646148681641\n",
      "Epoch: 1/1 - Step: 2129 - Loss: 6.731198310852051\n",
      "Epoch: 1/1 - Step: 2130 - Loss: 6.760862350463867\n",
      "Epoch: 1/1 - Step: 2131 - Loss: 6.705085754394531\n",
      "Epoch: 1/1 - Step: 2132 - Loss: 6.562410354614258\n",
      "Epoch: 1/1 - Step: 2133 - Loss: 6.583202362060547\n",
      "Epoch: 1/1 - Step: 2134 - Loss: 6.64570951461792\n",
      "Epoch: 1/1 - Step: 2135 - Loss: 6.74776029586792\n",
      "Epoch: 1/1 - Step: 2136 - Loss: 6.667965412139893\n",
      "Epoch: 1/1 - Step: 2137 - Loss: 6.605410099029541\n",
      "Epoch: 1/1 - Step: 2138 - Loss: 6.683993816375732\n",
      "Epoch: 1/1 - Step: 2139 - Loss: 6.50074577331543\n",
      "Epoch: 1/1 - Step: 2140 - Loss: 6.596649169921875\n",
      "Epoch: 1/1 - Step: 2141 - Loss: 6.575167179107666\n",
      "Epoch: 1/1 - Step: 2142 - Loss: 6.636261463165283\n",
      "Epoch: 1/1 - Step: 2143 - Loss: 6.646766662597656\n",
      "Epoch: 1/1 - Step: 2144 - Loss: 6.754075050354004\n",
      "Epoch: 1/1 - Step: 2145 - Loss: 6.559839725494385\n",
      "Epoch: 1/1 - Step: 2146 - Loss: 7.676840782165527\n",
      "Epoch: 1/1 - Step: 2147 - Loss: 8.80699348449707\n",
      "Epoch: 1/1 - Step: 2148 - Loss: 9.090039253234863\n",
      "Epoch: 1/1 - Step: 2149 - Loss: 7.211399078369141\n",
      "Epoch: 1/1 - Step: 2150 - Loss: 6.636536598205566\n",
      "Epoch: 1/1 - Step: 2151 - Loss: 6.610117435455322\n",
      "Epoch: 1/1 - Step: 2152 - Loss: 6.5530314445495605\n",
      "Epoch: 1/1 - Step: 2153 - Loss: 6.712050437927246\n",
      "Epoch: 1/1 - Step: 2154 - Loss: 6.637217998504639\n",
      "Epoch: 1/1 - Step: 2155 - Loss: 6.638029098510742\n",
      "Epoch: 1/1 - Step: 2156 - Loss: 6.625728130340576\n",
      "Epoch: 1/1 - Step: 2157 - Loss: 6.688838005065918\n",
      "Epoch: 1/1 - Step: 2158 - Loss: 6.646986484527588\n",
      "Epoch: 1/1 - Step: 2159 - Loss: 6.567559719085693\n",
      "Epoch: 1/1 - Step: 2160 - Loss: 6.638820171356201\n",
      "Epoch: 1/1 - Step: 2161 - Loss: 6.6616973876953125\n",
      "Epoch: 1/1 - Step: 2162 - Loss: 6.594651222229004\n",
      "Epoch: 1/1 - Step: 2163 - Loss: 6.571138858795166\n",
      "Epoch: 1/1 - Step: 2164 - Loss: 6.745059967041016\n",
      "Epoch: 1/1 - Step: 2165 - Loss: 6.653748035430908\n",
      "Epoch: 1/1 - Step: 2166 - Loss: 6.67526388168335\n",
      "Epoch: 1/1 - Step: 2167 - Loss: 6.513650417327881\n",
      "Epoch: 1/1 - Step: 2168 - Loss: 6.610417366027832\n",
      "Epoch: 1/1 - Step: 2169 - Loss: 6.638964653015137\n",
      "Epoch: 1/1 - Step: 2170 - Loss: 6.65581750869751\n",
      "Epoch: 1/1 - Step: 2171 - Loss: 6.676076889038086\n",
      "Epoch: 1/1 - Step: 2172 - Loss: 6.5817551612854\n",
      "Epoch: 1/1 - Step: 2173 - Loss: 6.573671340942383\n",
      "Epoch: 1/1 - Step: 2174 - Loss: 6.598862171173096\n",
      "Epoch: 1/1 - Step: 2175 - Loss: 6.647773742675781\n",
      "Epoch: 1/1 - Step: 2176 - Loss: 6.5853729248046875\n",
      "Epoch: 1/1 - Step: 2177 - Loss: 6.785677433013916\n",
      "Epoch: 1/1 - Step: 2178 - Loss: 6.624615669250488\n",
      "Epoch: 1/1 - Step: 2179 - Loss: 6.655786991119385\n",
      "Epoch: 1/1 - Step: 2180 - Loss: 6.5384297370910645\n",
      "Epoch: 1/1 - Step: 2181 - Loss: 6.506473541259766\n",
      "Epoch: 1/1 - Step: 2182 - Loss: 6.495540618896484\n",
      "Epoch: 1/1 - Step: 2183 - Loss: 6.48312520980835\n",
      "Epoch: 1/1 - Step: 2184 - Loss: 6.427923679351807\n",
      "Epoch: 1/1 - Step: 2185 - Loss: 6.485852241516113\n",
      "Epoch: 1/1 - Step: 2186 - Loss: 6.587017059326172\n",
      "Epoch: 1/1 - Step: 2187 - Loss: 6.515734672546387\n",
      "Epoch: 1/1 - Step: 2188 - Loss: 6.430388450622559\n",
      "Epoch: 1/1 - Step: 2189 - Loss: 6.498976230621338\n",
      "Epoch: 1/1 - Step: 2190 - Loss: 6.43006706237793\n",
      "Epoch: 1/1 - Step: 2191 - Loss: 6.637890815734863\n",
      "Epoch: 1/1 - Step: 2192 - Loss: 6.7566704750061035\n",
      "Epoch: 1/1 - Step: 2193 - Loss: 6.66016960144043\n",
      "Epoch: 1/1 - Step: 2194 - Loss: 6.471079349517822\n",
      "Epoch: 1/1 - Step: 2195 - Loss: 6.634510517120361\n",
      "Epoch: 1/1 - Step: 2196 - Loss: 6.605188369750977\n",
      "Epoch: 1/1 - Step: 2197 - Loss: 6.690358638763428\n",
      "Epoch: 1/1 - Step: 2198 - Loss: 6.506607532501221\n",
      "Epoch: 1/1 - Step: 2199 - Loss: 6.544714450836182\n",
      "Epoch: 1/1 - Step: 2200 - Loss: 6.473269939422607\n",
      "Epoch: 1/1 - Step: 2201 - Loss: 6.568155765533447\n",
      "Epoch: 1/1 - Step: 2202 - Loss: 6.613744735717773\n",
      "Epoch: 1/1 - Step: 2203 - Loss: 6.470146179199219\n",
      "Epoch: 1/1 - Step: 2204 - Loss: 6.469518184661865\n",
      "Epoch: 1/1 - Step: 2205 - Loss: 6.635639667510986\n",
      "Epoch: 1/1 - Step: 2206 - Loss: 6.699532985687256\n",
      "Epoch: 1/1 - Step: 2207 - Loss: 6.448974609375\n",
      "Epoch: 1/1 - Step: 2208 - Loss: 6.5116729736328125\n",
      "Epoch: 1/1 - Step: 2209 - Loss: 6.499692440032959\n",
      "Epoch: 1/1 - Step: 2210 - Loss: 6.561072826385498\n",
      "Epoch: 1/1 - Step: 2211 - Loss: 6.654994487762451\n",
      "Epoch: 1/1 - Step: 2212 - Loss: 6.602055549621582\n",
      "Epoch: 1/1 - Step: 2213 - Loss: 6.445396423339844\n",
      "Epoch: 1/1 - Step: 2214 - Loss: 6.674729347229004\n",
      "Epoch: 1/1 - Step: 2215 - Loss: 6.435804843902588\n",
      "Epoch: 1/1 - Step: 2216 - Loss: 6.59860372543335\n",
      "Epoch: 1/1 - Step: 2217 - Loss: 6.454838275909424\n",
      "Epoch: 1/1 - Step: 2218 - Loss: 6.411279201507568\n",
      "Epoch: 1/1 - Step: 2219 - Loss: 6.477622032165527\n",
      "Epoch: 1/1 - Step: 2220 - Loss: 6.670583724975586\n",
      "Epoch: 1/1 - Step: 2221 - Loss: 6.534054279327393\n",
      "Epoch: 1/1 - Step: 2222 - Loss: 6.396236419677734\n",
      "Epoch: 1/1 - Step: 2223 - Loss: 6.5539116859436035\n",
      "Epoch: 1/1 - Step: 2224 - Loss: 6.511048793792725\n",
      "Epoch: 1/1 - Step: 2225 - Loss: 6.743343353271484\n",
      "Epoch: 1/1 - Step: 2226 - Loss: 6.754029273986816\n",
      "Epoch: 1/1 - Step: 2227 - Loss: 6.753263473510742\n",
      "Epoch: 1/1 - Step: 2228 - Loss: 6.72044563293457\n",
      "Epoch: 1/1 - Step: 2229 - Loss: 6.5774641036987305\n",
      "Epoch: 1/1 - Step: 2230 - Loss: 6.518648147583008\n",
      "Epoch: 1/1 - Step: 2231 - Loss: 6.431753158569336\n",
      "Epoch: 1/1 - Step: 2232 - Loss: 6.3780364990234375\n",
      "Epoch: 1/1 - Step: 2233 - Loss: 6.445916652679443\n",
      "Epoch: 1/1 - Step: 2234 - Loss: 6.527498722076416\n",
      "Epoch: 1/1 - Step: 2235 - Loss: 6.393249034881592\n",
      "Epoch: 1/1 - Step: 2236 - Loss: 6.372457504272461\n",
      "Epoch: 1/1 - Step: 2237 - Loss: 6.544570446014404\n",
      "Epoch: 1/1 - Step: 2238 - Loss: 6.753666400909424\n",
      "Epoch: 1/1 - Step: 2239 - Loss: 6.362300872802734\n",
      "Epoch: 1/1 - Step: 2240 - Loss: 6.525774955749512\n",
      "Epoch: 1/1 - Step: 2241 - Loss: 6.511790752410889\n",
      "Epoch: 1/1 - Step: 2242 - Loss: 6.534463882446289\n",
      "Epoch: 1/1 - Step: 2243 - Loss: 6.544765472412109\n",
      "Epoch: 1/1 - Step: 2244 - Loss: 6.41842794418335\n",
      "Epoch: 1/1 - Step: 2245 - Loss: 6.5621657371521\n",
      "Epoch: 1/1 - Step: 2246 - Loss: 6.472168922424316\n",
      "Epoch: 1/1 - Step: 2247 - Loss: 6.493741989135742\n",
      "Epoch: 1/1 - Step: 2248 - Loss: 6.461655139923096\n",
      "Epoch: 1/1 - Step: 2249 - Loss: 6.448473930358887\n",
      "Epoch: 1/1 - Step: 2250 - Loss: 6.637257099151611\n",
      "Epoch: 1/1 - Step: 2251 - Loss: 6.585207462310791\n",
      "Epoch: 1/1 - Step: 2252 - Loss: 6.502869129180908\n",
      "Epoch: 1/1 - Step: 2253 - Loss: 6.525714874267578\n",
      "Epoch: 1/1 - Step: 2254 - Loss: 6.502348899841309\n",
      "Epoch: 1/1 - Step: 2255 - Loss: 6.446703910827637\n",
      "Epoch: 1/1 - Step: 2256 - Loss: 6.4001688957214355\n",
      "Epoch: 1/1 - Step: 2257 - Loss: 6.397111892700195\n",
      "Epoch: 1/1 - Step: 2258 - Loss: 6.549257755279541\n",
      "Epoch: 1/1 - Step: 2259 - Loss: 6.402982711791992\n",
      "Epoch: 1/1 - Step: 2260 - Loss: 6.460048675537109\n",
      "Epoch: 1/1 - Step: 2261 - Loss: 6.426219940185547\n",
      "Epoch: 1/1 - Step: 2262 - Loss: 6.392928123474121\n",
      "Epoch: 1/1 - Step: 2263 - Loss: 6.3454670906066895\n",
      "Epoch: 1/1 - Step: 2264 - Loss: 6.472018718719482\n",
      "Epoch: 1/1 - Step: 2265 - Loss: 6.46168327331543\n",
      "Epoch: 1/1 - Step: 2266 - Loss: 6.745460510253906\n",
      "Epoch: 1/1 - Step: 2267 - Loss: 6.634552478790283\n",
      "Epoch: 1/1 - Step: 2268 - Loss: 6.41500186920166\n",
      "Epoch: 1/1 - Step: 2269 - Loss: 6.4608612060546875\n",
      "Epoch: 1/1 - Step: 2270 - Loss: 6.47352409362793\n",
      "Epoch: 1/1 - Step: 2271 - Loss: 6.552357196807861\n",
      "Epoch: 1/1 - Step: 2272 - Loss: 6.470874309539795\n",
      "Epoch: 1/1 - Step: 2273 - Loss: 6.432350158691406\n",
      "Epoch: 1/1 - Step: 2274 - Loss: 6.470318794250488\n",
      "Epoch: 1/1 - Step: 2275 - Loss: 6.48345947265625\n",
      "Epoch: 1/1 - Step: 2276 - Loss: 6.485313415527344\n",
      "Epoch: 1/1 - Step: 2277 - Loss: 6.419522285461426\n",
      "Epoch: 1/1 - Step: 2278 - Loss: 6.535886764526367\n",
      "Epoch: 1/1 - Step: 2279 - Loss: 8.84155559539795\n",
      "Epoch: 1/1 - Step: 2280 - Loss: 7.356637001037598\n",
      "Epoch: 1/1 - Step: 2281 - Loss: 6.545004844665527\n",
      "Epoch: 1/1 - Step: 2282 - Loss: 6.338414192199707\n",
      "Epoch: 1/1 - Step: 2283 - Loss: 6.3892059326171875\n",
      "Epoch: 1/1 - Step: 2284 - Loss: 6.507063388824463\n",
      "Epoch: 1/1 - Step: 2285 - Loss: 6.658738136291504\n",
      "Epoch: 1/1 - Step: 2286 - Loss: 6.475773811340332\n",
      "Epoch: 1/1 - Step: 2287 - Loss: 6.4789958000183105\n",
      "Epoch: 1/1 - Step: 2288 - Loss: 6.597970008850098\n",
      "Epoch: 1/1 - Step: 2289 - Loss: 6.633927345275879\n",
      "Epoch: 1/1 - Step: 2290 - Loss: 6.694772243499756\n",
      "Epoch: 1/1 - Step: 2291 - Loss: 6.541005611419678\n",
      "Epoch: 1/1 - Step: 2292 - Loss: 6.399087905883789\n",
      "Epoch: 1/1 - Step: 2293 - Loss: 6.438169956207275\n",
      "Epoch: 1/1 - Step: 2294 - Loss: 6.574498653411865\n",
      "Epoch: 1/1 - Step: 2295 - Loss: 6.682211399078369\n",
      "Epoch: 1/1 - Step: 2296 - Loss: 6.455430507659912\n",
      "Epoch: 1/1 - Step: 2297 - Loss: 6.3303399085998535\n",
      "Epoch: 1/1 - Step: 2298 - Loss: 6.255162715911865\n",
      "Epoch: 1/1 - Step: 2299 - Loss: 6.384644508361816\n",
      "Epoch: 1/1 - Step: 2300 - Loss: 6.648133277893066\n",
      "Epoch: 1/1 - Step: 2301 - Loss: 6.3893938064575195\n",
      "Epoch: 1/1 - Step: 2302 - Loss: 6.4483232498168945\n",
      "Epoch: 1/1 - Step: 2303 - Loss: 6.507657527923584\n",
      "Epoch: 1/1 - Step: 2304 - Loss: 6.493330001831055\n",
      "Epoch: 1/1 - Step: 2305 - Loss: 6.415366172790527\n",
      "Epoch: 1/1 - Step: 2306 - Loss: 6.59627103805542\n",
      "Epoch: 1/1 - Step: 2307 - Loss: 6.495036602020264\n",
      "Epoch: 1/1 - Step: 2308 - Loss: 6.409192085266113\n",
      "Epoch: 1/1 - Step: 2309 - Loss: 6.422643184661865\n",
      "Epoch: 1/1 - Step: 2310 - Loss: 6.485595703125\n",
      "Epoch: 1/1 - Step: 2311 - Loss: 6.44573450088501\n",
      "Epoch: 1/1 - Step: 2312 - Loss: 6.5312299728393555\n",
      "Epoch: 1/1 - Step: 2313 - Loss: 6.4945173263549805\n",
      "Epoch: 1/1 - Step: 2314 - Loss: 6.728335380554199\n",
      "Epoch: 1/1 - Step: 2315 - Loss: 6.351524353027344\n",
      "Epoch: 1/1 - Step: 2316 - Loss: 6.4475932121276855\n",
      "Epoch: 1/1 - Step: 2317 - Loss: 6.437351703643799\n",
      "Epoch: 1/1 - Step: 2318 - Loss: 6.448652744293213\n",
      "Epoch: 1/1 - Step: 2319 - Loss: 6.432837963104248\n",
      "Epoch: 1/1 - Step: 2320 - Loss: 6.50379753112793\n",
      "Epoch: 1/1 - Step: 2321 - Loss: 6.360008716583252\n",
      "Epoch: 1/1 - Step: 2322 - Loss: 6.478835582733154\n",
      "Epoch: 1/1 - Step: 2323 - Loss: 6.545929431915283\n",
      "Epoch: 1/1 - Step: 2324 - Loss: 6.350001335144043\n",
      "Epoch: 1/1 - Step: 2325 - Loss: 6.303011894226074\n",
      "Epoch: 1/1 - Step: 2326 - Loss: 6.414926052093506\n",
      "Epoch: 1/1 - Step: 2327 - Loss: 6.425723075866699\n",
      "Epoch: 1/1 - Step: 2328 - Loss: 6.440546989440918\n",
      "Epoch: 1/1 - Step: 2329 - Loss: 8.832446098327637\n",
      "Epoch: 1/1 - Step: 2330 - Loss: 7.3058576583862305\n",
      "Epoch: 1/1 - Step: 2331 - Loss: 6.6009416580200195\n",
      "Epoch: 1/1 - Step: 2332 - Loss: 6.57806396484375\n",
      "Epoch: 1/1 - Step: 2333 - Loss: 6.487904071807861\n",
      "Epoch: 1/1 - Step: 2334 - Loss: 6.444361686706543\n",
      "Epoch: 1/1 - Step: 2335 - Loss: 6.543075084686279\n",
      "Epoch: 1/1 - Step: 2336 - Loss: 6.464016437530518\n",
      "Epoch: 1/1 - Step: 2337 - Loss: 6.581578254699707\n",
      "Epoch: 1/1 - Step: 2338 - Loss: 6.438257217407227\n",
      "Epoch: 1/1 - Step: 2339 - Loss: 6.58898401260376\n",
      "Epoch: 1/1 - Step: 2340 - Loss: 6.567512035369873\n",
      "Epoch: 1/1 - Step: 2341 - Loss: 6.499974250793457\n",
      "Epoch: 1/1 - Step: 2342 - Loss: 6.3218817710876465\n",
      "Epoch: 1/1 - Step: 2343 - Loss: 6.344201564788818\n",
      "Epoch: 1/1 - Step: 2344 - Loss: 6.4836907386779785\n",
      "Epoch: 1/1 - Step: 2345 - Loss: 6.416041851043701\n",
      "Epoch: 1/1 - Step: 2346 - Loss: 6.520256519317627\n",
      "Epoch: 1/1 - Step: 2347 - Loss: 6.446020126342773\n",
      "Epoch: 1/1 - Step: 2348 - Loss: 6.470839023590088\n",
      "Epoch: 1/1 - Step: 2349 - Loss: 6.42750358581543\n",
      "Epoch: 1/1 - Step: 2350 - Loss: 6.39786958694458\n",
      "Epoch: 1/1 - Step: 2351 - Loss: 6.400278568267822\n",
      "Epoch: 1/1 - Step: 2352 - Loss: 6.453483581542969\n",
      "Epoch: 1/1 - Step: 2353 - Loss: 6.383018970489502\n",
      "Epoch: 1/1 - Step: 2354 - Loss: 6.3548583984375\n",
      "Epoch: 1/1 - Step: 2355 - Loss: 6.6367106437683105\n",
      "Epoch: 1/1 - Step: 2356 - Loss: 6.456265449523926\n",
      "Epoch: 1/1 - Step: 2357 - Loss: 6.300562858581543\n",
      "Epoch: 1/1 - Step: 2358 - Loss: 6.4029693603515625\n",
      "Epoch: 1/1 - Step: 2359 - Loss: 6.617767810821533\n",
      "Epoch: 1/1 - Step: 2360 - Loss: 6.416009902954102\n",
      "Epoch: 1/1 - Step: 2361 - Loss: 6.355679512023926\n",
      "Epoch: 1/1 - Step: 2362 - Loss: 6.435177326202393\n",
      "Epoch: 1/1 - Step: 2363 - Loss: 6.429222106933594\n",
      "Epoch: 1/1 - Step: 2364 - Loss: 8.797061920166016\n",
      "Epoch: 1/1 - Step: 2365 - Loss: 6.825606346130371\n",
      "Epoch: 1/1 - Step: 2366 - Loss: 6.438401222229004\n",
      "Epoch: 1/1 - Step: 2367 - Loss: 6.441640853881836\n",
      "Epoch: 1/1 - Step: 2368 - Loss: 6.710037708282471\n",
      "Epoch: 1/1 - Step: 2369 - Loss: 6.641784191131592\n",
      "Epoch: 1/1 - Step: 2370 - Loss: 6.563199520111084\n",
      "Epoch: 1/1 - Step: 2371 - Loss: 6.521669864654541\n",
      "Epoch: 1/1 - Step: 2372 - Loss: 6.678647994995117\n",
      "Epoch: 1/1 - Step: 2373 - Loss: 6.70359468460083\n",
      "Epoch: 1/1 - Step: 2374 - Loss: 6.636402130126953\n",
      "Epoch: 1/1 - Step: 2375 - Loss: 6.533515930175781\n",
      "Epoch: 1/1 - Step: 2376 - Loss: 6.712498188018799\n",
      "Epoch: 1/1 - Step: 2377 - Loss: 6.7414069175720215\n",
      "Epoch: 1/1 - Step: 2378 - Loss: 6.539402484893799\n",
      "Epoch: 1/1 - Step: 2379 - Loss: 6.682873249053955\n",
      "Epoch: 1/1 - Step: 2380 - Loss: 6.648812294006348\n",
      "Epoch: 1/1 - Step: 2381 - Loss: 6.66650915145874\n",
      "Epoch: 1/1 - Step: 2382 - Loss: 6.562466621398926\n",
      "Epoch: 1/1 - Step: 2383 - Loss: 6.705496788024902\n",
      "Epoch: 1/1 - Step: 2384 - Loss: 6.4544267654418945\n",
      "Epoch: 1/1 - Step: 2385 - Loss: 6.4263200759887695\n",
      "Epoch: 1/1 - Step: 2386 - Loss: 6.35406494140625\n",
      "Epoch: 1/1 - Step: 2387 - Loss: 6.431656360626221\n",
      "Epoch: 1/1 - Step: 2388 - Loss: 6.421045780181885\n",
      "Epoch: 1/1 - Step: 2389 - Loss: 7.12489128112793\n",
      "Epoch: 1/1 - Step: 2390 - Loss: 8.892806053161621\n",
      "Epoch: 1/1 - Step: 2391 - Loss: 9.073997497558594\n",
      "Epoch: 1/1 - Step: 2392 - Loss: 8.173538208007812\n",
      "Epoch: 1/1 - Step: 2393 - Loss: 6.634157657623291\n",
      "Epoch: 1/1 - Step: 2394 - Loss: 6.620606422424316\n",
      "Epoch: 1/1 - Step: 2395 - Loss: 6.516108512878418\n",
      "Epoch: 1/1 - Step: 2396 - Loss: 6.715734481811523\n",
      "Epoch: 1/1 - Step: 2397 - Loss: 6.683953285217285\n",
      "Epoch: 1/1 - Step: 2398 - Loss: 6.68984317779541\n",
      "Epoch: 1/1 - Step: 2399 - Loss: 6.598649978637695\n",
      "Epoch: 1/1 - Step: 2400 - Loss: 6.354823112487793\n",
      "Epoch: 1/1 - Step: 2401 - Loss: 6.315121650695801\n",
      "Epoch: 1/1 - Step: 2402 - Loss: 6.501194000244141\n",
      "Epoch: 1/1 - Step: 2403 - Loss: 6.593684673309326\n",
      "Epoch: 1/1 - Step: 2404 - Loss: 6.646280288696289\n",
      "Epoch: 1/1 - Step: 2405 - Loss: 6.556944370269775\n",
      "Epoch: 1/1 - Step: 2406 - Loss: 6.7291083335876465\n",
      "Epoch: 1/1 - Step: 2407 - Loss: 6.674164295196533\n",
      "Epoch: 1/1 - Step: 2408 - Loss: 6.695103645324707\n",
      "Epoch: 1/1 - Step: 2409 - Loss: 6.624995231628418\n",
      "Epoch: 1/1 - Step: 2410 - Loss: 6.688708782196045\n",
      "Epoch: 1/1 - Step: 2411 - Loss: 6.693058013916016\n",
      "Epoch: 1/1 - Step: 2412 - Loss: 6.666636943817139\n",
      "Epoch: 1/1 - Step: 2413 - Loss: 6.613577365875244\n",
      "Epoch: 1/1 - Step: 2414 - Loss: 6.6670989990234375\n",
      "Epoch: 1/1 - Step: 2415 - Loss: 6.528955936431885\n",
      "Epoch: 1/1 - Step: 2416 - Loss: 7.13513708114624\n",
      "Epoch: 1/1 - Step: 2417 - Loss: 7.542961597442627\n",
      "Epoch: 1/1 - Step: 2418 - Loss: 6.59534215927124\n",
      "Epoch: 1/1 - Step: 2419 - Loss: 6.586888790130615\n",
      "Epoch: 1/1 - Step: 2420 - Loss: 6.535253047943115\n",
      "Epoch: 1/1 - Step: 2421 - Loss: 6.5769219398498535\n",
      "Epoch: 1/1 - Step: 2422 - Loss: 6.471492767333984\n",
      "Epoch: 1/1 - Step: 2423 - Loss: 6.526001930236816\n",
      "Epoch: 1/1 - Step: 2424 - Loss: 6.634654521942139\n",
      "Epoch: 1/1 - Step: 2425 - Loss: 6.525665760040283\n",
      "Epoch: 1/1 - Step: 2426 - Loss: 6.619927406311035\n",
      "Epoch: 1/1 - Step: 2427 - Loss: 6.579829216003418\n",
      "Epoch: 1/1 - Step: 2428 - Loss: 6.559909820556641\n",
      "Epoch: 1/1 - Step: 2429 - Loss: 6.848000526428223\n",
      "Epoch: 1/1 - Step: 2430 - Loss: 6.59910774230957\n",
      "Epoch: 1/1 - Step: 2431 - Loss: 6.461384296417236\n",
      "Epoch: 1/1 - Step: 2432 - Loss: 6.5792036056518555\n",
      "Epoch: 1/1 - Step: 2433 - Loss: 6.476962089538574\n",
      "Epoch: 1/1 - Step: 2434 - Loss: 6.542695045471191\n",
      "Epoch: 1/1 - Step: 2435 - Loss: 6.513485908508301\n",
      "Epoch: 1/1 - Step: 2436 - Loss: 6.5706071853637695\n",
      "Epoch: 1/1 - Step: 2437 - Loss: 6.603263854980469\n",
      "Epoch: 1/1 - Step: 2438 - Loss: 6.584875106811523\n",
      "Epoch: 1/1 - Step: 2439 - Loss: 6.613142967224121\n",
      "Epoch: 1/1 - Step: 2440 - Loss: 6.5468339920043945\n",
      "Epoch: 1/1 - Step: 2441 - Loss: 6.5976948738098145\n",
      "Epoch: 1/1 - Step: 2442 - Loss: 6.561377048492432\n",
      "Epoch: 1/1 - Step: 2443 - Loss: 6.582627296447754\n",
      "Epoch: 1/1 - Step: 2444 - Loss: 6.59077262878418\n",
      "Epoch: 1/1 - Step: 2445 - Loss: 6.579967975616455\n",
      "Epoch: 1/1 - Step: 2446 - Loss: 6.693657398223877\n",
      "Epoch: 1/1 - Step: 2447 - Loss: 6.737070560455322\n",
      "Epoch: 1/1 - Step: 2448 - Loss: 6.379806995391846\n",
      "Epoch: 1/1 - Step: 2449 - Loss: 6.396902561187744\n",
      "Epoch: 1/1 - Step: 2450 - Loss: 6.389850616455078\n",
      "Epoch: 1/1 - Step: 2451 - Loss: 6.435318470001221\n",
      "Epoch: 1/1 - Step: 2452 - Loss: 6.442460060119629\n",
      "Epoch: 1/1 - Step: 2453 - Loss: 6.52360725402832\n",
      "Epoch: 1/1 - Step: 2454 - Loss: 6.53773307800293\n",
      "Epoch: 1/1 - Step: 2455 - Loss: 6.564111709594727\n",
      "Epoch: 1/1 - Step: 2456 - Loss: 6.590418815612793\n",
      "Epoch: 1/1 - Step: 2457 - Loss: 6.813291072845459\n",
      "Epoch: 1/1 - Step: 2458 - Loss: 6.627844333648682\n",
      "Epoch: 1/1 - Step: 2459 - Loss: 6.508893013000488\n",
      "Epoch: 1/1 - Step: 2460 - Loss: 6.22824239730835\n",
      "Epoch: 1/1 - Step: 2461 - Loss: 6.300105094909668\n",
      "Epoch: 1/1 - Step: 2462 - Loss: 6.241434097290039\n",
      "Epoch: 1/1 - Step: 2463 - Loss: 6.388197898864746\n",
      "Epoch: 1/1 - Step: 2464 - Loss: 6.503258228302002\n",
      "Epoch: 1/1 - Step: 2465 - Loss: 6.783311367034912\n",
      "Epoch: 1/1 - Step: 2466 - Loss: 6.595003128051758\n",
      "Epoch: 1/1 - Step: 2467 - Loss: 6.7966437339782715\n",
      "Epoch: 1/1 - Step: 2468 - Loss: 6.577847003936768\n",
      "Epoch: 1/1 - Step: 2469 - Loss: 6.5248260498046875\n",
      "Epoch: 1/1 - Step: 2470 - Loss: 6.61657190322876\n",
      "Epoch: 1/1 - Step: 2471 - Loss: 6.691478729248047\n",
      "Epoch: 1/1 - Step: 2472 - Loss: 6.769227504730225\n",
      "Epoch: 1/1 - Step: 2473 - Loss: 6.5177507400512695\n",
      "Epoch: 1/1 - Step: 2474 - Loss: 6.576840877532959\n",
      "Epoch: 1/1 - Step: 2475 - Loss: 6.546684741973877\n",
      "Epoch: 1/1 - Step: 2476 - Loss: 6.610967636108398\n",
      "Epoch: 1/1 - Step: 2477 - Loss: 6.710983753204346\n",
      "Epoch: 1/1 - Step: 2478 - Loss: 6.606561660766602\n",
      "Epoch: 1/1 - Step: 2479 - Loss: 6.561069011688232\n",
      "Epoch: 1/1 - Step: 2480 - Loss: 6.613029956817627\n",
      "Epoch: 1/1 - Step: 2481 - Loss: 6.44125509262085\n",
      "Epoch: 1/1 - Step: 2482 - Loss: 6.528034687042236\n",
      "Epoch: 1/1 - Step: 2483 - Loss: 6.585480690002441\n",
      "Epoch: 1/1 - Step: 2484 - Loss: 6.6235270500183105\n",
      "Epoch: 1/1 - Step: 2485 - Loss: 6.552616596221924\n",
      "Epoch: 1/1 - Step: 2486 - Loss: 6.679449558258057\n",
      "Epoch: 1/1 - Step: 2487 - Loss: 6.55156946182251\n",
      "Epoch: 1/1 - Step: 2488 - Loss: 8.088682174682617\n",
      "Epoch: 1/1 - Step: 2489 - Loss: 8.806654930114746\n",
      "Epoch: 1/1 - Step: 2490 - Loss: 8.912616729736328\n",
      "Epoch: 1/1 - Step: 2491 - Loss: 6.752251625061035\n",
      "Epoch: 1/1 - Step: 2492 - Loss: 6.572018623352051\n",
      "Epoch: 1/1 - Step: 2493 - Loss: 6.537290096282959\n",
      "Epoch: 1/1 - Step: 2494 - Loss: 6.514546871185303\n",
      "Epoch: 1/1 - Step: 2495 - Loss: 6.689853668212891\n",
      "Epoch: 1/1 - Step: 2496 - Loss: 6.576205730438232\n",
      "Epoch: 1/1 - Step: 2497 - Loss: 6.583630561828613\n",
      "Epoch: 1/1 - Step: 2498 - Loss: 6.594418525695801\n",
      "Epoch: 1/1 - Step: 2499 - Loss: 6.628612995147705\n",
      "Epoch: 1/1 - Step: 2500 - Loss: 6.58730936050415\n",
      "Epoch: 1/1 - Step: 2501 - Loss: 6.5020294189453125\n",
      "Epoch: 1/1 - Step: 2502 - Loss: 6.613966941833496\n",
      "Epoch: 1/1 - Step: 2503 - Loss: 6.58005952835083\n",
      "Epoch: 1/1 - Step: 2504 - Loss: 6.521914005279541\n",
      "Epoch: 1/1 - Step: 2505 - Loss: 6.58406400680542\n",
      "Epoch: 1/1 - Step: 2506 - Loss: 6.706558704376221\n",
      "Epoch: 1/1 - Step: 2507 - Loss: 6.570868968963623\n",
      "Epoch: 1/1 - Step: 2508 - Loss: 6.6091156005859375\n",
      "Epoch: 1/1 - Step: 2509 - Loss: 6.50265645980835\n",
      "Epoch: 1/1 - Step: 2510 - Loss: 6.511894226074219\n",
      "Epoch: 1/1 - Step: 2511 - Loss: 6.646365165710449\n",
      "Epoch: 1/1 - Step: 2512 - Loss: 6.561383247375488\n",
      "Epoch: 1/1 - Step: 2513 - Loss: 6.6311116218566895\n",
      "Epoch: 1/1 - Step: 2514 - Loss: 6.553922176361084\n",
      "Epoch: 1/1 - Step: 2515 - Loss: 6.501506805419922\n",
      "Epoch: 1/1 - Step: 2516 - Loss: 6.538355827331543\n",
      "Epoch: 1/1 - Step: 2517 - Loss: 6.633461952209473\n",
      "Epoch: 1/1 - Step: 2518 - Loss: 6.5654520988464355\n",
      "Epoch: 1/1 - Step: 2519 - Loss: 6.681396961212158\n",
      "Epoch: 1/1 - Step: 2520 - Loss: 6.572258949279785\n",
      "Epoch: 1/1 - Step: 2521 - Loss: 6.660327434539795\n",
      "Epoch: 1/1 - Step: 2522 - Loss: 6.446578502655029\n",
      "Epoch: 1/1 - Step: 2523 - Loss: 6.438938617706299\n",
      "Epoch: 1/1 - Step: 2524 - Loss: 6.459765911102295\n",
      "Epoch: 1/1 - Step: 2525 - Loss: 6.403127670288086\n",
      "Epoch: 1/1 - Step: 2526 - Loss: 6.441636085510254\n",
      "Epoch: 1/1 - Step: 2527 - Loss: 6.426288604736328\n",
      "Epoch: 1/1 - Step: 2528 - Loss: 6.525475978851318\n",
      "Epoch: 1/1 - Step: 2529 - Loss: 6.467228412628174\n",
      "Epoch: 1/1 - Step: 2530 - Loss: 6.355114936828613\n",
      "Epoch: 1/1 - Step: 2531 - Loss: 6.459298610687256\n",
      "Epoch: 1/1 - Step: 2532 - Loss: 6.3648858070373535\n",
      "Epoch: 1/1 - Step: 2533 - Loss: 6.620023250579834\n",
      "Epoch: 1/1 - Step: 2534 - Loss: 6.720963001251221\n",
      "Epoch: 1/1 - Step: 2535 - Loss: 6.660546779632568\n",
      "Epoch: 1/1 - Step: 2536 - Loss: 6.388236045837402\n",
      "Epoch: 1/1 - Step: 2537 - Loss: 6.589258670806885\n",
      "Epoch: 1/1 - Step: 2538 - Loss: 6.578168869018555\n",
      "Epoch: 1/1 - Step: 2539 - Loss: 6.601353168487549\n",
      "Epoch: 1/1 - Step: 2540 - Loss: 6.469876289367676\n",
      "Epoch: 1/1 - Step: 2541 - Loss: 6.479010581970215\n",
      "Epoch: 1/1 - Step: 2542 - Loss: 6.417018413543701\n",
      "Epoch: 1/1 - Step: 2543 - Loss: 6.509427070617676\n",
      "Epoch: 1/1 - Step: 2544 - Loss: 6.589117527008057\n",
      "Epoch: 1/1 - Step: 2545 - Loss: 6.385914325714111\n",
      "Epoch: 1/1 - Step: 2546 - Loss: 6.448690414428711\n",
      "Epoch: 1/1 - Step: 2547 - Loss: 6.620969772338867\n",
      "Epoch: 1/1 - Step: 2548 - Loss: 6.601421356201172\n",
      "Epoch: 1/1 - Step: 2549 - Loss: 6.420386791229248\n",
      "Epoch: 1/1 - Step: 2550 - Loss: 6.428964138031006\n",
      "Epoch: 1/1 - Step: 2551 - Loss: 6.527914524078369\n",
      "Epoch: 1/1 - Step: 2552 - Loss: 6.41066837310791\n",
      "Epoch: 1/1 - Step: 2553 - Loss: 6.772429943084717\n",
      "Epoch: 1/1 - Step: 2554 - Loss: 6.414906024932861\n",
      "Epoch: 1/1 - Step: 2555 - Loss: 6.470137596130371\n",
      "Epoch: 1/1 - Step: 2556 - Loss: 6.575611114501953\n",
      "Epoch: 1/1 - Step: 2557 - Loss: 6.373659133911133\n",
      "Epoch: 1/1 - Step: 2558 - Loss: 6.581638813018799\n",
      "Epoch: 1/1 - Step: 2559 - Loss: 6.380488872528076\n",
      "Epoch: 1/1 - Step: 2560 - Loss: 6.433032035827637\n",
      "Epoch: 1/1 - Step: 2561 - Loss: 6.3790106773376465\n",
      "Epoch: 1/1 - Step: 2562 - Loss: 6.6334991455078125\n",
      "Epoch: 1/1 - Step: 2563 - Loss: 6.428027629852295\n",
      "Epoch: 1/1 - Step: 2564 - Loss: 6.33743143081665\n",
      "Epoch: 1/1 - Step: 2565 - Loss: 6.5705437660217285\n",
      "Epoch: 1/1 - Step: 2566 - Loss: 6.502577781677246\n",
      "Epoch: 1/1 - Step: 2567 - Loss: 6.699012756347656\n",
      "Epoch: 1/1 - Step: 2568 - Loss: 6.779274940490723\n",
      "Epoch: 1/1 - Step: 2569 - Loss: 6.682773590087891\n",
      "Epoch: 1/1 - Step: 2570 - Loss: 6.5695343017578125\n",
      "Epoch: 1/1 - Step: 2571 - Loss: 6.565742492675781\n",
      "Epoch: 1/1 - Step: 2572 - Loss: 6.452565670013428\n",
      "Epoch: 1/1 - Step: 2573 - Loss: 6.343703746795654\n",
      "Epoch: 1/1 - Step: 2574 - Loss: 6.355713844299316\n",
      "Epoch: 1/1 - Step: 2575 - Loss: 6.3761115074157715\n",
      "Epoch: 1/1 - Step: 2576 - Loss: 6.47113037109375\n",
      "Epoch: 1/1 - Step: 2577 - Loss: 6.358774662017822\n",
      "Epoch: 1/1 - Step: 2578 - Loss: 6.301931381225586\n",
      "Epoch: 1/1 - Step: 2579 - Loss: 6.555237770080566\n",
      "Epoch: 1/1 - Step: 2580 - Loss: 6.673558235168457\n",
      "Epoch: 1/1 - Step: 2581 - Loss: 6.323980331420898\n",
      "Epoch: 1/1 - Step: 2582 - Loss: 6.488734722137451\n",
      "Epoch: 1/1 - Step: 2583 - Loss: 6.485649585723877\n",
      "Epoch: 1/1 - Step: 2584 - Loss: 6.404481887817383\n",
      "Epoch: 1/1 - Step: 2585 - Loss: 6.5184760093688965\n",
      "Epoch: 1/1 - Step: 2586 - Loss: 6.360589027404785\n",
      "Epoch: 1/1 - Step: 2587 - Loss: 6.490020751953125\n",
      "Epoch: 1/1 - Step: 2588 - Loss: 6.455515384674072\n",
      "Epoch: 1/1 - Step: 2589 - Loss: 6.424545764923096\n",
      "Epoch: 1/1 - Step: 2590 - Loss: 6.414615631103516\n",
      "Epoch: 1/1 - Step: 2591 - Loss: 6.395336151123047\n",
      "Epoch: 1/1 - Step: 2592 - Loss: 6.62136173248291\n",
      "Epoch: 1/1 - Step: 2593 - Loss: 6.4888434410095215\n",
      "Epoch: 1/1 - Step: 2594 - Loss: 6.456755638122559\n",
      "Epoch: 1/1 - Step: 2595 - Loss: 6.494825839996338\n",
      "Epoch: 1/1 - Step: 2596 - Loss: 6.481983661651611\n",
      "Epoch: 1/1 - Step: 2597 - Loss: 6.340301990509033\n",
      "Epoch: 1/1 - Step: 2598 - Loss: 6.346958160400391\n",
      "Epoch: 1/1 - Step: 2599 - Loss: 6.357428073883057\n",
      "Epoch: 1/1 - Step: 2600 - Loss: 6.478475570678711\n",
      "Epoch: 1/1 - Step: 2601 - Loss: 6.379473686218262\n",
      "Epoch: 1/1 - Step: 2602 - Loss: 6.393467426300049\n",
      "Epoch: 1/1 - Step: 2603 - Loss: 6.3897881507873535\n",
      "Epoch: 1/1 - Step: 2604 - Loss: 6.305914878845215\n",
      "Epoch: 1/1 - Step: 2605 - Loss: 6.350600719451904\n",
      "Epoch: 1/1 - Step: 2606 - Loss: 6.420984745025635\n",
      "Epoch: 1/1 - Step: 2607 - Loss: 6.483883380889893\n",
      "Epoch: 1/1 - Step: 2608 - Loss: 6.7220048904418945\n",
      "Epoch: 1/1 - Step: 2609 - Loss: 6.462963581085205\n",
      "Epoch: 1/1 - Step: 2610 - Loss: 6.385714530944824\n",
      "Epoch: 1/1 - Step: 2611 - Loss: 6.401887893676758\n",
      "Epoch: 1/1 - Step: 2612 - Loss: 6.46579647064209\n",
      "Epoch: 1/1 - Step: 2613 - Loss: 6.505453586578369\n",
      "Epoch: 1/1 - Step: 2614 - Loss: 6.382145404815674\n",
      "Epoch: 1/1 - Step: 2615 - Loss: 6.376564979553223\n",
      "Epoch: 1/1 - Step: 2616 - Loss: 6.469052791595459\n",
      "Epoch: 1/1 - Step: 2617 - Loss: 6.416059970855713\n",
      "Epoch: 1/1 - Step: 2618 - Loss: 6.391077995300293\n",
      "Epoch: 1/1 - Step: 2619 - Loss: 6.418264865875244\n",
      "Epoch: 1/1 - Step: 2620 - Loss: 6.84572172164917\n",
      "Epoch: 1/1 - Step: 2621 - Loss: 8.931142807006836\n",
      "Epoch: 1/1 - Step: 2622 - Loss: 6.830935001373291\n",
      "Epoch: 1/1 - Step: 2623 - Loss: 6.462571620941162\n",
      "Epoch: 1/1 - Step: 2624 - Loss: 6.245840549468994\n",
      "Epoch: 1/1 - Step: 2625 - Loss: 6.36816930770874\n",
      "Epoch: 1/1 - Step: 2626 - Loss: 6.564299583435059\n",
      "Epoch: 1/1 - Step: 2627 - Loss: 6.530025005340576\n",
      "Epoch: 1/1 - Step: 2628 - Loss: 6.418722629547119\n",
      "Epoch: 1/1 - Step: 2629 - Loss: 6.487165451049805\n",
      "Epoch: 1/1 - Step: 2630 - Loss: 6.574021339416504\n",
      "Epoch: 1/1 - Step: 2631 - Loss: 6.622017860412598\n",
      "Epoch: 1/1 - Step: 2632 - Loss: 6.651460647583008\n",
      "Epoch: 1/1 - Step: 2633 - Loss: 6.4192423820495605\n",
      "Epoch: 1/1 - Step: 2634 - Loss: 6.335723400115967\n",
      "Epoch: 1/1 - Step: 2635 - Loss: 6.422533988952637\n",
      "Epoch: 1/1 - Step: 2636 - Loss: 6.5545878410339355\n",
      "Epoch: 1/1 - Step: 2637 - Loss: 6.607384204864502\n",
      "Epoch: 1/1 - Step: 2638 - Loss: 6.410255432128906\n",
      "Epoch: 1/1 - Step: 2639 - Loss: 6.2565741539001465\n",
      "Epoch: 1/1 - Step: 2640 - Loss: 6.164760112762451\n",
      "Epoch: 1/1 - Step: 2641 - Loss: 6.434751510620117\n",
      "Epoch: 1/1 - Step: 2642 - Loss: 6.5497026443481445\n",
      "Epoch: 1/1 - Step: 2643 - Loss: 6.33453893661499\n",
      "Epoch: 1/1 - Step: 2644 - Loss: 6.431057929992676\n",
      "Epoch: 1/1 - Step: 2645 - Loss: 6.454397201538086\n",
      "Epoch: 1/1 - Step: 2646 - Loss: 6.421973705291748\n",
      "Epoch: 1/1 - Step: 2647 - Loss: 6.347012519836426\n",
      "Epoch: 1/1 - Step: 2648 - Loss: 6.574082374572754\n",
      "Epoch: 1/1 - Step: 2649 - Loss: 6.445950031280518\n",
      "Epoch: 1/1 - Step: 2650 - Loss: 6.385739326477051\n",
      "Epoch: 1/1 - Step: 2651 - Loss: 6.361766338348389\n",
      "Epoch: 1/1 - Step: 2652 - Loss: 6.3911027908325195\n",
      "Epoch: 1/1 - Step: 2653 - Loss: 6.489912033081055\n",
      "Epoch: 1/1 - Step: 2654 - Loss: 6.422693729400635\n",
      "Epoch: 1/1 - Step: 2655 - Loss: 6.5434346199035645\n",
      "Epoch: 1/1 - Step: 2656 - Loss: 6.603820323944092\n",
      "Epoch: 1/1 - Step: 2657 - Loss: 6.309101581573486\n",
      "Epoch: 1/1 - Step: 2658 - Loss: 6.4228997230529785\n",
      "Epoch: 1/1 - Step: 2659 - Loss: 6.369450569152832\n",
      "Epoch: 1/1 - Step: 2660 - Loss: 6.4298014640808105\n",
      "Epoch: 1/1 - Step: 2661 - Loss: 6.365599155426025\n",
      "Epoch: 1/1 - Step: 2662 - Loss: 6.427411079406738\n",
      "Epoch: 1/1 - Step: 2663 - Loss: 6.386589050292969\n",
      "Epoch: 1/1 - Step: 2664 - Loss: 6.415360450744629\n",
      "Epoch: 1/1 - Step: 2665 - Loss: 6.465731620788574\n",
      "Epoch: 1/1 - Step: 2666 - Loss: 6.278247356414795\n",
      "Epoch: 1/1 - Step: 2667 - Loss: 6.265636920928955\n",
      "Epoch: 1/1 - Step: 2668 - Loss: 6.382978439331055\n",
      "Epoch: 1/1 - Step: 2669 - Loss: 6.314070701599121\n",
      "Epoch: 1/1 - Step: 2670 - Loss: 7.003215312957764\n",
      "Epoch: 1/1 - Step: 2671 - Loss: 8.726856231689453\n",
      "Epoch: 1/1 - Step: 2672 - Loss: 6.839118003845215\n",
      "Epoch: 1/1 - Step: 2673 - Loss: 6.549735069274902\n",
      "Epoch: 1/1 - Step: 2674 - Loss: 6.46578311920166\n",
      "Epoch: 1/1 - Step: 2675 - Loss: 6.4757866859436035\n",
      "Epoch: 1/1 - Step: 2676 - Loss: 6.409311294555664\n",
      "Epoch: 1/1 - Step: 2677 - Loss: 6.435825347900391\n",
      "Epoch: 1/1 - Step: 2678 - Loss: 6.528501987457275\n",
      "Epoch: 1/1 - Step: 2679 - Loss: 6.432281017303467\n",
      "Epoch: 1/1 - Step: 2680 - Loss: 6.403273105621338\n",
      "Epoch: 1/1 - Step: 2681 - Loss: 6.590775489807129\n",
      "Epoch: 1/1 - Step: 2682 - Loss: 6.51760721206665\n",
      "Epoch: 1/1 - Step: 2683 - Loss: 6.367412567138672\n",
      "Epoch: 1/1 - Step: 2684 - Loss: 6.263301372528076\n",
      "Epoch: 1/1 - Step: 2685 - Loss: 6.360517501831055\n",
      "Epoch: 1/1 - Step: 2686 - Loss: 6.408650875091553\n",
      "Epoch: 1/1 - Step: 2687 - Loss: 6.442307949066162\n",
      "Epoch: 1/1 - Step: 2688 - Loss: 6.447121620178223\n",
      "Epoch: 1/1 - Step: 2689 - Loss: 6.383927822113037\n",
      "Epoch: 1/1 - Step: 2690 - Loss: 6.435723304748535\n",
      "Epoch: 1/1 - Step: 2691 - Loss: 6.326001167297363\n",
      "Epoch: 1/1 - Step: 2692 - Loss: 6.320516586303711\n",
      "Epoch: 1/1 - Step: 2693 - Loss: 6.406641483306885\n",
      "Epoch: 1/1 - Step: 2694 - Loss: 6.389533042907715\n",
      "Epoch: 1/1 - Step: 2695 - Loss: 6.334405422210693\n",
      "Epoch: 1/1 - Step: 2696 - Loss: 6.304280757904053\n",
      "Epoch: 1/1 - Step: 2697 - Loss: 6.595088005065918\n",
      "Epoch: 1/1 - Step: 2698 - Loss: 6.386152267456055\n",
      "Epoch: 1/1 - Step: 2699 - Loss: 6.28904390335083\n",
      "Epoch: 1/1 - Step: 2700 - Loss: 6.4322686195373535\n",
      "Epoch: 1/1 - Step: 2701 - Loss: 6.505936145782471\n",
      "Epoch: 1/1 - Step: 2702 - Loss: 6.3902764320373535\n",
      "Epoch: 1/1 - Step: 2703 - Loss: 6.280020713806152\n",
      "Epoch: 1/1 - Step: 2704 - Loss: 6.409388065338135\n",
      "Epoch: 1/1 - Step: 2705 - Loss: 6.76263427734375\n",
      "Epoch: 1/1 - Step: 2706 - Loss: 8.607879638671875\n",
      "Epoch: 1/1 - Step: 2707 - Loss: 6.536566734313965\n",
      "Epoch: 1/1 - Step: 2708 - Loss: 6.394950866699219\n",
      "Epoch: 1/1 - Step: 2709 - Loss: 6.453135013580322\n",
      "Epoch: 1/1 - Step: 2710 - Loss: 6.663138389587402\n",
      "Epoch: 1/1 - Step: 2711 - Loss: 6.525129795074463\n",
      "Epoch: 1/1 - Step: 2712 - Loss: 6.518863677978516\n",
      "Epoch: 1/1 - Step: 2713 - Loss: 6.617437362670898\n",
      "Epoch: 1/1 - Step: 2714 - Loss: 6.578387260437012\n",
      "Epoch: 1/1 - Step: 2715 - Loss: 6.614396572113037\n",
      "Epoch: 1/1 - Step: 2716 - Loss: 6.690248489379883\n",
      "Epoch: 1/1 - Step: 2717 - Loss: 6.508264064788818\n",
      "Epoch: 1/1 - Step: 2718 - Loss: 6.6177473068237305\n",
      "Epoch: 1/1 - Step: 2719 - Loss: 6.685670375823975\n",
      "Epoch: 1/1 - Step: 2720 - Loss: 6.5372748374938965\n",
      "Epoch: 1/1 - Step: 2721 - Loss: 6.617657661437988\n",
      "Epoch: 1/1 - Step: 2722 - Loss: 6.601940631866455\n",
      "Epoch: 1/1 - Step: 2723 - Loss: 6.604223251342773\n",
      "Epoch: 1/1 - Step: 2724 - Loss: 6.554023265838623\n",
      "Epoch: 1/1 - Step: 2725 - Loss: 6.665826320648193\n",
      "Epoch: 1/1 - Step: 2726 - Loss: 6.357664585113525\n",
      "Epoch: 1/1 - Step: 2727 - Loss: 6.353720188140869\n",
      "Epoch: 1/1 - Step: 2728 - Loss: 6.34268856048584\n",
      "Epoch: 1/1 - Step: 2729 - Loss: 6.388402462005615\n",
      "Epoch: 1/1 - Step: 2730 - Loss: 6.352123260498047\n",
      "Epoch: 1/1 - Step: 2731 - Loss: 7.610575199127197\n",
      "Epoch: 1/1 - Step: 2732 - Loss: 8.854679107666016\n",
      "Epoch: 1/1 - Step: 2733 - Loss: 9.056083679199219\n",
      "Epoch: 1/1 - Step: 2734 - Loss: 7.604935169219971\n",
      "Epoch: 1/1 - Step: 2735 - Loss: 6.514125823974609\n",
      "Epoch: 1/1 - Step: 2736 - Loss: 6.653088569641113\n",
      "Epoch: 1/1 - Step: 2737 - Loss: 6.431900978088379\n",
      "Epoch: 1/1 - Step: 2738 - Loss: 6.697214126586914\n",
      "Epoch: 1/1 - Step: 2739 - Loss: 6.609049320220947\n",
      "Epoch: 1/1 - Step: 2740 - Loss: 6.6733856201171875\n",
      "Epoch: 1/1 - Step: 2741 - Loss: 6.513667583465576\n",
      "Epoch: 1/1 - Step: 2742 - Loss: 6.227568626403809\n",
      "Epoch: 1/1 - Step: 2743 - Loss: 6.31518030166626\n",
      "Epoch: 1/1 - Step: 2744 - Loss: 6.533008575439453\n",
      "Epoch: 1/1 - Step: 2745 - Loss: 6.529930591583252\n",
      "Epoch: 1/1 - Step: 2746 - Loss: 6.58493185043335\n",
      "Epoch: 1/1 - Step: 2747 - Loss: 6.54250431060791\n",
      "Epoch: 1/1 - Step: 2748 - Loss: 6.675619602203369\n",
      "Epoch: 1/1 - Step: 2749 - Loss: 6.613129615783691\n",
      "Epoch: 1/1 - Step: 2750 - Loss: 6.661026954650879\n",
      "Epoch: 1/1 - Step: 2751 - Loss: 6.663480758666992\n",
      "Epoch: 1/1 - Step: 2752 - Loss: 6.570240497589111\n",
      "Epoch: 1/1 - Step: 2753 - Loss: 6.6361188888549805\n",
      "Epoch: 1/1 - Step: 2754 - Loss: 6.632103443145752\n",
      "Epoch: 1/1 - Step: 2755 - Loss: 6.581730365753174\n",
      "Epoch: 1/1 - Step: 2756 - Loss: 6.5439910888671875\n",
      "Epoch: 1/1 - Step: 2757 - Loss: 6.533265113830566\n",
      "Epoch: 1/1 - Step: 2758 - Loss: 7.485497951507568\n",
      "Epoch: 1/1 - Step: 2759 - Loss: 7.1057891845703125\n",
      "Epoch: 1/1 - Step: 2760 - Loss: 6.580581188201904\n",
      "Epoch: 1/1 - Step: 2761 - Loss: 6.523065567016602\n",
      "Epoch: 1/1 - Step: 2762 - Loss: 6.524450302124023\n",
      "Epoch: 1/1 - Step: 2763 - Loss: 6.461033821105957\n",
      "Epoch: 1/1 - Step: 2764 - Loss: 6.488895893096924\n",
      "Epoch: 1/1 - Step: 2765 - Loss: 6.521613597869873\n",
      "Epoch: 1/1 - Step: 2766 - Loss: 6.584678649902344\n",
      "Epoch: 1/1 - Step: 2767 - Loss: 6.482394218444824\n",
      "Epoch: 1/1 - Step: 2768 - Loss: 6.562706470489502\n",
      "Epoch: 1/1 - Step: 2769 - Loss: 6.492602825164795\n",
      "Epoch: 1/1 - Step: 2770 - Loss: 6.591683864593506\n",
      "Epoch: 1/1 - Step: 2771 - Loss: 6.82494592666626\n",
      "Epoch: 1/1 - Step: 2772 - Loss: 6.4823994636535645\n",
      "Epoch: 1/1 - Step: 2773 - Loss: 6.412817001342773\n",
      "Epoch: 1/1 - Step: 2774 - Loss: 6.523156642913818\n",
      "Epoch: 1/1 - Step: 2775 - Loss: 6.4649763107299805\n",
      "Epoch: 1/1 - Step: 2776 - Loss: 6.481537818908691\n",
      "Epoch: 1/1 - Step: 2777 - Loss: 6.506322383880615\n",
      "Epoch: 1/1 - Step: 2778 - Loss: 6.52390718460083\n",
      "Epoch: 1/1 - Step: 2779 - Loss: 6.524690628051758\n",
      "Epoch: 1/1 - Step: 2780 - Loss: 6.567840099334717\n",
      "Epoch: 1/1 - Step: 2781 - Loss: 6.554361820220947\n",
      "Epoch: 1/1 - Step: 2782 - Loss: 6.468328952789307\n",
      "Epoch: 1/1 - Step: 2783 - Loss: 6.624975204467773\n",
      "Epoch: 1/1 - Step: 2784 - Loss: 6.46528434753418\n",
      "Epoch: 1/1 - Step: 2785 - Loss: 6.572776794433594\n",
      "Epoch: 1/1 - Step: 2786 - Loss: 6.5183258056640625\n",
      "Epoch: 1/1 - Step: 2787 - Loss: 6.58882474899292\n",
      "Epoch: 1/1 - Step: 2788 - Loss: 6.677286624908447\n",
      "Epoch: 1/1 - Step: 2789 - Loss: 6.631312847137451\n",
      "Epoch: 1/1 - Step: 2790 - Loss: 6.291933536529541\n",
      "Epoch: 1/1 - Step: 2791 - Loss: 6.335607528686523\n",
      "Epoch: 1/1 - Step: 2792 - Loss: 6.416842937469482\n",
      "Epoch: 1/1 - Step: 2793 - Loss: 6.355101585388184\n",
      "Epoch: 1/1 - Step: 2794 - Loss: 6.418973922729492\n",
      "Epoch: 1/1 - Step: 2795 - Loss: 6.468480587005615\n",
      "Epoch: 1/1 - Step: 2796 - Loss: 6.456351280212402\n",
      "Epoch: 1/1 - Step: 2797 - Loss: 6.571981430053711\n",
      "Epoch: 1/1 - Step: 2798 - Loss: 6.544076442718506\n",
      "Epoch: 1/1 - Step: 2799 - Loss: 6.821247100830078\n",
      "Epoch: 1/1 - Step: 2800 - Loss: 6.564436435699463\n",
      "Epoch: 1/1 - Step: 2801 - Loss: 6.347801208496094\n",
      "Epoch: 1/1 - Step: 2802 - Loss: 6.1943678855896\n",
      "Epoch: 1/1 - Step: 2803 - Loss: 6.252827167510986\n",
      "Epoch: 1/1 - Step: 2804 - Loss: 6.18682336807251\n",
      "Epoch: 1/1 - Step: 2805 - Loss: 6.44624137878418\n",
      "Epoch: 1/1 - Step: 2806 - Loss: 6.4845476150512695\n",
      "Epoch: 1/1 - Step: 2807 - Loss: 6.669834613800049\n",
      "Epoch: 1/1 - Step: 2808 - Loss: 6.638855457305908\n",
      "Epoch: 1/1 - Step: 2809 - Loss: 6.735545635223389\n",
      "Epoch: 1/1 - Step: 2810 - Loss: 6.520804405212402\n",
      "Epoch: 1/1 - Step: 2811 - Loss: 6.4637980461120605\n",
      "Epoch: 1/1 - Step: 2812 - Loss: 6.5923991203308105\n",
      "Epoch: 1/1 - Step: 2813 - Loss: 6.708226680755615\n",
      "Epoch: 1/1 - Step: 2814 - Loss: 6.678377151489258\n",
      "Epoch: 1/1 - Step: 2815 - Loss: 6.466885089874268\n",
      "Epoch: 1/1 - Step: 2816 - Loss: 6.490785121917725\n",
      "Epoch: 1/1 - Step: 2817 - Loss: 6.5231099128723145\n",
      "Epoch: 1/1 - Step: 2818 - Loss: 6.633396625518799\n",
      "Epoch: 1/1 - Step: 2819 - Loss: 6.704378128051758\n",
      "Epoch: 1/1 - Step: 2820 - Loss: 6.484259605407715\n",
      "Epoch: 1/1 - Step: 2821 - Loss: 6.533988952636719\n",
      "Epoch: 1/1 - Step: 2822 - Loss: 6.501863479614258\n",
      "Epoch: 1/1 - Step: 2823 - Loss: 6.53511905670166\n",
      "Epoch: 1/1 - Step: 2824 - Loss: 6.4383368492126465\n",
      "Epoch: 1/1 - Step: 2825 - Loss: 6.560337543487549\n",
      "Epoch: 1/1 - Step: 2826 - Loss: 6.619048595428467\n",
      "Epoch: 1/1 - Step: 2827 - Loss: 6.539820194244385\n",
      "Epoch: 1/1 - Step: 2828 - Loss: 6.555455207824707\n",
      "Epoch: 1/1 - Step: 2829 - Loss: 6.5955023765563965\n",
      "Epoch: 1/1 - Step: 2830 - Loss: 8.438213348388672\n",
      "Epoch: 1/1 - Step: 2831 - Loss: 8.84052562713623\n",
      "Epoch: 1/1 - Step: 2832 - Loss: 8.515472412109375\n",
      "Epoch: 1/1 - Step: 2833 - Loss: 6.537700653076172\n",
      "Epoch: 1/1 - Step: 2834 - Loss: 6.469837188720703\n",
      "Epoch: 1/1 - Step: 2835 - Loss: 6.508383274078369\n",
      "Epoch: 1/1 - Step: 2836 - Loss: 6.548436641693115\n",
      "Epoch: 1/1 - Step: 2837 - Loss: 6.672003746032715\n",
      "Epoch: 1/1 - Step: 2838 - Loss: 6.480619430541992\n",
      "Epoch: 1/1 - Step: 2839 - Loss: 6.530074596405029\n",
      "Epoch: 1/1 - Step: 2840 - Loss: 6.544121742248535\n",
      "Epoch: 1/1 - Step: 2841 - Loss: 6.5829010009765625\n",
      "Epoch: 1/1 - Step: 2842 - Loss: 6.54397439956665\n",
      "Epoch: 1/1 - Step: 2843 - Loss: 6.498041152954102\n",
      "Epoch: 1/1 - Step: 2844 - Loss: 6.529346466064453\n",
      "Epoch: 1/1 - Step: 2845 - Loss: 6.548268795013428\n",
      "Epoch: 1/1 - Step: 2846 - Loss: 6.408074855804443\n",
      "Epoch: 1/1 - Step: 2847 - Loss: 6.61398983001709\n",
      "Epoch: 1/1 - Step: 2848 - Loss: 6.606899738311768\n",
      "Epoch: 1/1 - Step: 2849 - Loss: 6.538265705108643\n",
      "Epoch: 1/1 - Step: 2850 - Loss: 6.555912494659424\n",
      "Epoch: 1/1 - Step: 2851 - Loss: 6.490149974822998\n",
      "Epoch: 1/1 - Step: 2852 - Loss: 6.472111701965332\n",
      "Epoch: 1/1 - Step: 2853 - Loss: 6.584986209869385\n",
      "Epoch: 1/1 - Step: 2854 - Loss: 6.558529376983643\n",
      "Epoch: 1/1 - Step: 2855 - Loss: 6.539666175842285\n",
      "Epoch: 1/1 - Step: 2856 - Loss: 6.491382598876953\n",
      "Epoch: 1/1 - Step: 2857 - Loss: 6.488894939422607\n",
      "Epoch: 1/1 - Step: 2858 - Loss: 6.543231010437012\n",
      "Epoch: 1/1 - Step: 2859 - Loss: 6.524510860443115\n",
      "Epoch: 1/1 - Step: 2860 - Loss: 6.648887634277344\n",
      "Epoch: 1/1 - Step: 2861 - Loss: 6.534419059753418\n",
      "Epoch: 1/1 - Step: 2862 - Loss: 6.585458755493164\n",
      "Epoch: 1/1 - Step: 2863 - Loss: 6.554811954498291\n",
      "Epoch: 1/1 - Step: 2864 - Loss: 6.428797721862793\n",
      "Epoch: 1/1 - Step: 2865 - Loss: 6.384209156036377\n",
      "Epoch: 1/1 - Step: 2866 - Loss: 6.419356822967529\n",
      "Epoch: 1/1 - Step: 2867 - Loss: 6.349550724029541\n",
      "Epoch: 1/1 - Step: 2868 - Loss: 6.400197982788086\n",
      "Epoch: 1/1 - Step: 2869 - Loss: 6.389255046844482\n",
      "Epoch: 1/1 - Step: 2870 - Loss: 6.441442012786865\n",
      "Epoch: 1/1 - Step: 2871 - Loss: 6.431449890136719\n",
      "Epoch: 1/1 - Step: 2872 - Loss: 6.339200496673584\n",
      "Epoch: 1/1 - Step: 2873 - Loss: 6.414422035217285\n",
      "Epoch: 1/1 - Step: 2874 - Loss: 6.34020471572876\n",
      "Epoch: 1/1 - Step: 2875 - Loss: 6.620587348937988\n",
      "Epoch: 1/1 - Step: 2876 - Loss: 6.678928375244141\n",
      "Epoch: 1/1 - Step: 2877 - Loss: 6.533143520355225\n",
      "Epoch: 1/1 - Step: 2878 - Loss: 6.401997089385986\n",
      "Epoch: 1/1 - Step: 2879 - Loss: 6.4866251945495605\n",
      "Epoch: 1/1 - Step: 2880 - Loss: 6.582787036895752\n",
      "Epoch: 1/1 - Step: 2881 - Loss: 6.543446063995361\n",
      "Epoch: 1/1 - Step: 2882 - Loss: 6.418884754180908\n",
      "Epoch: 1/1 - Step: 2883 - Loss: 6.43182897567749\n",
      "Epoch: 1/1 - Step: 2884 - Loss: 6.437014579772949\n",
      "Epoch: 1/1 - Step: 2885 - Loss: 6.442412376403809\n",
      "Epoch: 1/1 - Step: 2886 - Loss: 6.474257946014404\n",
      "Epoch: 1/1 - Step: 2887 - Loss: 6.391643524169922\n",
      "Epoch: 1/1 - Step: 2888 - Loss: 6.461066246032715\n",
      "Epoch: 1/1 - Step: 2889 - Loss: 6.5969767570495605\n",
      "Epoch: 1/1 - Step: 2890 - Loss: 6.473597049713135\n",
      "Epoch: 1/1 - Step: 2891 - Loss: 6.385251045227051\n",
      "Epoch: 1/1 - Step: 2892 - Loss: 6.414506435394287\n",
      "Epoch: 1/1 - Step: 2893 - Loss: 6.463104724884033\n",
      "Epoch: 1/1 - Step: 2894 - Loss: 6.345489978790283\n",
      "Epoch: 1/1 - Step: 2895 - Loss: 6.767344951629639\n",
      "Epoch: 1/1 - Step: 2896 - Loss: 6.302927017211914\n",
      "Epoch: 1/1 - Step: 2897 - Loss: 6.501284122467041\n",
      "Epoch: 1/1 - Step: 2898 - Loss: 6.516475200653076\n",
      "Epoch: 1/1 - Step: 2899 - Loss: 6.350303649902344\n",
      "Epoch: 1/1 - Step: 2900 - Loss: 6.584876537322998\n",
      "Epoch: 1/1 - Step: 2901 - Loss: 6.259280681610107\n",
      "Epoch: 1/1 - Step: 2902 - Loss: 6.4272074699401855\n",
      "Epoch: 1/1 - Step: 2903 - Loss: 6.3257317543029785\n",
      "Epoch: 1/1 - Step: 2904 - Loss: 6.590608596801758\n",
      "Epoch: 1/1 - Step: 2905 - Loss: 6.401379585266113\n",
      "Epoch: 1/1 - Step: 2906 - Loss: 6.356805801391602\n",
      "Epoch: 1/1 - Step: 2907 - Loss: 6.456523895263672\n",
      "Epoch: 1/1 - Step: 2908 - Loss: 6.532857418060303\n",
      "Epoch: 1/1 - Step: 2909 - Loss: 6.635128974914551\n",
      "Epoch: 1/1 - Step: 2910 - Loss: 6.782645225524902\n",
      "Epoch: 1/1 - Step: 2911 - Loss: 6.579172611236572\n",
      "Epoch: 1/1 - Step: 2912 - Loss: 6.54878568649292\n",
      "Epoch: 1/1 - Step: 2913 - Loss: 6.479491233825684\n",
      "Epoch: 1/1 - Step: 2914 - Loss: 6.42830228805542\n",
      "Epoch: 1/1 - Step: 2915 - Loss: 6.2369561195373535\n",
      "Epoch: 1/1 - Step: 2916 - Loss: 6.324909210205078\n",
      "Epoch: 1/1 - Step: 2917 - Loss: 6.415693759918213\n",
      "Epoch: 1/1 - Step: 2918 - Loss: 6.365527629852295\n",
      "Epoch: 1/1 - Step: 2919 - Loss: 6.3211469650268555\n",
      "Epoch: 1/1 - Step: 2920 - Loss: 6.2689714431762695\n",
      "Epoch: 1/1 - Step: 2921 - Loss: 6.593418121337891\n",
      "Epoch: 1/1 - Step: 2922 - Loss: 6.488792896270752\n",
      "Epoch: 1/1 - Step: 2923 - Loss: 6.391016483306885\n",
      "Epoch: 1/1 - Step: 2924 - Loss: 6.390323162078857\n",
      "Epoch: 1/1 - Step: 2925 - Loss: 6.457495212554932\n",
      "Epoch: 1/1 - Step: 2926 - Loss: 6.357769966125488\n",
      "Epoch: 1/1 - Step: 2927 - Loss: 6.437289714813232\n",
      "Epoch: 1/1 - Step: 2928 - Loss: 6.3751397132873535\n",
      "Epoch: 1/1 - Step: 2929 - Loss: 6.40985107421875\n",
      "Epoch: 1/1 - Step: 2930 - Loss: 6.3921003341674805\n",
      "Epoch: 1/1 - Step: 2931 - Loss: 6.386542797088623\n",
      "Epoch: 1/1 - Step: 2932 - Loss: 6.3624067306518555\n",
      "Epoch: 1/1 - Step: 2933 - Loss: 6.434114456176758\n",
      "Epoch: 1/1 - Step: 2934 - Loss: 6.5095086097717285\n",
      "Epoch: 1/1 - Step: 2935 - Loss: 6.482234954833984\n",
      "Epoch: 1/1 - Step: 2936 - Loss: 6.39575719833374\n",
      "Epoch: 1/1 - Step: 2937 - Loss: 6.4900312423706055\n",
      "Epoch: 1/1 - Step: 2938 - Loss: 6.401864528656006\n",
      "Epoch: 1/1 - Step: 2939 - Loss: 6.329051494598389\n",
      "Epoch: 1/1 - Step: 2940 - Loss: 6.300713539123535\n",
      "Epoch: 1/1 - Step: 2941 - Loss: 6.342118740081787\n",
      "Epoch: 1/1 - Step: 2942 - Loss: 6.425309181213379\n",
      "Epoch: 1/1 - Step: 2943 - Loss: 6.318408012390137\n",
      "Epoch: 1/1 - Step: 2944 - Loss: 6.37853479385376\n",
      "Epoch: 1/1 - Step: 2945 - Loss: 6.298562526702881\n",
      "Epoch: 1/1 - Step: 2946 - Loss: 6.267467021942139\n",
      "Epoch: 1/1 - Step: 2947 - Loss: 6.340404033660889\n",
      "Epoch: 1/1 - Step: 2948 - Loss: 6.368332386016846\n",
      "Epoch: 1/1 - Step: 2949 - Loss: 6.491794586181641\n",
      "Epoch: 1/1 - Step: 2950 - Loss: 6.686694145202637\n",
      "Epoch: 1/1 - Step: 2951 - Loss: 6.38473653793335\n",
      "Epoch: 1/1 - Step: 2952 - Loss: 6.353468418121338\n",
      "Epoch: 1/1 - Step: 2953 - Loss: 6.406574726104736\n",
      "Epoch: 1/1 - Step: 2954 - Loss: 6.371981143951416\n",
      "Epoch: 1/1 - Step: 2955 - Loss: 6.518546104431152\n",
      "Epoch: 1/1 - Step: 2956 - Loss: 6.3080949783325195\n",
      "Epoch: 1/1 - Step: 2957 - Loss: 6.31460428237915\n",
      "Epoch: 1/1 - Step: 2958 - Loss: 6.41010046005249\n",
      "Epoch: 1/1 - Step: 2959 - Loss: 6.42555046081543\n",
      "Epoch: 1/1 - Step: 2960 - Loss: 6.288135051727295\n",
      "Epoch: 1/1 - Step: 2961 - Loss: 6.3582234382629395\n",
      "Epoch: 1/1 - Step: 2962 - Loss: 7.397534370422363\n",
      "Epoch: 1/1 - Step: 2963 - Loss: 8.61072826385498\n",
      "Epoch: 1/1 - Step: 2964 - Loss: 6.468424320220947\n",
      "Epoch: 1/1 - Step: 2965 - Loss: 6.375253200531006\n",
      "Epoch: 1/1 - Step: 2966 - Loss: 6.236680030822754\n",
      "Epoch: 1/1 - Step: 2967 - Loss: 6.332107067108154\n",
      "Epoch: 1/1 - Step: 2968 - Loss: 6.560956954956055\n",
      "Epoch: 1/1 - Step: 2969 - Loss: 6.453408241271973\n",
      "Epoch: 1/1 - Step: 2970 - Loss: 6.398864269256592\n",
      "Epoch: 1/1 - Step: 2971 - Loss: 6.437657356262207\n",
      "Epoch: 1/1 - Step: 2972 - Loss: 6.572584629058838\n",
      "Epoch: 1/1 - Step: 2973 - Loss: 6.6435627937316895\n",
      "Epoch: 1/1 - Step: 2974 - Loss: 6.540401935577393\n",
      "Epoch: 1/1 - Step: 2975 - Loss: 6.371147155761719\n",
      "Epoch: 1/1 - Step: 2976 - Loss: 6.252269744873047\n",
      "Epoch: 1/1 - Step: 2977 - Loss: 6.444268226623535\n",
      "Epoch: 1/1 - Step: 2978 - Loss: 6.483549118041992\n",
      "Epoch: 1/1 - Step: 2979 - Loss: 6.548100471496582\n",
      "Epoch: 1/1 - Step: 2980 - Loss: 6.3490166664123535\n",
      "Epoch: 1/1 - Step: 2981 - Loss: 6.16843318939209\n",
      "Epoch: 1/1 - Step: 2982 - Loss: 6.145554542541504\n",
      "Epoch: 1/1 - Step: 2983 - Loss: 6.504187107086182\n",
      "Epoch: 1/1 - Step: 2984 - Loss: 6.445070266723633\n",
      "Epoch: 1/1 - Step: 2985 - Loss: 6.2956390380859375\n",
      "Epoch: 1/1 - Step: 2986 - Loss: 6.446844100952148\n",
      "Epoch: 1/1 - Step: 2987 - Loss: 6.382664203643799\n",
      "Epoch: 1/1 - Step: 2988 - Loss: 6.3641743659973145\n",
      "Epoch: 1/1 - Step: 2989 - Loss: 6.332269668579102\n",
      "Epoch: 1/1 - Step: 2990 - Loss: 6.549168109893799\n",
      "Epoch: 1/1 - Step: 2991 - Loss: 6.373213291168213\n",
      "Epoch: 1/1 - Step: 2992 - Loss: 6.318418979644775\n",
      "Epoch: 1/1 - Step: 2993 - Loss: 6.336601257324219\n",
      "Epoch: 1/1 - Step: 2994 - Loss: 6.360896587371826\n",
      "Epoch: 1/1 - Step: 2995 - Loss: 6.4608612060546875\n",
      "Epoch: 1/1 - Step: 2996 - Loss: 6.390504837036133\n",
      "Epoch: 1/1 - Step: 2997 - Loss: 6.600209712982178\n",
      "Epoch: 1/1 - Step: 2998 - Loss: 6.401385307312012\n",
      "Epoch: 1/1 - Step: 2999 - Loss: 6.302817344665527\n",
      "Epoch: 1/1 - Step: 3000 - Loss: 6.370273590087891\n",
      "Epoch: 1/1 - Step: 3001 - Loss: 6.3622589111328125\n",
      "Epoch: 1/1 - Step: 3002 - Loss: 6.378962993621826\n",
      "Epoch: 1/1 - Step: 3003 - Loss: 6.314609527587891\n",
      "Epoch: 1/1 - Step: 3004 - Loss: 6.328914642333984\n",
      "Epoch: 1/1 - Step: 3005 - Loss: 6.3955607414245605\n",
      "Epoch: 1/1 - Step: 3006 - Loss: 6.412235260009766\n",
      "Epoch: 1/1 - Step: 3007 - Loss: 6.329665660858154\n",
      "Epoch: 1/1 - Step: 3008 - Loss: 6.199412822723389\n",
      "Epoch: 1/1 - Step: 3009 - Loss: 6.295258045196533\n",
      "Epoch: 1/1 - Step: 3010 - Loss: 6.342968463897705\n",
      "Epoch: 1/1 - Step: 3011 - Loss: 6.253227233886719\n",
      "Epoch: 1/1 - Step: 3012 - Loss: 7.520654201507568\n",
      "Epoch: 1/1 - Step: 3013 - Loss: 8.448009490966797\n",
      "Epoch: 1/1 - Step: 3014 - Loss: 6.532534599304199\n",
      "Epoch: 1/1 - Step: 3015 - Loss: 6.439291000366211\n",
      "Epoch: 1/1 - Step: 3016 - Loss: 6.502548694610596\n",
      "Epoch: 1/1 - Step: 3017 - Loss: 6.395641803741455\n",
      "Epoch: 1/1 - Step: 3018 - Loss: 6.370311737060547\n",
      "Epoch: 1/1 - Step: 3019 - Loss: 6.35784912109375\n",
      "Epoch: 1/1 - Step: 3020 - Loss: 6.564069747924805\n",
      "Epoch: 1/1 - Step: 3021 - Loss: 6.340942859649658\n",
      "Epoch: 1/1 - Step: 3022 - Loss: 6.445368766784668\n",
      "Epoch: 1/1 - Step: 3023 - Loss: 6.4863409996032715\n",
      "Epoch: 1/1 - Step: 3024 - Loss: 6.4679036140441895\n",
      "Epoch: 1/1 - Step: 3025 - Loss: 6.315845966339111\n",
      "Epoch: 1/1 - Step: 3026 - Loss: 6.2286295890808105\n",
      "Epoch: 1/1 - Step: 3027 - Loss: 6.3445048332214355\n",
      "Epoch: 1/1 - Step: 3028 - Loss: 6.3279852867126465\n",
      "Epoch: 1/1 - Step: 3029 - Loss: 6.451816082000732\n",
      "Epoch: 1/1 - Step: 3030 - Loss: 6.389867305755615\n",
      "Epoch: 1/1 - Step: 3031 - Loss: 6.312695026397705\n",
      "Epoch: 1/1 - Step: 3032 - Loss: 6.395998954772949\n",
      "Epoch: 1/1 - Step: 3033 - Loss: 6.274911403656006\n",
      "Epoch: 1/1 - Step: 3034 - Loss: 6.284940719604492\n",
      "Epoch: 1/1 - Step: 3035 - Loss: 6.4345784187316895\n",
      "Epoch: 1/1 - Step: 3036 - Loss: 6.3063249588012695\n",
      "Epoch: 1/1 - Step: 3037 - Loss: 6.2706451416015625\n",
      "Epoch: 1/1 - Step: 3038 - Loss: 6.337841987609863\n",
      "Epoch: 1/1 - Step: 3039 - Loss: 6.520051002502441\n",
      "Epoch: 1/1 - Step: 3040 - Loss: 6.312826156616211\n",
      "Epoch: 1/1 - Step: 3041 - Loss: 6.283330917358398\n",
      "Epoch: 1/1 - Step: 3042 - Loss: 6.502625942230225\n",
      "Epoch: 1/1 - Step: 3043 - Loss: 6.300934791564941\n",
      "Epoch: 1/1 - Step: 3044 - Loss: 6.347472190856934\n",
      "Epoch: 1/1 - Step: 3045 - Loss: 6.256740570068359\n",
      "Epoch: 1/1 - Step: 3046 - Loss: 6.353853225708008\n",
      "Epoch: 1/1 - Step: 3047 - Loss: 7.2523345947265625\n",
      "Epoch: 1/1 - Step: 3048 - Loss: 8.056714057922363\n",
      "Epoch: 1/1 - Step: 3049 - Loss: 6.468904495239258\n",
      "Epoch: 1/1 - Step: 3050 - Loss: 6.3578996658325195\n",
      "Epoch: 1/1 - Step: 3051 - Loss: 6.469211101531982\n",
      "Epoch: 1/1 - Step: 3052 - Loss: 6.602282524108887\n",
      "Epoch: 1/1 - Step: 3053 - Loss: 6.48273229598999\n",
      "Epoch: 1/1 - Step: 3054 - Loss: 6.495664119720459\n",
      "Epoch: 1/1 - Step: 3055 - Loss: 6.595429420471191\n",
      "Epoch: 1/1 - Step: 3056 - Loss: 6.562318801879883\n",
      "Epoch: 1/1 - Step: 3057 - Loss: 6.5877790451049805\n",
      "Epoch: 1/1 - Step: 3058 - Loss: 6.567800045013428\n",
      "Epoch: 1/1 - Step: 3059 - Loss: 6.549008846282959\n",
      "Epoch: 1/1 - Step: 3060 - Loss: 6.619935512542725\n",
      "Epoch: 1/1 - Step: 3061 - Loss: 6.591515064239502\n",
      "Epoch: 1/1 - Step: 3062 - Loss: 6.527383804321289\n",
      "Epoch: 1/1 - Step: 3063 - Loss: 6.550881385803223\n",
      "Epoch: 1/1 - Step: 3064 - Loss: 6.6310648918151855\n",
      "Epoch: 1/1 - Step: 3065 - Loss: 6.512918949127197\n",
      "Epoch: 1/1 - Step: 3066 - Loss: 6.513792037963867\n",
      "Epoch: 1/1 - Step: 3067 - Loss: 6.610333442687988\n",
      "Epoch: 1/1 - Step: 3068 - Loss: 6.365823745727539\n",
      "Epoch: 1/1 - Step: 3069 - Loss: 6.26379919052124\n",
      "Epoch: 1/1 - Step: 3070 - Loss: 6.34477424621582\n",
      "Epoch: 1/1 - Step: 3071 - Loss: 6.349781036376953\n",
      "Epoch: 1/1 - Step: 3072 - Loss: 6.317009925842285\n",
      "Epoch: 1/1 - Step: 3073 - Loss: 8.064213752746582\n",
      "Epoch: 1/1 - Step: 3074 - Loss: 8.831703186035156\n",
      "Epoch: 1/1 - Step: 3075 - Loss: 8.97032642364502\n",
      "Epoch: 1/1 - Step: 3076 - Loss: 7.030930519104004\n",
      "Epoch: 1/1 - Step: 3077 - Loss: 6.534381866455078\n",
      "Epoch: 1/1 - Step: 3078 - Loss: 6.560927867889404\n",
      "Epoch: 1/1 - Step: 3079 - Loss: 6.40882682800293\n",
      "Epoch: 1/1 - Step: 3080 - Loss: 6.6636176109313965\n",
      "Epoch: 1/1 - Step: 3081 - Loss: 6.582677364349365\n",
      "Epoch: 1/1 - Step: 3082 - Loss: 6.641006946563721\n",
      "Epoch: 1/1 - Step: 3083 - Loss: 6.384870529174805\n",
      "Epoch: 1/1 - Step: 3084 - Loss: 6.153187274932861\n",
      "Epoch: 1/1 - Step: 3085 - Loss: 6.321252346038818\n",
      "Epoch: 1/1 - Step: 3086 - Loss: 6.528002738952637\n",
      "Epoch: 1/1 - Step: 3087 - Loss: 6.456190586090088\n",
      "Epoch: 1/1 - Step: 3088 - Loss: 6.5614705085754395\n",
      "Epoch: 1/1 - Step: 3089 - Loss: 6.54677677154541\n",
      "Epoch: 1/1 - Step: 3090 - Loss: 6.6302571296691895\n",
      "Epoch: 1/1 - Step: 3091 - Loss: 6.563998699188232\n",
      "Epoch: 1/1 - Step: 3092 - Loss: 6.615800857543945\n",
      "Epoch: 1/1 - Step: 3093 - Loss: 6.595440864562988\n",
      "Epoch: 1/1 - Step: 3094 - Loss: 6.5952959060668945\n",
      "Epoch: 1/1 - Step: 3095 - Loss: 6.594107151031494\n",
      "Epoch: 1/1 - Step: 3096 - Loss: 6.591857433319092\n",
      "Epoch: 1/1 - Step: 3097 - Loss: 6.530590534210205\n",
      "Epoch: 1/1 - Step: 3098 - Loss: 6.496286869049072\n",
      "Epoch: 1/1 - Step: 3099 - Loss: 6.525120258331299\n",
      "Epoch: 1/1 - Step: 3100 - Loss: 7.75382137298584\n",
      "Epoch: 1/1 - Step: 3101 - Loss: 6.697820663452148\n",
      "Epoch: 1/1 - Step: 3102 - Loss: 6.565516948699951\n",
      "Epoch: 1/1 - Step: 3103 - Loss: 6.464321613311768\n",
      "Epoch: 1/1 - Step: 3104 - Loss: 6.470831871032715\n",
      "Epoch: 1/1 - Step: 3105 - Loss: 6.409763813018799\n",
      "Epoch: 1/1 - Step: 3106 - Loss: 6.4385223388671875\n",
      "Epoch: 1/1 - Step: 3107 - Loss: 6.516169548034668\n",
      "Epoch: 1/1 - Step: 3108 - Loss: 6.502378940582275\n",
      "Epoch: 1/1 - Step: 3109 - Loss: 6.424286842346191\n",
      "Epoch: 1/1 - Step: 3110 - Loss: 6.557015419006348\n",
      "Epoch: 1/1 - Step: 3111 - Loss: 6.438706874847412\n",
      "Epoch: 1/1 - Step: 3112 - Loss: 6.5733184814453125\n",
      "Epoch: 1/1 - Step: 3113 - Loss: 6.782872676849365\n",
      "Epoch: 1/1 - Step: 3114 - Loss: 6.38896369934082\n",
      "Epoch: 1/1 - Step: 3115 - Loss: 6.452364444732666\n",
      "Epoch: 1/1 - Step: 3116 - Loss: 6.4213175773620605\n",
      "Epoch: 1/1 - Step: 3117 - Loss: 6.390768051147461\n",
      "Epoch: 1/1 - Step: 3118 - Loss: 6.478914737701416\n",
      "Epoch: 1/1 - Step: 3119 - Loss: 6.448236465454102\n",
      "Epoch: 1/1 - Step: 3120 - Loss: 6.5010552406311035\n",
      "Epoch: 1/1 - Step: 3121 - Loss: 6.485727787017822\n",
      "Epoch: 1/1 - Step: 3122 - Loss: 6.566626071929932\n",
      "Epoch: 1/1 - Step: 3123 - Loss: 6.480130672454834\n",
      "Epoch: 1/1 - Step: 3124 - Loss: 6.454364776611328\n",
      "Epoch: 1/1 - Step: 3125 - Loss: 6.567380428314209\n",
      "Epoch: 1/1 - Step: 3126 - Loss: 6.467435836791992\n",
      "Epoch: 1/1 - Step: 3127 - Loss: 6.506754398345947\n",
      "Epoch: 1/1 - Step: 3128 - Loss: 6.445932388305664\n",
      "Epoch: 1/1 - Step: 3129 - Loss: 6.58667516708374\n",
      "Epoch: 1/1 - Step: 3130 - Loss: 6.691405773162842\n",
      "Epoch: 1/1 - Step: 3131 - Loss: 6.466007709503174\n",
      "Epoch: 1/1 - Step: 3132 - Loss: 6.282135963439941\n",
      "Epoch: 1/1 - Step: 3133 - Loss: 6.270717620849609\n",
      "Epoch: 1/1 - Step: 3134 - Loss: 6.43703031539917\n",
      "Epoch: 1/1 - Step: 3135 - Loss: 6.2651872634887695\n",
      "Epoch: 1/1 - Step: 3136 - Loss: 6.451827526092529\n",
      "Epoch: 1/1 - Step: 3137 - Loss: 6.377236843109131\n",
      "Epoch: 1/1 - Step: 3138 - Loss: 6.453546047210693\n",
      "Epoch: 1/1 - Step: 3139 - Loss: 6.513130187988281\n",
      "Epoch: 1/1 - Step: 3140 - Loss: 6.546195030212402\n",
      "Epoch: 1/1 - Step: 3141 - Loss: 6.815091609954834\n",
      "Epoch: 1/1 - Step: 3142 - Loss: 6.502986431121826\n",
      "Epoch: 1/1 - Step: 3143 - Loss: 6.182694435119629\n",
      "Epoch: 1/1 - Step: 3144 - Loss: 6.232820987701416\n",
      "Epoch: 1/1 - Step: 3145 - Loss: 6.148473262786865\n",
      "Epoch: 1/1 - Step: 3146 - Loss: 6.1514129638671875\n",
      "Epoch: 1/1 - Step: 3147 - Loss: 6.448770523071289\n",
      "Epoch: 1/1 - Step: 3148 - Loss: 6.541795253753662\n",
      "Epoch: 1/1 - Step: 3149 - Loss: 6.64757776260376\n",
      "Epoch: 1/1 - Step: 3150 - Loss: 6.637174129486084\n",
      "Epoch: 1/1 - Step: 3151 - Loss: 6.674468040466309\n",
      "Epoch: 1/1 - Step: 3152 - Loss: 6.391433238983154\n",
      "Epoch: 1/1 - Step: 3153 - Loss: 6.513940811157227\n",
      "Epoch: 1/1 - Step: 3154 - Loss: 6.570237159729004\n",
      "Epoch: 1/1 - Step: 3155 - Loss: 6.626831531524658\n",
      "Epoch: 1/1 - Step: 3156 - Loss: 6.624338150024414\n",
      "Epoch: 1/1 - Step: 3157 - Loss: 6.4444732666015625\n",
      "Epoch: 1/1 - Step: 3158 - Loss: 6.4511308670043945\n",
      "Epoch: 1/1 - Step: 3159 - Loss: 6.506960868835449\n",
      "Epoch: 1/1 - Step: 3160 - Loss: 6.612620830535889\n",
      "Epoch: 1/1 - Step: 3161 - Loss: 6.670441150665283\n",
      "Epoch: 1/1 - Step: 3162 - Loss: 6.435495376586914\n",
      "Epoch: 1/1 - Step: 3163 - Loss: 6.518902778625488\n",
      "Epoch: 1/1 - Step: 3164 - Loss: 6.427875518798828\n",
      "Epoch: 1/1 - Step: 3165 - Loss: 6.500216007232666\n",
      "Epoch: 1/1 - Step: 3166 - Loss: 6.356217384338379\n",
      "Epoch: 1/1 - Step: 3167 - Loss: 6.526798248291016\n",
      "Epoch: 1/1 - Step: 3168 - Loss: 6.567131042480469\n",
      "Epoch: 1/1 - Step: 3169 - Loss: 6.569823741912842\n",
      "Epoch: 1/1 - Step: 3170 - Loss: 6.506504535675049\n",
      "Epoch: 1/1 - Step: 3171 - Loss: 6.807001113891602\n",
      "Epoch: 1/1 - Step: 3172 - Loss: 8.580634117126465\n",
      "Epoch: 1/1 - Step: 3173 - Loss: 8.884115219116211\n",
      "Epoch: 1/1 - Step: 3174 - Loss: 7.876617908477783\n",
      "Epoch: 1/1 - Step: 3175 - Loss: 6.556744575500488\n",
      "Epoch: 1/1 - Step: 3176 - Loss: 6.405270576477051\n",
      "Epoch: 1/1 - Step: 3177 - Loss: 6.458699703216553\n",
      "Epoch: 1/1 - Step: 3178 - Loss: 6.545706272125244\n",
      "Epoch: 1/1 - Step: 3179 - Loss: 6.6239213943481445\n",
      "Epoch: 1/1 - Step: 3180 - Loss: 6.412203788757324\n",
      "Epoch: 1/1 - Step: 3181 - Loss: 6.516639232635498\n",
      "Epoch: 1/1 - Step: 3182 - Loss: 6.568234920501709\n",
      "Epoch: 1/1 - Step: 3183 - Loss: 6.519484519958496\n",
      "Epoch: 1/1 - Step: 3184 - Loss: 6.475172996520996\n",
      "Epoch: 1/1 - Step: 3185 - Loss: 6.46212911605835\n",
      "Epoch: 1/1 - Step: 3186 - Loss: 6.527136325836182\n",
      "Epoch: 1/1 - Step: 3187 - Loss: 6.514584541320801\n",
      "Epoch: 1/1 - Step: 3188 - Loss: 6.348088264465332\n",
      "Epoch: 1/1 - Step: 3189 - Loss: 6.588754653930664\n",
      "Epoch: 1/1 - Step: 3190 - Loss: 6.552054405212402\n",
      "Epoch: 1/1 - Step: 3191 - Loss: 6.505624294281006\n",
      "Epoch: 1/1 - Step: 3192 - Loss: 6.4859418869018555\n",
      "Epoch: 1/1 - Step: 3193 - Loss: 6.470483779907227\n",
      "Epoch: 1/1 - Step: 3194 - Loss: 6.451418399810791\n",
      "Epoch: 1/1 - Step: 3195 - Loss: 6.540623188018799\n",
      "Epoch: 1/1 - Step: 3196 - Loss: 6.468115329742432\n",
      "Epoch: 1/1 - Step: 3197 - Loss: 6.5220537185668945\n",
      "Epoch: 1/1 - Step: 3198 - Loss: 6.47572660446167\n",
      "Epoch: 1/1 - Step: 3199 - Loss: 6.4206085205078125\n",
      "Epoch: 1/1 - Step: 3200 - Loss: 6.562195301055908\n",
      "Epoch: 1/1 - Step: 3201 - Loss: 6.393753528594971\n",
      "Epoch: 1/1 - Step: 3202 - Loss: 6.699350357055664\n",
      "Epoch: 1/1 - Step: 3203 - Loss: 6.494329929351807\n",
      "Epoch: 1/1 - Step: 3204 - Loss: 6.533241271972656\n",
      "Epoch: 1/1 - Step: 3205 - Loss: 6.44011926651001\n",
      "Epoch: 1/1 - Step: 3206 - Loss: 6.386369705200195\n",
      "Epoch: 1/1 - Step: 3207 - Loss: 6.361889362335205\n",
      "Epoch: 1/1 - Step: 3208 - Loss: 6.363729476928711\n",
      "Epoch: 1/1 - Step: 3209 - Loss: 6.305835247039795\n",
      "Epoch: 1/1 - Step: 3210 - Loss: 6.347203731536865\n",
      "Epoch: 1/1 - Step: 3211 - Loss: 6.374364852905273\n",
      "Epoch: 1/1 - Step: 3212 - Loss: 6.464998245239258\n",
      "Epoch: 1/1 - Step: 3213 - Loss: 6.350827217102051\n",
      "Epoch: 1/1 - Step: 3214 - Loss: 6.3252272605896\n",
      "Epoch: 1/1 - Step: 3215 - Loss: 6.3368120193481445\n",
      "Epoch: 1/1 - Step: 3216 - Loss: 6.346795082092285\n",
      "Epoch: 1/1 - Step: 3217 - Loss: 6.602035999298096\n",
      "Epoch: 1/1 - Step: 3218 - Loss: 6.609075546264648\n",
      "Epoch: 1/1 - Step: 3219 - Loss: 6.479442596435547\n",
      "Epoch: 1/1 - Step: 3220 - Loss: 6.4308085441589355\n",
      "Epoch: 1/1 - Step: 3221 - Loss: 6.43704080581665\n",
      "Epoch: 1/1 - Step: 3222 - Loss: 6.593591690063477\n",
      "Epoch: 1/1 - Step: 3223 - Loss: 6.41857385635376\n",
      "Epoch: 1/1 - Step: 3224 - Loss: 6.399168014526367\n",
      "Epoch: 1/1 - Step: 3225 - Loss: 6.347063064575195\n",
      "Epoch: 1/1 - Step: 3226 - Loss: 6.419038772583008\n",
      "Epoch: 1/1 - Step: 3227 - Loss: 6.457953929901123\n",
      "Epoch: 1/1 - Step: 3228 - Loss: 6.40225076675415\n",
      "Epoch: 1/1 - Step: 3229 - Loss: 6.308134078979492\n",
      "Epoch: 1/1 - Step: 3230 - Loss: 6.486975193023682\n",
      "Epoch: 1/1 - Step: 3231 - Loss: 6.604098796844482\n",
      "Epoch: 1/1 - Step: 3232 - Loss: 6.357813358306885\n",
      "Epoch: 1/1 - Step: 3233 - Loss: 6.357547283172607\n",
      "Epoch: 1/1 - Step: 3234 - Loss: 6.402794361114502\n",
      "Epoch: 1/1 - Step: 3235 - Loss: 6.352270603179932\n",
      "Epoch: 1/1 - Step: 3236 - Loss: 6.417025089263916\n",
      "Epoch: 1/1 - Step: 3237 - Loss: 6.637715816497803\n",
      "Epoch: 1/1 - Step: 3238 - Loss: 6.282588481903076\n",
      "Epoch: 1/1 - Step: 3239 - Loss: 6.541540145874023\n",
      "Epoch: 1/1 - Step: 3240 - Loss: 6.383352756500244\n",
      "Epoch: 1/1 - Step: 3241 - Loss: 6.449010372161865\n",
      "Epoch: 1/1 - Step: 3242 - Loss: 6.420191287994385\n",
      "Epoch: 1/1 - Step: 3243 - Loss: 6.2406511306762695\n",
      "Epoch: 1/1 - Step: 3244 - Loss: 6.425438404083252\n",
      "Epoch: 1/1 - Step: 3245 - Loss: 6.335611820220947\n",
      "Epoch: 1/1 - Step: 3246 - Loss: 6.507523536682129\n",
      "Epoch: 1/1 - Step: 3247 - Loss: 6.349702835083008\n",
      "Epoch: 1/1 - Step: 3248 - Loss: 6.325918674468994\n",
      "Epoch: 1/1 - Step: 3249 - Loss: 6.413656234741211\n",
      "Epoch: 1/1 - Step: 3250 - Loss: 6.559078216552734\n",
      "Epoch: 1/1 - Step: 3251 - Loss: 6.5406084060668945\n",
      "Epoch: 1/1 - Step: 3252 - Loss: 6.769035339355469\n",
      "Epoch: 1/1 - Step: 3253 - Loss: 6.570943832397461\n",
      "Epoch: 1/1 - Step: 3254 - Loss: 6.51062536239624\n",
      "Epoch: 1/1 - Step: 3255 - Loss: 6.395331382751465\n",
      "Epoch: 1/1 - Step: 3256 - Loss: 6.340296268463135\n",
      "Epoch: 1/1 - Step: 3257 - Loss: 6.255481719970703\n",
      "Epoch: 1/1 - Step: 3258 - Loss: 6.254512786865234\n",
      "Epoch: 1/1 - Step: 3259 - Loss: 6.44697904586792\n",
      "Epoch: 1/1 - Step: 3260 - Loss: 6.2734293937683105\n",
      "Epoch: 1/1 - Step: 3261 - Loss: 6.254847049713135\n",
      "Epoch: 1/1 - Step: 3262 - Loss: 6.332763195037842\n",
      "Epoch: 1/1 - Step: 3263 - Loss: 6.631097793579102\n",
      "Epoch: 1/1 - Step: 3264 - Loss: 6.303525447845459\n",
      "Epoch: 1/1 - Step: 3265 - Loss: 6.360124111175537\n",
      "Epoch: 1/1 - Step: 3266 - Loss: 6.412106990814209\n",
      "Epoch: 1/1 - Step: 3267 - Loss: 6.3951849937438965\n",
      "Epoch: 1/1 - Step: 3268 - Loss: 6.358701705932617\n",
      "Epoch: 1/1 - Step: 3269 - Loss: 6.319789886474609\n",
      "Epoch: 1/1 - Step: 3270 - Loss: 6.357486724853516\n",
      "Epoch: 1/1 - Step: 3271 - Loss: 6.373718738555908\n",
      "Epoch: 1/1 - Step: 3272 - Loss: 6.368853569030762\n",
      "Epoch: 1/1 - Step: 3273 - Loss: 6.351840496063232\n",
      "Epoch: 1/1 - Step: 3274 - Loss: 6.297204494476318\n",
      "Epoch: 1/1 - Step: 3275 - Loss: 6.47753381729126\n",
      "Epoch: 1/1 - Step: 3276 - Loss: 6.439640045166016\n",
      "Epoch: 1/1 - Step: 3277 - Loss: 6.427265167236328\n",
      "Epoch: 1/1 - Step: 3278 - Loss: 6.369293212890625\n",
      "Epoch: 1/1 - Step: 3279 - Loss: 6.392520427703857\n",
      "Epoch: 1/1 - Step: 3280 - Loss: 6.359434127807617\n",
      "Epoch: 1/1 - Step: 3281 - Loss: 6.2780022621154785\n",
      "Epoch: 1/1 - Step: 3282 - Loss: 6.310861587524414\n",
      "Epoch: 1/1 - Step: 3283 - Loss: 6.323768615722656\n",
      "Epoch: 1/1 - Step: 3284 - Loss: 6.348946571350098\n",
      "Epoch: 1/1 - Step: 3285 - Loss: 6.274415969848633\n",
      "Epoch: 1/1 - Step: 3286 - Loss: 6.307955265045166\n",
      "Epoch: 1/1 - Step: 3287 - Loss: 6.280550479888916\n",
      "Epoch: 1/1 - Step: 3288 - Loss: 6.203765869140625\n",
      "Epoch: 1/1 - Step: 3289 - Loss: 6.353866100311279\n",
      "Epoch: 1/1 - Step: 3290 - Loss: 6.324265480041504\n",
      "Epoch: 1/1 - Step: 3291 - Loss: 6.4730682373046875\n",
      "Epoch: 1/1 - Step: 3292 - Loss: 6.608895301818848\n",
      "Epoch: 1/1 - Step: 3293 - Loss: 6.325734615325928\n",
      "Epoch: 1/1 - Step: 3294 - Loss: 6.32895565032959\n",
      "Epoch: 1/1 - Step: 3295 - Loss: 6.323995113372803\n",
      "Epoch: 1/1 - Step: 3296 - Loss: 6.404759883880615\n",
      "Epoch: 1/1 - Step: 3297 - Loss: 6.425743103027344\n",
      "Epoch: 1/1 - Step: 3298 - Loss: 6.247395038604736\n",
      "Epoch: 1/1 - Step: 3299 - Loss: 6.3605194091796875\n",
      "Epoch: 1/1 - Step: 3300 - Loss: 6.367067337036133\n",
      "Epoch: 1/1 - Step: 3301 - Loss: 6.356908321380615\n",
      "Epoch: 1/1 - Step: 3302 - Loss: 6.2786712646484375\n",
      "Epoch: 1/1 - Step: 3303 - Loss: 6.30188512802124\n",
      "Epoch: 1/1 - Step: 3304 - Loss: 7.899901390075684\n",
      "Epoch: 1/1 - Step: 3305 - Loss: 8.026430130004883\n",
      "Epoch: 1/1 - Step: 3306 - Loss: 6.454432010650635\n",
      "Epoch: 1/1 - Step: 3307 - Loss: 6.266921043395996\n",
      "Epoch: 1/1 - Step: 3308 - Loss: 6.19592809677124\n",
      "Epoch: 1/1 - Step: 3309 - Loss: 6.354503631591797\n",
      "Epoch: 1/1 - Step: 3310 - Loss: 6.569512844085693\n",
      "Epoch: 1/1 - Step: 3311 - Loss: 6.320661544799805\n",
      "Epoch: 1/1 - Step: 3312 - Loss: 6.406121730804443\n",
      "Epoch: 1/1 - Step: 3313 - Loss: 6.3650922775268555\n",
      "Epoch: 1/1 - Step: 3314 - Loss: 6.579249382019043\n",
      "Epoch: 1/1 - Step: 3315 - Loss: 6.58072566986084\n",
      "Epoch: 1/1 - Step: 3316 - Loss: 6.505895137786865\n",
      "Epoch: 1/1 - Step: 3317 - Loss: 6.3035359382629395\n",
      "Epoch: 1/1 - Step: 3318 - Loss: 6.23078727722168\n",
      "Epoch: 1/1 - Step: 3319 - Loss: 6.421760082244873\n",
      "Epoch: 1/1 - Step: 3320 - Loss: 6.489527225494385\n",
      "Epoch: 1/1 - Step: 3321 - Loss: 6.453904628753662\n",
      "Epoch: 1/1 - Step: 3322 - Loss: 6.265681743621826\n",
      "Epoch: 1/1 - Step: 3323 - Loss: 6.1175055503845215\n",
      "Epoch: 1/1 - Step: 3324 - Loss: 6.138937473297119\n",
      "Epoch: 1/1 - Step: 3325 - Loss: 6.5587921142578125\n",
      "Epoch: 1/1 - Step: 3326 - Loss: 6.270804405212402\n",
      "Epoch: 1/1 - Step: 3327 - Loss: 6.268990993499756\n",
      "Epoch: 1/1 - Step: 3328 - Loss: 6.432289123535156\n",
      "Epoch: 1/1 - Step: 3329 - Loss: 6.3648600578308105\n",
      "Epoch: 1/1 - Step: 3330 - Loss: 6.314738750457764\n",
      "Epoch: 1/1 - Step: 3331 - Loss: 6.335025787353516\n",
      "Epoch: 1/1 - Step: 3332 - Loss: 6.497029781341553\n",
      "Epoch: 1/1 - Step: 3333 - Loss: 6.271471977233887\n",
      "Epoch: 1/1 - Step: 3334 - Loss: 6.315518856048584\n",
      "Epoch: 1/1 - Step: 3335 - Loss: 6.331038951873779\n",
      "Epoch: 1/1 - Step: 3336 - Loss: 6.305713653564453\n",
      "Epoch: 1/1 - Step: 3337 - Loss: 6.4235734939575195\n",
      "Epoch: 1/1 - Step: 3338 - Loss: 6.363158702850342\n",
      "Epoch: 1/1 - Step: 3339 - Loss: 6.603931427001953\n",
      "Epoch: 1/1 - Step: 3340 - Loss: 6.282365322113037\n",
      "Epoch: 1/1 - Step: 3341 - Loss: 6.2778143882751465\n",
      "Epoch: 1/1 - Step: 3342 - Loss: 6.33028507232666\n",
      "Epoch: 1/1 - Step: 3343 - Loss: 6.357422828674316\n",
      "Epoch: 1/1 - Step: 3344 - Loss: 6.341845512390137\n",
      "Epoch: 1/1 - Step: 3345 - Loss: 6.279234886169434\n",
      "Epoch: 1/1 - Step: 3346 - Loss: 6.2493510246276855\n",
      "Epoch: 1/1 - Step: 3347 - Loss: 6.409937381744385\n",
      "Epoch: 1/1 - Step: 3348 - Loss: 6.4273576736450195\n",
      "Epoch: 1/1 - Step: 3349 - Loss: 6.230575084686279\n",
      "Epoch: 1/1 - Step: 3350 - Loss: 6.141116142272949\n",
      "Epoch: 1/1 - Step: 3351 - Loss: 6.246959209442139\n",
      "Epoch: 1/1 - Step: 3352 - Loss: 6.318599700927734\n",
      "Epoch: 1/1 - Step: 3353 - Loss: 6.23552131652832\n",
      "Epoch: 1/1 - Step: 3354 - Loss: 7.9913787841796875\n",
      "Epoch: 1/1 - Step: 3355 - Loss: 7.892881393432617\n",
      "Epoch: 1/1 - Step: 3356 - Loss: 6.481383323669434\n",
      "Epoch: 1/1 - Step: 3357 - Loss: 6.433967590332031\n",
      "Epoch: 1/1 - Step: 3358 - Loss: 6.43634557723999\n",
      "Epoch: 1/1 - Step: 3359 - Loss: 6.328031539916992\n",
      "Epoch: 1/1 - Step: 3360 - Loss: 6.3628010749816895\n",
      "Epoch: 1/1 - Step: 3361 - Loss: 6.364269256591797\n",
      "Epoch: 1/1 - Step: 3362 - Loss: 6.533145904541016\n",
      "Epoch: 1/1 - Step: 3363 - Loss: 6.210525989532471\n",
      "Epoch: 1/1 - Step: 3364 - Loss: 6.438111305236816\n",
      "Epoch: 1/1 - Step: 3365 - Loss: 6.463048458099365\n",
      "Epoch: 1/1 - Step: 3366 - Loss: 6.420279026031494\n",
      "Epoch: 1/1 - Step: 3367 - Loss: 6.275391101837158\n",
      "Epoch: 1/1 - Step: 3368 - Loss: 6.199279308319092\n",
      "Epoch: 1/1 - Step: 3369 - Loss: 6.295196056365967\n",
      "Epoch: 1/1 - Step: 3370 - Loss: 6.317807674407959\n",
      "Epoch: 1/1 - Step: 3371 - Loss: 6.412055015563965\n",
      "Epoch: 1/1 - Step: 3372 - Loss: 6.301019668579102\n",
      "Epoch: 1/1 - Step: 3373 - Loss: 6.356645584106445\n",
      "Epoch: 1/1 - Step: 3374 - Loss: 6.334502696990967\n",
      "Epoch: 1/1 - Step: 3375 - Loss: 6.231531620025635\n",
      "Epoch: 1/1 - Step: 3376 - Loss: 6.238359451293945\n",
      "Epoch: 1/1 - Step: 3377 - Loss: 6.405171871185303\n",
      "Epoch: 1/1 - Step: 3378 - Loss: 6.299027919769287\n",
      "Epoch: 1/1 - Step: 3379 - Loss: 6.18773889541626\n",
      "Epoch: 1/1 - Step: 3380 - Loss: 6.393244743347168\n",
      "Epoch: 1/1 - Step: 3381 - Loss: 6.452893257141113\n",
      "Epoch: 1/1 - Step: 3382 - Loss: 6.1969170570373535\n",
      "Epoch: 1/1 - Step: 3383 - Loss: 6.252472877502441\n",
      "Epoch: 1/1 - Step: 3384 - Loss: 6.514739513397217\n",
      "Epoch: 1/1 - Step: 3385 - Loss: 6.269674301147461\n",
      "Epoch: 1/1 - Step: 3386 - Loss: 6.273208141326904\n",
      "Epoch: 1/1 - Step: 3387 - Loss: 6.218722343444824\n",
      "Epoch: 1/1 - Step: 3388 - Loss: 6.354521751403809\n",
      "Epoch: 1/1 - Step: 3389 - Loss: 7.770534038543701\n",
      "Epoch: 1/1 - Step: 3390 - Loss: 7.471684455871582\n",
      "Epoch: 1/1 - Step: 3391 - Loss: 6.371638774871826\n",
      "Epoch: 1/1 - Step: 3392 - Loss: 6.306136131286621\n",
      "Epoch: 1/1 - Step: 3393 - Loss: 6.489182949066162\n",
      "Epoch: 1/1 - Step: 3394 - Loss: 6.527764320373535\n",
      "Epoch: 1/1 - Step: 3395 - Loss: 6.476048469543457\n",
      "Epoch: 1/1 - Step: 3396 - Loss: 6.50739860534668\n",
      "Epoch: 1/1 - Step: 3397 - Loss: 6.500584602355957\n",
      "Epoch: 1/1 - Step: 3398 - Loss: 6.628304958343506\n",
      "Epoch: 1/1 - Step: 3399 - Loss: 6.55463171005249\n",
      "Epoch: 1/1 - Step: 3400 - Loss: 6.4754180908203125\n",
      "Epoch: 1/1 - Step: 3401 - Loss: 6.569642066955566\n",
      "Epoch: 1/1 - Step: 3402 - Loss: 6.6070685386657715\n",
      "Epoch: 1/1 - Step: 3403 - Loss: 6.498314380645752\n",
      "Epoch: 1/1 - Step: 3404 - Loss: 6.537945747375488\n",
      "Epoch: 1/1 - Step: 3405 - Loss: 6.5018534660339355\n",
      "Epoch: 1/1 - Step: 3406 - Loss: 6.6314849853515625\n",
      "Epoch: 1/1 - Step: 3407 - Loss: 6.482484817504883\n",
      "Epoch: 1/1 - Step: 3408 - Loss: 6.512610912322998\n",
      "Epoch: 1/1 - Step: 3409 - Loss: 6.489467620849609\n",
      "Epoch: 1/1 - Step: 3410 - Loss: 6.325371742248535\n",
      "Epoch: 1/1 - Step: 3411 - Loss: 6.2290449142456055\n",
      "Epoch: 1/1 - Step: 3412 - Loss: 6.297852993011475\n",
      "Epoch: 1/1 - Step: 3413 - Loss: 6.315278053283691\n",
      "Epoch: 1/1 - Step: 3414 - Loss: 6.285548686981201\n",
      "Epoch: 1/1 - Step: 3415 - Loss: 8.52928352355957\n",
      "Epoch: 1/1 - Step: 3416 - Loss: 8.789249420166016\n",
      "Epoch: 1/1 - Step: 3417 - Loss: 8.831907272338867\n",
      "Epoch: 1/1 - Step: 3418 - Loss: 6.599328994750977\n",
      "Epoch: 1/1 - Step: 3419 - Loss: 6.505694389343262\n",
      "Epoch: 1/1 - Step: 3420 - Loss: 6.430957317352295\n",
      "Epoch: 1/1 - Step: 3421 - Loss: 6.52769660949707\n",
      "Epoch: 1/1 - Step: 3422 - Loss: 6.574861526489258\n",
      "Epoch: 1/1 - Step: 3423 - Loss: 6.5196428298950195\n",
      "Epoch: 1/1 - Step: 3424 - Loss: 6.623898506164551\n",
      "Epoch: 1/1 - Step: 3425 - Loss: 6.260298252105713\n",
      "Epoch: 1/1 - Step: 3426 - Loss: 6.129383563995361\n",
      "Epoch: 1/1 - Step: 3427 - Loss: 6.345187664031982\n",
      "Epoch: 1/1 - Step: 3428 - Loss: 6.43340539932251\n",
      "Epoch: 1/1 - Step: 3429 - Loss: 6.538946628570557\n",
      "Epoch: 1/1 - Step: 3430 - Loss: 6.418932914733887\n",
      "Epoch: 1/1 - Step: 3431 - Loss: 6.5541510581970215\n",
      "Epoch: 1/1 - Step: 3432 - Loss: 6.596716403961182\n",
      "Epoch: 1/1 - Step: 3433 - Loss: 6.531277656555176\n",
      "Epoch: 1/1 - Step: 3434 - Loss: 6.55939245223999\n",
      "Epoch: 1/1 - Step: 3435 - Loss: 6.5736494064331055\n",
      "Epoch: 1/1 - Step: 3436 - Loss: 6.554278373718262\n",
      "Epoch: 1/1 - Step: 3437 - Loss: 6.561036586761475\n",
      "Epoch: 1/1 - Step: 3438 - Loss: 6.538280963897705\n",
      "Epoch: 1/1 - Step: 3439 - Loss: 6.528109073638916\n",
      "Epoch: 1/1 - Step: 3440 - Loss: 6.426990509033203\n",
      "Epoch: 1/1 - Step: 3441 - Loss: 6.493366241455078\n",
      "Epoch: 1/1 - Step: 3442 - Loss: 7.978250503540039\n",
      "Epoch: 1/1 - Step: 3443 - Loss: 6.400094985961914\n",
      "Epoch: 1/1 - Step: 3444 - Loss: 6.4913835525512695\n",
      "Epoch: 1/1 - Step: 3445 - Loss: 6.467987537384033\n",
      "Epoch: 1/1 - Step: 3446 - Loss: 6.458505153656006\n",
      "Epoch: 1/1 - Step: 3447 - Loss: 6.355110168457031\n",
      "Epoch: 1/1 - Step: 3448 - Loss: 6.413539409637451\n",
      "Epoch: 1/1 - Step: 3449 - Loss: 6.485961437225342\n",
      "Epoch: 1/1 - Step: 3450 - Loss: 6.444797515869141\n",
      "Epoch: 1/1 - Step: 3451 - Loss: 6.40797758102417\n",
      "Epoch: 1/1 - Step: 3452 - Loss: 6.477471351623535\n",
      "Epoch: 1/1 - Step: 3453 - Loss: 6.439398765563965\n",
      "Epoch: 1/1 - Step: 3454 - Loss: 6.655772686004639\n",
      "Epoch: 1/1 - Step: 3455 - Loss: 6.605945587158203\n",
      "Epoch: 1/1 - Step: 3456 - Loss: 6.3570332527160645\n",
      "Epoch: 1/1 - Step: 3457 - Loss: 6.442224979400635\n",
      "Epoch: 1/1 - Step: 3458 - Loss: 6.316936492919922\n",
      "Epoch: 1/1 - Step: 3459 - Loss: 6.456820964813232\n",
      "Epoch: 1/1 - Step: 3460 - Loss: 6.370898723602295\n",
      "Epoch: 1/1 - Step: 3461 - Loss: 6.4277424812316895\n",
      "Epoch: 1/1 - Step: 3462 - Loss: 6.501380920410156\n",
      "Epoch: 1/1 - Step: 3463 - Loss: 6.444528579711914\n",
      "Epoch: 1/1 - Step: 3464 - Loss: 6.534800052642822\n",
      "Epoch: 1/1 - Step: 3465 - Loss: 6.437069416046143\n",
      "Epoch: 1/1 - Step: 3466 - Loss: 6.4214653968811035\n",
      "Epoch: 1/1 - Step: 3467 - Loss: 6.487182140350342\n",
      "Epoch: 1/1 - Step: 3468 - Loss: 6.433676719665527\n",
      "Epoch: 1/1 - Step: 3469 - Loss: 6.493041038513184\n",
      "Epoch: 1/1 - Step: 3470 - Loss: 6.394314289093018\n",
      "Epoch: 1/1 - Step: 3471 - Loss: 6.5804123878479\n",
      "Epoch: 1/1 - Step: 3472 - Loss: 6.7477006912231445\n",
      "Epoch: 1/1 - Step: 3473 - Loss: 6.295324325561523\n",
      "Epoch: 1/1 - Step: 3474 - Loss: 6.260904788970947\n",
      "Epoch: 1/1 - Step: 3475 - Loss: 6.219301700592041\n",
      "Epoch: 1/1 - Step: 3476 - Loss: 6.401429653167725\n",
      "Epoch: 1/1 - Step: 3477 - Loss: 6.255450248718262\n",
      "Epoch: 1/1 - Step: 3478 - Loss: 6.44770622253418\n",
      "Epoch: 1/1 - Step: 3479 - Loss: 6.364826679229736\n",
      "Epoch: 1/1 - Step: 3480 - Loss: 6.41929292678833\n",
      "Epoch: 1/1 - Step: 3481 - Loss: 6.438418865203857\n",
      "Epoch: 1/1 - Step: 3482 - Loss: 6.619410514831543\n",
      "Epoch: 1/1 - Step: 3483 - Loss: 6.629174709320068\n",
      "Epoch: 1/1 - Step: 3484 - Loss: 6.477184772491455\n",
      "Epoch: 1/1 - Step: 3485 - Loss: 6.132275104522705\n",
      "Epoch: 1/1 - Step: 3486 - Loss: 6.196138381958008\n",
      "Epoch: 1/1 - Step: 3487 - Loss: 6.101418972015381\n",
      "Epoch: 1/1 - Step: 3488 - Loss: 6.253478050231934\n",
      "Epoch: 1/1 - Step: 3489 - Loss: 6.322679042816162\n",
      "Epoch: 1/1 - Step: 3490 - Loss: 6.612812519073486\n",
      "Epoch: 1/1 - Step: 3491 - Loss: 6.553250312805176\n",
      "Epoch: 1/1 - Step: 3492 - Loss: 6.620371341705322\n",
      "Epoch: 1/1 - Step: 3493 - Loss: 6.564449310302734\n",
      "Epoch: 1/1 - Step: 3494 - Loss: 6.362929821014404\n",
      "Epoch: 1/1 - Step: 3495 - Loss: 6.544310092926025\n",
      "Epoch: 1/1 - Step: 3496 - Loss: 6.554988384246826\n",
      "Epoch: 1/1 - Step: 3497 - Loss: 6.6268157958984375\n",
      "Epoch: 1/1 - Step: 3498 - Loss: 6.570674419403076\n",
      "Epoch: 1/1 - Step: 3499 - Loss: 6.393346309661865\n",
      "Epoch: 1/1 - Step: 3500 - Loss: 6.386621952056885\n",
      "Epoch: 1/1 - Step: 3501 - Loss: 6.537535190582275\n",
      "Epoch: 1/1 - Step: 3502 - Loss: 6.569620609283447\n",
      "Epoch: 1/1 - Step: 3503 - Loss: 6.561926364898682\n",
      "Epoch: 1/1 - Step: 3504 - Loss: 6.428691864013672\n",
      "Epoch: 1/1 - Step: 3505 - Loss: 6.523647308349609\n",
      "Epoch: 1/1 - Step: 3506 - Loss: 6.347745418548584\n",
      "Epoch: 1/1 - Step: 3507 - Loss: 6.44705057144165\n",
      "Epoch: 1/1 - Step: 3508 - Loss: 6.383360862731934\n",
      "Epoch: 1/1 - Step: 3509 - Loss: 6.503382205963135\n",
      "Epoch: 1/1 - Step: 3510 - Loss: 6.520627021789551\n",
      "Epoch: 1/1 - Step: 3511 - Loss: 6.611491680145264\n",
      "Epoch: 1/1 - Step: 3512 - Loss: 6.3824310302734375\n",
      "Epoch: 1/1 - Step: 3513 - Loss: 7.267484188079834\n",
      "Epoch: 1/1 - Step: 3514 - Loss: 8.504587173461914\n",
      "Epoch: 1/1 - Step: 3515 - Loss: 8.867316246032715\n",
      "Epoch: 1/1 - Step: 3516 - Loss: 7.311408519744873\n",
      "Epoch: 1/1 - Step: 3517 - Loss: 6.502511978149414\n",
      "Epoch: 1/1 - Step: 3518 - Loss: 6.438578128814697\n",
      "Epoch: 1/1 - Step: 3519 - Loss: 6.380685329437256\n",
      "Epoch: 1/1 - Step: 3520 - Loss: 6.5941243171691895\n",
      "Epoch: 1/1 - Step: 3521 - Loss: 6.465014934539795\n",
      "Epoch: 1/1 - Step: 3522 - Loss: 6.4274773597717285\n",
      "Epoch: 1/1 - Step: 3523 - Loss: 6.47041654586792\n",
      "Epoch: 1/1 - Step: 3524 - Loss: 6.52212381362915\n",
      "Epoch: 1/1 - Step: 3525 - Loss: 6.486810207366943\n",
      "Epoch: 1/1 - Step: 3526 - Loss: 6.400892734527588\n",
      "Epoch: 1/1 - Step: 3527 - Loss: 6.437093734741211\n",
      "Epoch: 1/1 - Step: 3528 - Loss: 6.476484298706055\n",
      "Epoch: 1/1 - Step: 3529 - Loss: 6.491011619567871\n",
      "Epoch: 1/1 - Step: 3530 - Loss: 6.355907440185547\n",
      "Epoch: 1/1 - Step: 3531 - Loss: 6.60799503326416\n",
      "Epoch: 1/1 - Step: 3532 - Loss: 6.4611029624938965\n",
      "Epoch: 1/1 - Step: 3533 - Loss: 6.502717018127441\n",
      "Epoch: 1/1 - Step: 3534 - Loss: 6.37835168838501\n",
      "Epoch: 1/1 - Step: 3535 - Loss: 6.444275379180908\n",
      "Epoch: 1/1 - Step: 3536 - Loss: 6.438960552215576\n",
      "Epoch: 1/1 - Step: 3537 - Loss: 6.48724365234375\n",
      "Epoch: 1/1 - Step: 3538 - Loss: 6.487010478973389\n",
      "Epoch: 1/1 - Step: 3539 - Loss: 6.432981491088867\n",
      "Epoch: 1/1 - Step: 3540 - Loss: 6.419184684753418\n",
      "Epoch: 1/1 - Step: 3541 - Loss: 6.4461894035339355\n",
      "Epoch: 1/1 - Step: 3542 - Loss: 6.474641799926758\n",
      "Epoch: 1/1 - Step: 3543 - Loss: 6.408212184906006\n",
      "Epoch: 1/1 - Step: 3544 - Loss: 6.624948024749756\n",
      "Epoch: 1/1 - Step: 3545 - Loss: 6.502975940704346\n",
      "Epoch: 1/1 - Step: 3546 - Loss: 6.508425235748291\n",
      "Epoch: 1/1 - Step: 3547 - Loss: 6.386873245239258\n",
      "Epoch: 1/1 - Step: 3548 - Loss: 6.292521953582764\n",
      "Epoch: 1/1 - Step: 3549 - Loss: 6.325536727905273\n",
      "Epoch: 1/1 - Step: 3550 - Loss: 6.350004196166992\n",
      "Epoch: 1/1 - Step: 3551 - Loss: 6.245890140533447\n",
      "Epoch: 1/1 - Step: 3552 - Loss: 6.304579257965088\n",
      "Epoch: 1/1 - Step: 3553 - Loss: 6.420216083526611\n",
      "Epoch: 1/1 - Step: 3554 - Loss: 6.3490824699401855\n",
      "Epoch: 1/1 - Step: 3555 - Loss: 6.331670761108398\n",
      "Epoch: 1/1 - Step: 3556 - Loss: 6.267947196960449\n",
      "Epoch: 1/1 - Step: 3557 - Loss: 6.300151348114014\n",
      "Epoch: 1/1 - Step: 3558 - Loss: 6.409351825714111\n",
      "Epoch: 1/1 - Step: 3559 - Loss: 6.620331764221191\n",
      "Epoch: 1/1 - Step: 3560 - Loss: 6.513071537017822\n",
      "Epoch: 1/1 - Step: 3561 - Loss: 6.363215446472168\n",
      "Epoch: 1/1 - Step: 3562 - Loss: 6.478043556213379\n",
      "Epoch: 1/1 - Step: 3563 - Loss: 6.435537338256836\n",
      "Epoch: 1/1 - Step: 3564 - Loss: 6.4921722412109375\n",
      "Epoch: 1/1 - Step: 3565 - Loss: 6.376720905303955\n",
      "Epoch: 1/1 - Step: 3566 - Loss: 6.36849308013916\n",
      "Epoch: 1/1 - Step: 3567 - Loss: 6.3023223876953125\n",
      "Epoch: 1/1 - Step: 3568 - Loss: 6.412714004516602\n",
      "Epoch: 1/1 - Step: 3569 - Loss: 6.448248386383057\n",
      "Epoch: 1/1 - Step: 3570 - Loss: 6.293519020080566\n",
      "Epoch: 1/1 - Step: 3571 - Loss: 6.290305137634277\n",
      "Epoch: 1/1 - Step: 3572 - Loss: 6.509875774383545\n",
      "Epoch: 1/1 - Step: 3573 - Loss: 6.532103538513184\n",
      "Epoch: 1/1 - Step: 3574 - Loss: 6.278359889984131\n",
      "Epoch: 1/1 - Step: 3575 - Loss: 6.350830078125\n",
      "Epoch: 1/1 - Step: 3576 - Loss: 6.354721546173096\n",
      "Epoch: 1/1 - Step: 3577 - Loss: 6.372193336486816\n",
      "Epoch: 1/1 - Step: 3578 - Loss: 6.4418792724609375\n",
      "Epoch: 1/1 - Step: 3579 - Loss: 6.518226146697998\n",
      "Epoch: 1/1 - Step: 3580 - Loss: 6.2562174797058105\n",
      "Epoch: 1/1 - Step: 3581 - Loss: 6.576154708862305\n",
      "Epoch: 1/1 - Step: 3582 - Loss: 6.261600971221924\n",
      "Epoch: 1/1 - Step: 3583 - Loss: 6.448268890380859\n",
      "Epoch: 1/1 - Step: 3584 - Loss: 6.315942764282227\n",
      "Epoch: 1/1 - Step: 3585 - Loss: 6.208174228668213\n",
      "Epoch: 1/1 - Step: 3586 - Loss: 6.391465187072754\n",
      "Epoch: 1/1 - Step: 3587 - Loss: 6.430343151092529\n",
      "Epoch: 1/1 - Step: 3588 - Loss: 6.379030704498291\n",
      "Epoch: 1/1 - Step: 3589 - Loss: 6.274417400360107\n",
      "Epoch: 1/1 - Step: 3590 - Loss: 6.379199504852295\n",
      "Epoch: 1/1 - Step: 3591 - Loss: 6.3258056640625\n",
      "Epoch: 1/1 - Step: 3592 - Loss: 6.624155044555664\n",
      "Epoch: 1/1 - Step: 3593 - Loss: 6.531210899353027\n",
      "Epoch: 1/1 - Step: 3594 - Loss: 6.666572570800781\n",
      "Epoch: 1/1 - Step: 3595 - Loss: 6.587430000305176\n",
      "Epoch: 1/1 - Step: 3596 - Loss: 6.401891231536865\n",
      "Epoch: 1/1 - Step: 3597 - Loss: 6.39564847946167\n",
      "Epoch: 1/1 - Step: 3598 - Loss: 6.228489875793457\n",
      "Epoch: 1/1 - Step: 3599 - Loss: 6.221065044403076\n",
      "Epoch: 1/1 - Step: 3600 - Loss: 6.2645368576049805\n",
      "Epoch: 1/1 - Step: 3601 - Loss: 6.396320819854736\n",
      "Epoch: 1/1 - Step: 3602 - Loss: 6.2151336669921875\n",
      "Epoch: 1/1 - Step: 3603 - Loss: 6.2041778564453125\n",
      "Epoch: 1/1 - Step: 3604 - Loss: 6.358710289001465\n",
      "Epoch: 1/1 - Step: 3605 - Loss: 6.629227638244629\n",
      "Epoch: 1/1 - Step: 3606 - Loss: 6.17968225479126\n",
      "Epoch: 1/1 - Step: 3607 - Loss: 6.346665859222412\n",
      "Epoch: 1/1 - Step: 3608 - Loss: 6.331882953643799\n",
      "Epoch: 1/1 - Step: 3609 - Loss: 6.387477397918701\n",
      "Epoch: 1/1 - Step: 3610 - Loss: 6.3363752365112305\n",
      "Epoch: 1/1 - Step: 3611 - Loss: 6.24157190322876\n",
      "Epoch: 1/1 - Step: 3612 - Loss: 6.387760162353516\n",
      "Epoch: 1/1 - Step: 3613 - Loss: 6.318305015563965\n",
      "Epoch: 1/1 - Step: 3614 - Loss: 6.336184024810791\n",
      "Epoch: 1/1 - Step: 3615 - Loss: 6.29112434387207\n",
      "Epoch: 1/1 - Step: 3616 - Loss: 6.297610282897949\n",
      "Epoch: 1/1 - Step: 3617 - Loss: 6.4506072998046875\n",
      "Epoch: 1/1 - Step: 3618 - Loss: 6.456160068511963\n",
      "Epoch: 1/1 - Step: 3619 - Loss: 6.299599647521973\n",
      "Epoch: 1/1 - Step: 3620 - Loss: 6.370741844177246\n",
      "Epoch: 1/1 - Step: 3621 - Loss: 6.3293776512146\n",
      "Epoch: 1/1 - Step: 3622 - Loss: 6.307456016540527\n",
      "Epoch: 1/1 - Step: 3623 - Loss: 6.224036693572998\n",
      "Epoch: 1/1 - Step: 3624 - Loss: 6.245972633361816\n",
      "Epoch: 1/1 - Step: 3625 - Loss: 6.343972682952881\n",
      "Epoch: 1/1 - Step: 3626 - Loss: 6.284625053405762\n",
      "Epoch: 1/1 - Step: 3627 - Loss: 6.281567573547363\n",
      "Epoch: 1/1 - Step: 3628 - Loss: 6.303755283355713\n",
      "Epoch: 1/1 - Step: 3629 - Loss: 6.23144006729126\n",
      "Epoch: 1/1 - Step: 3630 - Loss: 6.170596599578857\n",
      "Epoch: 1/1 - Step: 3631 - Loss: 6.310234546661377\n",
      "Epoch: 1/1 - Step: 3632 - Loss: 6.328395843505859\n",
      "Epoch: 1/1 - Step: 3633 - Loss: 6.503978729248047\n",
      "Epoch: 1/1 - Step: 3634 - Loss: 6.4754815101623535\n",
      "Epoch: 1/1 - Step: 3635 - Loss: 6.298606872558594\n",
      "Epoch: 1/1 - Step: 3636 - Loss: 6.266857147216797\n",
      "Epoch: 1/1 - Step: 3637 - Loss: 6.335789203643799\n",
      "Epoch: 1/1 - Step: 3638 - Loss: 6.38193416595459\n",
      "Epoch: 1/1 - Step: 3639 - Loss: 6.333017826080322\n",
      "Epoch: 1/1 - Step: 3640 - Loss: 6.2495880126953125\n",
      "Epoch: 1/1 - Step: 3641 - Loss: 6.322145938873291\n",
      "Epoch: 1/1 - Step: 3642 - Loss: 6.316771507263184\n",
      "Epoch: 1/1 - Step: 3643 - Loss: 6.348407745361328\n",
      "Epoch: 1/1 - Step: 3644 - Loss: 6.251021862030029\n",
      "Epoch: 1/1 - Step: 3645 - Loss: 6.295464992523193\n",
      "Epoch: 1/1 - Step: 3646 - Loss: 8.324384689331055\n",
      "Epoch: 1/1 - Step: 3647 - Loss: 7.471997261047363\n",
      "Epoch: 1/1 - Step: 3648 - Loss: 6.423713684082031\n",
      "Epoch: 1/1 - Step: 3649 - Loss: 6.180830478668213\n",
      "Epoch: 1/1 - Step: 3650 - Loss: 6.232246398925781\n",
      "Epoch: 1/1 - Step: 3651 - Loss: 6.313785552978516\n",
      "Epoch: 1/1 - Step: 3652 - Loss: 6.531126022338867\n",
      "Epoch: 1/1 - Step: 3653 - Loss: 6.331887722015381\n",
      "Epoch: 1/1 - Step: 3654 - Loss: 6.323841571807861\n",
      "Epoch: 1/1 - Step: 3655 - Loss: 6.416218280792236\n",
      "Epoch: 1/1 - Step: 3656 - Loss: 6.488270282745361\n",
      "Epoch: 1/1 - Step: 3657 - Loss: 6.575438976287842\n",
      "Epoch: 1/1 - Step: 3658 - Loss: 6.41995906829834\n",
      "Epoch: 1/1 - Step: 3659 - Loss: 6.250600337982178\n",
      "Epoch: 1/1 - Step: 3660 - Loss: 6.2354936599731445\n",
      "Epoch: 1/1 - Step: 3661 - Loss: 6.4191999435424805\n",
      "Epoch: 1/1 - Step: 3662 - Loss: 6.557642459869385\n",
      "Epoch: 1/1 - Step: 3663 - Loss: 6.31984281539917\n",
      "Epoch: 1/1 - Step: 3664 - Loss: 6.198999881744385\n",
      "Epoch: 1/1 - Step: 3665 - Loss: 6.062082767486572\n",
      "Epoch: 1/1 - Step: 3666 - Loss: 6.184640407562256\n",
      "Epoch: 1/1 - Step: 3667 - Loss: 6.526112079620361\n",
      "Epoch: 1/1 - Step: 3668 - Loss: 6.201720714569092\n",
      "Epoch: 1/1 - Step: 3669 - Loss: 6.293753623962402\n",
      "Epoch: 1/1 - Step: 3670 - Loss: 6.369443416595459\n",
      "Epoch: 1/1 - Step: 3671 - Loss: 6.31744384765625\n",
      "Epoch: 1/1 - Step: 3672 - Loss: 6.272298812866211\n",
      "Epoch: 1/1 - Step: 3673 - Loss: 6.420821666717529\n",
      "Epoch: 1/1 - Step: 3674 - Loss: 6.353155136108398\n",
      "Epoch: 1/1 - Step: 3675 - Loss: 6.229272842407227\n",
      "Epoch: 1/1 - Step: 3676 - Loss: 6.310030937194824\n",
      "Epoch: 1/1 - Step: 3677 - Loss: 6.3197832107543945\n",
      "Epoch: 1/1 - Step: 3678 - Loss: 6.27344274520874\n",
      "Epoch: 1/1 - Step: 3679 - Loss: 6.399840354919434\n",
      "Epoch: 1/1 - Step: 3680 - Loss: 6.33014440536499\n",
      "Epoch: 1/1 - Step: 3681 - Loss: 6.567619800567627\n",
      "Epoch: 1/1 - Step: 3682 - Loss: 6.187396049499512\n",
      "Epoch: 1/1 - Step: 3683 - Loss: 6.280919551849365\n",
      "Epoch: 1/1 - Step: 3684 - Loss: 6.28317403793335\n",
      "Epoch: 1/1 - Step: 3685 - Loss: 6.2840423583984375\n",
      "Epoch: 1/1 - Step: 3686 - Loss: 6.281177043914795\n",
      "Epoch: 1/1 - Step: 3687 - Loss: 6.329727649688721\n",
      "Epoch: 1/1 - Step: 3688 - Loss: 6.196619987487793\n",
      "Epoch: 1/1 - Step: 3689 - Loss: 6.3173604011535645\n",
      "Epoch: 1/1 - Step: 3690 - Loss: 6.420947551727295\n",
      "Epoch: 1/1 - Step: 3691 - Loss: 6.174657344818115\n",
      "Epoch: 1/1 - Step: 3692 - Loss: 6.139559745788574\n",
      "Epoch: 1/1 - Step: 3693 - Loss: 6.2199835777282715\n",
      "Epoch: 1/1 - Step: 3694 - Loss: 6.304867267608643\n",
      "Epoch: 1/1 - Step: 3695 - Loss: 6.19948673248291\n",
      "Epoch: 1/1 - Step: 3696 - Loss: 8.359278678894043\n",
      "Epoch: 1/1 - Step: 3697 - Loss: 7.373005390167236\n",
      "Epoch: 1/1 - Step: 3698 - Loss: 6.488379955291748\n",
      "Epoch: 1/1 - Step: 3699 - Loss: 6.4237518310546875\n",
      "Epoch: 1/1 - Step: 3700 - Loss: 6.3834428787231445\n",
      "Epoch: 1/1 - Step: 3701 - Loss: 6.312936305999756\n",
      "Epoch: 1/1 - Step: 3702 - Loss: 6.355806350708008\n",
      "Epoch: 1/1 - Step: 3703 - Loss: 6.288609027862549\n",
      "Epoch: 1/1 - Step: 3704 - Loss: 6.52363920211792\n",
      "Epoch: 1/1 - Step: 3705 - Loss: 6.2160964012146\n",
      "Epoch: 1/1 - Step: 3706 - Loss: 6.414807319641113\n",
      "Epoch: 1/1 - Step: 3707 - Loss: 6.403671741485596\n",
      "Epoch: 1/1 - Step: 3708 - Loss: 6.428566932678223\n",
      "Epoch: 1/1 - Step: 3709 - Loss: 6.191136360168457\n",
      "Epoch: 1/1 - Step: 3710 - Loss: 6.150412082672119\n",
      "Epoch: 1/1 - Step: 3711 - Loss: 6.295365810394287\n",
      "Epoch: 1/1 - Step: 3712 - Loss: 6.3051323890686035\n",
      "Epoch: 1/1 - Step: 3713 - Loss: 6.360832214355469\n",
      "Epoch: 1/1 - Step: 3714 - Loss: 6.288408279418945\n",
      "Epoch: 1/1 - Step: 3715 - Loss: 6.307453632354736\n",
      "Epoch: 1/1 - Step: 3716 - Loss: 6.2584099769592285\n",
      "Epoch: 1/1 - Step: 3717 - Loss: 6.1915812492370605\n",
      "Epoch: 1/1 - Step: 3718 - Loss: 6.246579647064209\n",
      "Epoch: 1/1 - Step: 3719 - Loss: 6.3517913818359375\n",
      "Epoch: 1/1 - Step: 3720 - Loss: 6.1978607177734375\n",
      "Epoch: 1/1 - Step: 3721 - Loss: 6.1874003410339355\n",
      "Epoch: 1/1 - Step: 3722 - Loss: 6.4474406242370605\n",
      "Epoch: 1/1 - Step: 3723 - Loss: 6.349627494812012\n",
      "Epoch: 1/1 - Step: 3724 - Loss: 6.142811298370361\n",
      "Epoch: 1/1 - Step: 3725 - Loss: 6.208442687988281\n",
      "Epoch: 1/1 - Step: 3726 - Loss: 6.4968647956848145\n",
      "Epoch: 1/1 - Step: 3727 - Loss: 6.216787338256836\n",
      "Epoch: 1/1 - Step: 3728 - Loss: 6.232141971588135\n",
      "Epoch: 1/1 - Step: 3729 - Loss: 6.227555751800537\n",
      "Epoch: 1/1 - Step: 3730 - Loss: 6.285045146942139\n",
      "Epoch: 1/1 - Step: 3731 - Loss: 8.255253791809082\n",
      "Epoch: 1/1 - Step: 3732 - Loss: 6.991276264190674\n",
      "Epoch: 1/1 - Step: 3733 - Loss: 6.2880425453186035\n",
      "Epoch: 1/1 - Step: 3734 - Loss: 6.301373481750488\n",
      "Epoch: 1/1 - Step: 3735 - Loss: 6.521084308624268\n",
      "Epoch: 1/1 - Step: 3736 - Loss: 6.526697635650635\n",
      "Epoch: 1/1 - Step: 3737 - Loss: 6.374532699584961\n",
      "Epoch: 1/1 - Step: 3738 - Loss: 6.406938076019287\n",
      "Epoch: 1/1 - Step: 3739 - Loss: 6.501208305358887\n",
      "Epoch: 1/1 - Step: 3740 - Loss: 6.634039402008057\n",
      "Epoch: 1/1 - Step: 3741 - Loss: 6.526742935180664\n",
      "Epoch: 1/1 - Step: 3742 - Loss: 6.370016574859619\n",
      "Epoch: 1/1 - Step: 3743 - Loss: 6.552444934844971\n",
      "Epoch: 1/1 - Step: 3744 - Loss: 6.621125221252441\n",
      "Epoch: 1/1 - Step: 3745 - Loss: 6.413512229919434\n",
      "Epoch: 1/1 - Step: 3746 - Loss: 6.488525867462158\n",
      "Epoch: 1/1 - Step: 3747 - Loss: 6.537694454193115\n",
      "Epoch: 1/1 - Step: 3748 - Loss: 6.544116973876953\n",
      "Epoch: 1/1 - Step: 3749 - Loss: 6.473943710327148\n",
      "Epoch: 1/1 - Step: 3750 - Loss: 6.550205707550049\n",
      "Epoch: 1/1 - Step: 3751 - Loss: 6.375524997711182\n",
      "Epoch: 1/1 - Step: 3752 - Loss: 6.28176212310791\n",
      "Epoch: 1/1 - Step: 3753 - Loss: 6.211923122406006\n",
      "Epoch: 1/1 - Step: 3754 - Loss: 6.240901470184326\n",
      "Epoch: 1/1 - Step: 3755 - Loss: 6.288182735443115\n",
      "Epoch: 1/1 - Step: 3756 - Loss: 6.666547775268555\n",
      "Epoch: 1/1 - Step: 3757 - Loss: 8.604022026062012\n",
      "Epoch: 1/1 - Step: 3758 - Loss: 8.752981185913086\n",
      "Epoch: 1/1 - Step: 3759 - Loss: 8.329129219055176\n",
      "Epoch: 1/1 - Step: 3760 - Loss: 6.541006088256836\n",
      "Epoch: 1/1 - Step: 3761 - Loss: 6.451403617858887\n",
      "Epoch: 1/1 - Step: 3762 - Loss: 6.3788933753967285\n",
      "Epoch: 1/1 - Step: 3763 - Loss: 6.556571960449219\n",
      "Epoch: 1/1 - Step: 3764 - Loss: 6.5481181144714355\n",
      "Epoch: 1/1 - Step: 3765 - Loss: 6.519607067108154\n",
      "Epoch: 1/1 - Step: 3766 - Loss: 6.513571262359619\n",
      "Epoch: 1/1 - Step: 3767 - Loss: 6.181708335876465\n",
      "Epoch: 1/1 - Step: 3768 - Loss: 6.098952770233154\n",
      "Epoch: 1/1 - Step: 3769 - Loss: 6.3469624519348145\n",
      "Epoch: 1/1 - Step: 3770 - Loss: 6.41434907913208\n",
      "Epoch: 1/1 - Step: 3771 - Loss: 6.5441131591796875\n",
      "Epoch: 1/1 - Step: 3772 - Loss: 6.381099224090576\n",
      "Epoch: 1/1 - Step: 3773 - Loss: 6.5337748527526855\n",
      "Epoch: 1/1 - Step: 3774 - Loss: 6.5397491455078125\n",
      "Epoch: 1/1 - Step: 3775 - Loss: 6.565377712249756\n",
      "Epoch: 1/1 - Step: 3776 - Loss: 6.482563018798828\n",
      "Epoch: 1/1 - Step: 3777 - Loss: 6.549081325531006\n",
      "Epoch: 1/1 - Step: 3778 - Loss: 6.505622863769531\n",
      "Epoch: 1/1 - Step: 3779 - Loss: 6.5189385414123535\n",
      "Epoch: 1/1 - Step: 3780 - Loss: 6.477435111999512\n",
      "Epoch: 1/1 - Step: 3781 - Loss: 6.495781421661377\n",
      "Epoch: 1/1 - Step: 3782 - Loss: 6.386175632476807\n",
      "Epoch: 1/1 - Step: 3783 - Loss: 6.757076263427734\n",
      "Epoch: 1/1 - Step: 3784 - Loss: 7.556881904602051\n",
      "Epoch: 1/1 - Step: 3785 - Loss: 6.425328731536865\n",
      "Epoch: 1/1 - Step: 3786 - Loss: 6.484389781951904\n",
      "Epoch: 1/1 - Step: 3787 - Loss: 6.383919715881348\n",
      "Epoch: 1/1 - Step: 3788 - Loss: 6.446045398712158\n",
      "Epoch: 1/1 - Step: 3789 - Loss: 6.336459159851074\n",
      "Epoch: 1/1 - Step: 3790 - Loss: 6.35684061050415\n",
      "Epoch: 1/1 - Step: 3791 - Loss: 6.499974727630615\n",
      "Epoch: 1/1 - Step: 3792 - Loss: 6.3653388023376465\n",
      "Epoch: 1/1 - Step: 3793 - Loss: 6.478021144866943\n",
      "Epoch: 1/1 - Step: 3794 - Loss: 6.420373439788818\n",
      "Epoch: 1/1 - Step: 3795 - Loss: 6.384206295013428\n",
      "Epoch: 1/1 - Step: 3796 - Loss: 6.701763153076172\n",
      "Epoch: 1/1 - Step: 3797 - Loss: 6.484541893005371\n",
      "Epoch: 1/1 - Step: 3798 - Loss: 6.326139450073242\n",
      "Epoch: 1/1 - Step: 3799 - Loss: 6.4102091789245605\n",
      "Epoch: 1/1 - Step: 3800 - Loss: 6.3307318687438965\n",
      "Epoch: 1/1 - Step: 3801 - Loss: 6.377912521362305\n",
      "Epoch: 1/1 - Step: 3802 - Loss: 6.357064723968506\n",
      "Epoch: 1/1 - Step: 3803 - Loss: 6.433902740478516\n",
      "Epoch: 1/1 - Step: 3804 - Loss: 6.446859359741211\n",
      "Epoch: 1/1 - Step: 3805 - Loss: 6.409836292266846\n",
      "Epoch: 1/1 - Step: 3806 - Loss: 6.498024940490723\n",
      "Epoch: 1/1 - Step: 3807 - Loss: 6.411775588989258\n",
      "Epoch: 1/1 - Step: 3808 - Loss: 6.439903736114502\n",
      "Epoch: 1/1 - Step: 3809 - Loss: 6.392211437225342\n",
      "Epoch: 1/1 - Step: 3810 - Loss: 6.456026077270508\n",
      "Epoch: 1/1 - Step: 3811 - Loss: 6.457203388214111\n",
      "Epoch: 1/1 - Step: 3812 - Loss: 6.36639928817749\n",
      "Epoch: 1/1 - Step: 3813 - Loss: 6.536280155181885\n",
      "Epoch: 1/1 - Step: 3814 - Loss: 6.668954372406006\n",
      "Epoch: 1/1 - Step: 3815 - Loss: 6.242392539978027\n",
      "Epoch: 1/1 - Step: 3816 - Loss: 6.2512617111206055\n",
      "Epoch: 1/1 - Step: 3817 - Loss: 6.234116554260254\n",
      "Epoch: 1/1 - Step: 3818 - Loss: 6.331955432891846\n",
      "Epoch: 1/1 - Step: 3819 - Loss: 6.27686882019043\n",
      "Epoch: 1/1 - Step: 3820 - Loss: 6.408473491668701\n",
      "Epoch: 1/1 - Step: 3821 - Loss: 6.348769664764404\n",
      "Epoch: 1/1 - Step: 3822 - Loss: 6.41666841506958\n",
      "Epoch: 1/1 - Step: 3823 - Loss: 6.4077372550964355\n",
      "Epoch: 1/1 - Step: 3824 - Loss: 6.632168292999268\n",
      "Epoch: 1/1 - Step: 3825 - Loss: 6.542806625366211\n",
      "Epoch: 1/1 - Step: 3826 - Loss: 6.416672229766846\n",
      "Epoch: 1/1 - Step: 3827 - Loss: 6.073363780975342\n",
      "Epoch: 1/1 - Step: 3828 - Loss: 6.1540679931640625\n",
      "Epoch: 1/1 - Step: 3829 - Loss: 6.066449165344238\n",
      "Epoch: 1/1 - Step: 3830 - Loss: 6.239807605743408\n",
      "Epoch: 1/1 - Step: 3831 - Loss: 6.327108860015869\n",
      "Epoch: 1/1 - Step: 3832 - Loss: 6.677186489105225\n",
      "Epoch: 1/1 - Step: 3833 - Loss: 6.399545669555664\n",
      "Epoch: 1/1 - Step: 3834 - Loss: 6.681490898132324\n",
      "Epoch: 1/1 - Step: 3835 - Loss: 6.46038293838501\n",
      "Epoch: 1/1 - Step: 3836 - Loss: 6.364138603210449\n",
      "Epoch: 1/1 - Step: 3837 - Loss: 6.467482566833496\n",
      "Epoch: 1/1 - Step: 3838 - Loss: 6.581875324249268\n",
      "Epoch: 1/1 - Step: 3839 - Loss: 6.609518051147461\n",
      "Epoch: 1/1 - Step: 3840 - Loss: 6.461294651031494\n",
      "Epoch: 1/1 - Step: 3841 - Loss: 6.401874542236328\n",
      "Epoch: 1/1 - Step: 3842 - Loss: 6.377960681915283\n",
      "Epoch: 1/1 - Step: 3843 - Loss: 6.485538482666016\n",
      "Epoch: 1/1 - Step: 3844 - Loss: 6.550449371337891\n",
      "Epoch: 1/1 - Step: 3845 - Loss: 6.475147724151611\n",
      "Epoch: 1/1 - Step: 3846 - Loss: 6.409483909606934\n",
      "Epoch: 1/1 - Step: 3847 - Loss: 6.507236957550049\n",
      "Epoch: 1/1 - Step: 3848 - Loss: 6.296760082244873\n",
      "Epoch: 1/1 - Step: 3849 - Loss: 6.401184558868408\n",
      "Epoch: 1/1 - Step: 3850 - Loss: 6.395746231079102\n",
      "Epoch: 1/1 - Step: 3851 - Loss: 6.446717262268066\n",
      "Epoch: 1/1 - Step: 3852 - Loss: 6.486566543579102\n",
      "Epoch: 1/1 - Step: 3853 - Loss: 6.592230796813965\n",
      "Epoch: 1/1 - Step: 3854 - Loss: 6.420834064483643\n",
      "Epoch: 1/1 - Step: 3855 - Loss: 7.59032678604126\n",
      "Epoch: 1/1 - Step: 3856 - Loss: 8.542688369750977\n",
      "Epoch: 1/1 - Step: 3857 - Loss: 8.801608085632324\n",
      "Epoch: 1/1 - Step: 3858 - Loss: 6.7835540771484375\n",
      "Epoch: 1/1 - Step: 3859 - Loss: 6.4823126792907715\n",
      "Epoch: 1/1 - Step: 3860 - Loss: 6.370180606842041\n",
      "Epoch: 1/1 - Step: 3861 - Loss: 6.386897087097168\n",
      "Epoch: 1/1 - Step: 3862 - Loss: 6.546236515045166\n",
      "Epoch: 1/1 - Step: 3863 - Loss: 6.471948623657227\n",
      "Epoch: 1/1 - Step: 3864 - Loss: 6.402058124542236\n",
      "Epoch: 1/1 - Step: 3865 - Loss: 6.4525980949401855\n",
      "Epoch: 1/1 - Step: 3866 - Loss: 6.469533443450928\n",
      "Epoch: 1/1 - Step: 3867 - Loss: 6.445372104644775\n",
      "Epoch: 1/1 - Step: 3868 - Loss: 6.322686672210693\n",
      "Epoch: 1/1 - Step: 3869 - Loss: 6.434519290924072\n",
      "Epoch: 1/1 - Step: 3870 - Loss: 6.460080623626709\n",
      "Epoch: 1/1 - Step: 3871 - Loss: 6.385913372039795\n",
      "Epoch: 1/1 - Step: 3872 - Loss: 6.392519474029541\n",
      "Epoch: 1/1 - Step: 3873 - Loss: 6.5835185050964355\n",
      "Epoch: 1/1 - Step: 3874 - Loss: 6.413733959197998\n",
      "Epoch: 1/1 - Step: 3875 - Loss: 6.455621242523193\n",
      "Epoch: 1/1 - Step: 3876 - Loss: 6.32967472076416\n",
      "Epoch: 1/1 - Step: 3877 - Loss: 6.3824238777160645\n",
      "Epoch: 1/1 - Step: 3878 - Loss: 6.465654373168945\n",
      "Epoch: 1/1 - Step: 3879 - Loss: 6.451678276062012\n",
      "Epoch: 1/1 - Step: 3880 - Loss: 6.488969802856445\n",
      "Epoch: 1/1 - Step: 3881 - Loss: 6.3596577644348145\n",
      "Epoch: 1/1 - Step: 3882 - Loss: 6.357447624206543\n",
      "Epoch: 1/1 - Step: 3883 - Loss: 6.39636754989624\n",
      "Epoch: 1/1 - Step: 3884 - Loss: 6.464024066925049\n",
      "Epoch: 1/1 - Step: 3885 - Loss: 6.391937732696533\n",
      "Epoch: 1/1 - Step: 3886 - Loss: 6.569906234741211\n",
      "Epoch: 1/1 - Step: 3887 - Loss: 6.446985721588135\n",
      "Epoch: 1/1 - Step: 3888 - Loss: 6.508874416351318\n",
      "Epoch: 1/1 - Step: 3889 - Loss: 6.31980562210083\n",
      "Epoch: 1/1 - Step: 3890 - Loss: 6.301817893981934\n",
      "Epoch: 1/1 - Step: 3891 - Loss: 6.307498455047607\n",
      "Epoch: 1/1 - Step: 3892 - Loss: 6.274646282196045\n",
      "Epoch: 1/1 - Step: 3893 - Loss: 6.261294841766357\n",
      "Epoch: 1/1 - Step: 3894 - Loss: 6.28217887878418\n",
      "Epoch: 1/1 - Step: 3895 - Loss: 6.364776134490967\n",
      "Epoch: 1/1 - Step: 3896 - Loss: 6.336379528045654\n",
      "Epoch: 1/1 - Step: 3897 - Loss: 6.229783058166504\n",
      "Epoch: 1/1 - Step: 3898 - Loss: 6.288768291473389\n",
      "Epoch: 1/1 - Step: 3899 - Loss: 6.241476058959961\n",
      "Epoch: 1/1 - Step: 3900 - Loss: 6.445103645324707\n",
      "Epoch: 1/1 - Step: 3901 - Loss: 6.576660633087158\n",
      "Epoch: 1/1 - Step: 3902 - Loss: 6.553259372711182\n",
      "Epoch: 1/1 - Step: 3903 - Loss: 6.222677707672119\n",
      "Epoch: 1/1 - Step: 3904 - Loss: 6.455533027648926\n",
      "Epoch: 1/1 - Step: 3905 - Loss: 6.4053239822387695\n",
      "Epoch: 1/1 - Step: 3906 - Loss: 6.474539279937744\n",
      "Epoch: 1/1 - Step: 3907 - Loss: 6.3184123039245605\n",
      "Epoch: 1/1 - Step: 3908 - Loss: 6.374748706817627\n",
      "Epoch: 1/1 - Step: 3909 - Loss: 6.288577079772949\n",
      "Epoch: 1/1 - Step: 3910 - Loss: 6.370446681976318\n",
      "Epoch: 1/1 - Step: 3911 - Loss: 6.406527519226074\n",
      "Epoch: 1/1 - Step: 3912 - Loss: 6.25838041305542\n",
      "Epoch: 1/1 - Step: 3913 - Loss: 6.2827301025390625\n",
      "Epoch: 1/1 - Step: 3914 - Loss: 6.474934101104736\n",
      "Epoch: 1/1 - Step: 3915 - Loss: 6.5066609382629395\n",
      "Epoch: 1/1 - Step: 3916 - Loss: 6.228330612182617\n",
      "Epoch: 1/1 - Step: 3917 - Loss: 6.315823554992676\n",
      "Epoch: 1/1 - Step: 3918 - Loss: 6.35562801361084\n",
      "Epoch: 1/1 - Step: 3919 - Loss: 6.3038010597229\n",
      "Epoch: 1/1 - Step: 3920 - Loss: 6.5511932373046875\n",
      "Epoch: 1/1 - Step: 3921 - Loss: 6.322062015533447\n",
      "Epoch: 1/1 - Step: 3922 - Loss: 6.283559799194336\n",
      "Epoch: 1/1 - Step: 3923 - Loss: 6.522083282470703\n",
      "Epoch: 1/1 - Step: 3924 - Loss: 6.2516655921936035\n",
      "Epoch: 1/1 - Step: 3925 - Loss: 6.430052757263184\n",
      "Epoch: 1/1 - Step: 3926 - Loss: 6.285181045532227\n",
      "Epoch: 1/1 - Step: 3927 - Loss: 6.243331432342529\n",
      "Epoch: 1/1 - Step: 3928 - Loss: 6.248166084289551\n",
      "Epoch: 1/1 - Step: 3929 - Loss: 6.498652935028076\n",
      "Epoch: 1/1 - Step: 3930 - Loss: 6.293864727020264\n",
      "Epoch: 1/1 - Step: 3931 - Loss: 6.192885875701904\n",
      "Epoch: 1/1 - Step: 3932 - Loss: 6.405383110046387\n",
      "Epoch: 1/1 - Step: 3933 - Loss: 6.322482585906982\n",
      "Epoch: 1/1 - Step: 3934 - Loss: 6.590269565582275\n",
      "Epoch: 1/1 - Step: 3935 - Loss: 6.620624542236328\n",
      "Epoch: 1/1 - Step: 3936 - Loss: 6.532175540924072\n",
      "Epoch: 1/1 - Step: 3937 - Loss: 6.518105983734131\n",
      "Epoch: 1/1 - Step: 3938 - Loss: 6.401477813720703\n",
      "Epoch: 1/1 - Step: 3939 - Loss: 6.308011531829834\n",
      "Epoch: 1/1 - Step: 3940 - Loss: 6.215954780578613\n",
      "Epoch: 1/1 - Step: 3941 - Loss: 6.183830738067627\n",
      "Epoch: 1/1 - Step: 3942 - Loss: 6.259303092956543\n",
      "Epoch: 1/1 - Step: 3943 - Loss: 6.318663597106934\n",
      "Epoch: 1/1 - Step: 3944 - Loss: 6.199041843414307\n",
      "Epoch: 1/1 - Step: 3945 - Loss: 6.173787593841553\n",
      "Epoch: 1/1 - Step: 3946 - Loss: 6.343975067138672\n",
      "Epoch: 1/1 - Step: 3947 - Loss: 6.608766555786133\n",
      "Epoch: 1/1 - Step: 3948 - Loss: 6.1613006591796875\n",
      "Epoch: 1/1 - Step: 3949 - Loss: 6.312403202056885\n",
      "Epoch: 1/1 - Step: 3950 - Loss: 6.331352710723877\n",
      "Epoch: 1/1 - Step: 3951 - Loss: 6.2859578132629395\n",
      "Epoch: 1/1 - Step: 3952 - Loss: 6.348658084869385\n",
      "Epoch: 1/1 - Step: 3953 - Loss: 6.200225830078125\n",
      "Epoch: 1/1 - Step: 3954 - Loss: 6.34214448928833\n",
      "Epoch: 1/1 - Step: 3955 - Loss: 6.297449588775635\n",
      "Epoch: 1/1 - Step: 3956 - Loss: 6.288997650146484\n",
      "Epoch: 1/1 - Step: 3957 - Loss: 6.269017696380615\n",
      "Epoch: 1/1 - Step: 3958 - Loss: 6.281093120574951\n",
      "Epoch: 1/1 - Step: 3959 - Loss: 6.446300029754639\n",
      "Epoch: 1/1 - Step: 3960 - Loss: 6.360541820526123\n",
      "Epoch: 1/1 - Step: 3961 - Loss: 6.325840950012207\n",
      "Epoch: 1/1 - Step: 3962 - Loss: 6.335170745849609\n",
      "Epoch: 1/1 - Step: 3963 - Loss: 6.3089752197265625\n",
      "Epoch: 1/1 - Step: 3964 - Loss: 6.253975868225098\n",
      "Epoch: 1/1 - Step: 3965 - Loss: 6.197022438049316\n",
      "Epoch: 1/1 - Step: 3966 - Loss: 6.198724746704102\n",
      "Epoch: 1/1 - Step: 3967 - Loss: 6.374563694000244\n",
      "Epoch: 1/1 - Step: 3968 - Loss: 6.216617584228516\n",
      "Epoch: 1/1 - Step: 3969 - Loss: 6.272317409515381\n",
      "Epoch: 1/1 - Step: 3970 - Loss: 6.230090141296387\n",
      "Epoch: 1/1 - Step: 3971 - Loss: 6.212001323699951\n",
      "Epoch: 1/1 - Step: 3972 - Loss: 6.14195442199707\n",
      "Epoch: 1/1 - Step: 3973 - Loss: 6.292283535003662\n",
      "Epoch: 1/1 - Step: 3974 - Loss: 6.291604042053223\n",
      "Epoch: 1/1 - Step: 3975 - Loss: 6.571187973022461\n",
      "Epoch: 1/1 - Step: 3976 - Loss: 6.366976737976074\n",
      "Epoch: 1/1 - Step: 3977 - Loss: 6.2120819091796875\n",
      "Epoch: 1/1 - Step: 3978 - Loss: 6.250636577606201\n",
      "Epoch: 1/1 - Step: 3979 - Loss: 6.314081192016602\n",
      "Epoch: 1/1 - Step: 3980 - Loss: 6.384355068206787\n",
      "Epoch: 1/1 - Step: 3981 - Loss: 6.265191078186035\n",
      "Epoch: 1/1 - Step: 3982 - Loss: 6.2282328605651855\n",
      "Epoch: 1/1 - Step: 3983 - Loss: 6.271932601928711\n",
      "Epoch: 1/1 - Step: 3984 - Loss: 6.315837383270264\n",
      "Epoch: 1/1 - Step: 3985 - Loss: 6.244447231292725\n",
      "Epoch: 1/1 - Step: 3986 - Loss: 6.244547367095947\n",
      "Epoch: 1/1 - Step: 3987 - Loss: 6.4440813064575195\n",
      "Epoch: 1/1 - Step: 3988 - Loss: 8.613774299621582\n",
      "Epoch: 1/1 - Step: 3989 - Loss: 6.950814723968506\n",
      "Epoch: 1/1 - Step: 3990 - Loss: 6.373227596282959\n",
      "Epoch: 1/1 - Step: 3991 - Loss: 6.118687629699707\n",
      "Epoch: 1/1 - Step: 3992 - Loss: 6.208165645599365\n",
      "Epoch: 1/1 - Step: 3993 - Loss: 6.361390113830566\n",
      "Epoch: 1/1 - Step: 3994 - Loss: 6.455016136169434\n",
      "Epoch: 1/1 - Step: 3995 - Loss: 6.277522563934326\n",
      "Epoch: 1/1 - Step: 3996 - Loss: 6.31792688369751\n",
      "Epoch: 1/1 - Step: 3997 - Loss: 6.410749912261963\n",
      "Epoch: 1/1 - Step: 3998 - Loss: 6.487577438354492\n",
      "Epoch: 1/1 - Step: 3999 - Loss: 6.503785133361816\n",
      "Epoch: 1/1 - Step: 4000 - Loss: 6.3433380126953125\n",
      "Epoch: 1/1 - Step: 4001 - Loss: 6.180784225463867\n",
      "Epoch: 1/1 - Step: 4002 - Loss: 6.250746250152588\n",
      "Epoch: 1/1 - Step: 4003 - Loss: 6.439450263977051\n",
      "Epoch: 1/1 - Step: 4004 - Loss: 6.512197017669678\n",
      "Epoch: 1/1 - Step: 4005 - Loss: 6.240996837615967\n",
      "Epoch: 1/1 - Step: 4006 - Loss: 6.126936912536621\n",
      "Epoch: 1/1 - Step: 4007 - Loss: 6.046485424041748\n",
      "Epoch: 1/1 - Step: 4008 - Loss: 6.182004928588867\n",
      "Epoch: 1/1 - Step: 4009 - Loss: 6.474931716918945\n",
      "Epoch: 1/1 - Step: 4010 - Loss: 6.162783145904541\n",
      "Epoch: 1/1 - Step: 4011 - Loss: 6.26194429397583\n",
      "Epoch: 1/1 - Step: 4012 - Loss: 6.3217949867248535\n",
      "Epoch: 1/1 - Step: 4013 - Loss: 6.297309875488281\n",
      "Epoch: 1/1 - Step: 4014 - Loss: 6.212327480316162\n",
      "Epoch: 1/1 - Step: 4015 - Loss: 6.432121753692627\n",
      "Epoch: 1/1 - Step: 4016 - Loss: 6.301743507385254\n",
      "Epoch: 1/1 - Step: 4017 - Loss: 6.250837326049805\n",
      "Epoch: 1/1 - Step: 4018 - Loss: 6.201536655426025\n",
      "Epoch: 1/1 - Step: 4019 - Loss: 6.28587007522583\n",
      "Epoch: 1/1 - Step: 4020 - Loss: 6.314607620239258\n",
      "Epoch: 1/1 - Step: 4021 - Loss: 6.2992939949035645\n",
      "Epoch: 1/1 - Step: 4022 - Loss: 6.35014533996582\n",
      "Epoch: 1/1 - Step: 4023 - Loss: 6.502213954925537\n",
      "Epoch: 1/1 - Step: 4024 - Loss: 6.181363582611084\n",
      "Epoch: 1/1 - Step: 4025 - Loss: 6.246707916259766\n",
      "Epoch: 1/1 - Step: 4026 - Loss: 6.2484612464904785\n",
      "Epoch: 1/1 - Step: 4027 - Loss: 6.246511936187744\n",
      "Epoch: 1/1 - Step: 4028 - Loss: 6.228712558746338\n",
      "Epoch: 1/1 - Step: 4029 - Loss: 6.30908727645874\n",
      "Epoch: 1/1 - Step: 4030 - Loss: 6.201415061950684\n",
      "Epoch: 1/1 - Step: 4031 - Loss: 6.252624988555908\n",
      "Epoch: 1/1 - Step: 4032 - Loss: 6.368874549865723\n",
      "Epoch: 1/1 - Step: 4033 - Loss: 6.14068078994751\n",
      "Epoch: 1/1 - Step: 4034 - Loss: 6.096187114715576\n",
      "Epoch: 1/1 - Step: 4035 - Loss: 6.267768859863281\n",
      "Epoch: 1/1 - Step: 4036 - Loss: 6.213597297668457\n",
      "Epoch: 1/1 - Step: 4037 - Loss: 6.496339321136475\n",
      "Epoch: 1/1 - Step: 4038 - Loss: 8.458550453186035\n",
      "Epoch: 1/1 - Step: 4039 - Loss: 6.958029270172119\n",
      "Epoch: 1/1 - Step: 4040 - Loss: 6.44401741027832\n",
      "Epoch: 1/1 - Step: 4041 - Loss: 6.373209476470947\n",
      "Epoch: 1/1 - Step: 4042 - Loss: 6.3440680503845215\n",
      "Epoch: 1/1 - Step: 4043 - Loss: 6.247416973114014\n",
      "Epoch: 1/1 - Step: 4044 - Loss: 6.3584370613098145\n",
      "Epoch: 1/1 - Step: 4045 - Loss: 6.3443498611450195\n",
      "Epoch: 1/1 - Step: 4046 - Loss: 6.353124141693115\n",
      "Epoch: 1/1 - Step: 4047 - Loss: 6.24633264541626\n",
      "Epoch: 1/1 - Step: 4048 - Loss: 6.433145999908447\n",
      "Epoch: 1/1 - Step: 4049 - Loss: 6.400704383850098\n",
      "Epoch: 1/1 - Step: 4050 - Loss: 6.290885925292969\n",
      "Epoch: 1/1 - Step: 4051 - Loss: 6.1142449378967285\n",
      "Epoch: 1/1 - Step: 4052 - Loss: 6.16127872467041\n",
      "Epoch: 1/1 - Step: 4053 - Loss: 6.306570529937744\n",
      "Epoch: 1/1 - Step: 4054 - Loss: 6.301482677459717\n",
      "Epoch: 1/1 - Step: 4055 - Loss: 6.297503471374512\n",
      "Epoch: 1/1 - Step: 4056 - Loss: 6.221806049346924\n",
      "Epoch: 1/1 - Step: 4057 - Loss: 6.313445091247559\n",
      "Epoch: 1/1 - Step: 4058 - Loss: 6.202353477478027\n",
      "Epoch: 1/1 - Step: 4059 - Loss: 6.194390296936035\n",
      "Epoch: 1/1 - Step: 4060 - Loss: 6.2161407470703125\n",
      "Epoch: 1/1 - Step: 4061 - Loss: 6.248785018920898\n",
      "Epoch: 1/1 - Step: 4062 - Loss: 6.221916675567627\n",
      "Epoch: 1/1 - Step: 4063 - Loss: 6.172418117523193\n",
      "Epoch: 1/1 - Step: 4064 - Loss: 6.453055381774902\n",
      "Epoch: 1/1 - Step: 4065 - Loss: 6.271764755249023\n",
      "Epoch: 1/1 - Step: 4066 - Loss: 6.12034797668457\n",
      "Epoch: 1/1 - Step: 4067 - Loss: 6.27050256729126\n",
      "Epoch: 1/1 - Step: 4068 - Loss: 6.372314453125\n",
      "Epoch: 1/1 - Step: 4069 - Loss: 6.229767799377441\n",
      "Epoch: 1/1 - Step: 4070 - Loss: 6.17841100692749\n",
      "Epoch: 1/1 - Step: 4071 - Loss: 6.216172218322754\n",
      "Epoch: 1/1 - Step: 4072 - Loss: 6.305205821990967\n",
      "Epoch: 1/1 - Step: 4073 - Loss: 8.661836624145508\n",
      "Epoch: 1/1 - Step: 4074 - Loss: 6.452128887176514\n",
      "Epoch: 1/1 - Step: 4075 - Loss: 6.264368534088135\n",
      "Epoch: 1/1 - Step: 4076 - Loss: 6.307661056518555\n",
      "Epoch: 1/1 - Step: 4077 - Loss: 6.522339820861816\n",
      "Epoch: 1/1 - Step: 4078 - Loss: 6.465291500091553\n",
      "Epoch: 1/1 - Step: 4079 - Loss: 6.39257287979126\n",
      "Epoch: 1/1 - Step: 4080 - Loss: 6.386417865753174\n",
      "Epoch: 1/1 - Step: 4081 - Loss: 6.522348403930664\n",
      "Epoch: 1/1 - Step: 4082 - Loss: 6.542819023132324\n",
      "Epoch: 1/1 - Step: 4083 - Loss: 6.497311115264893\n",
      "Epoch: 1/1 - Step: 4084 - Loss: 6.405983924865723\n",
      "Epoch: 1/1 - Step: 4085 - Loss: 6.520932674407959\n",
      "Epoch: 1/1 - Step: 4086 - Loss: 6.587928295135498\n",
      "Epoch: 1/1 - Step: 4087 - Loss: 6.399189472198486\n",
      "Epoch: 1/1 - Step: 4088 - Loss: 6.506486415863037\n",
      "Epoch: 1/1 - Step: 4089 - Loss: 6.517258644104004\n",
      "Epoch: 1/1 - Step: 4090 - Loss: 6.493410587310791\n",
      "Epoch: 1/1 - Step: 4091 - Loss: 6.42713737487793\n",
      "Epoch: 1/1 - Step: 4092 - Loss: 6.556078910827637\n",
      "Epoch: 1/1 - Step: 4093 - Loss: 6.233379364013672\n",
      "Epoch: 1/1 - Step: 4094 - Loss: 6.2349629402160645\n",
      "Epoch: 1/1 - Step: 4095 - Loss: 6.193540096282959\n",
      "Epoch: 1/1 - Step: 4096 - Loss: 6.225368499755859\n",
      "Epoch: 1/1 - Step: 4097 - Loss: 6.228031158447266\n",
      "Epoch: 1/1 - Step: 4098 - Loss: 7.124661922454834\n",
      "Epoch: 1/1 - Step: 4099 - Loss: 8.579546928405762\n",
      "Epoch: 1/1 - Step: 4100 - Loss: 8.800024032592773\n",
      "Epoch: 1/1 - Step: 4101 - Loss: 7.731689453125\n"
     ]
    }
   ],
   "source": [
    "train(model, batch_size=batch_size, epochs=epochs, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2d24941a-ba1a-4a23-9a2b-5d12ec3853b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 - Step: 1 - Loss: 6.43322229385376\n",
      "Epoch: 1/1 - Step: 2 - Loss: 6.422721862792969\n",
      "Epoch: 1/1 - Step: 3 - Loss: 6.343315601348877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-29c2f0eeb493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-bc518b3ef512>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, batch_size, lr, clip, print_every)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m# calculate the loss and perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# back-propagate error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, batch_size=batch_size, epochs=epochs, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(model, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[token2int[t]]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "  \n",
    "    # push to CPU\n",
    "    inputs = inputs.cpu()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top 3 values\n",
    "    top_n_idx = p.argsort()[-5:][::-1]\n",
    "\n",
    "    # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2,3,4],1)[0]]\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model, size, prompt='in this paper'):\n",
    "        \n",
    "    # push to CPU\n",
    "    model.cpu()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.ones(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    toks = prompt.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(model, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "645a8dd9-4a9c-4059-8c23-28e753ca0cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper browsers sofc cnlvr sonante parlvote through flairnlp analyser tection delved ratmcu procedure editor seqs proactivity procedure editor editor please shows lipschitz lowerbound shows rodynamiques exercices implicit semdis hosting cmce vocaux sva ratmcu tection cmce narrativeqa sva ben cmce implicit criticisms cmce ben members hack hack hack scalar maple identifiers dsvi'"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, 50, 'in this paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a1e11-ef43-428a-9db9-a3f418018314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
