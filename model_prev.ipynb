{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2731b1-f80b-4587-9834-5f7d3402ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c322678-97e7-44ed-8155-d14cfd47a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f89a46-8f42-477f-90ff-592fffa0b62b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe343191-55af-48bb-b966-55ad552efcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized.txt','r') as f:\n",
    "    tokenized = eval(f.read())\n",
    "    \n",
    "with open('data/tokens.txt','r') as f:\n",
    "    tokens = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f210f4c1-b171-4e41-be01-ee4dd311b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.load('data/x.npy')\n",
    "y_arr = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c71560-f7a1-437a-b023-fd9004a7b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5841, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b21bf4-ad35-4c76-abc0-307060aa3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4084a92-0f22-4e42-ab06-95ebeffccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(x_arr)\n",
    "ids = list(range(dataset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13ec988-2ff5-47f3-b15c-c48307304e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = int(np.floor(valid_split * dataset_len))\n",
    "val_ids = np.random.choice(ids, size=val_len, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd395d6c-bada-4844-a2e0-4926815bf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.array(x_arr)[val_ids] \n",
    "y_val = np.array(y_arr)[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ca0ddd-101d-437c-a1d6-8b43eb9da709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 100), (1168, 100))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67520dd9-c365-44c1-8f2d-9ebed3878bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.delete(x_arr, val_ids, axis=0)\n",
    "y_train = np.delete(y_arr, val_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5147fa1-457d-44b2-a9da-c58739c7ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4673, 100), (4673, 100))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129df162-b5dd-4fc5-9ffa-63b8765ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x_arr, y_arr, batch_size):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for n in range(batch_size, x_arr.shape[0], batch_size):\n",
    "        x = x_arr[pos:n]\n",
    "        y = y_arr[pos:n]\n",
    "        pos = n\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27066b95-79ce-4403-97d8-1b918ca73497",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'train': x_train , 'val': x_val}\n",
    "y_data = {'train': y_train , 'val': y_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9823008-e8f0-46e4-a20d-c6a7915b97b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cc00f-6613-4965-aeb1-532785684f6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4ae5c6-cf7a-4286-ab02-9700f0eb6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load(\"w2v.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b755d9f8-b239-4ae6-a05e-0484e125cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17862, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size = w2v_model.wv.vectors.shape\n",
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2fa4c61-c375-42d9-8c23-a6a39478fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = w2v_model.wv.vectors.shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f57a05-44f3-4f17-81db-49792038b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tensors = torch.FloatTensor(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d8edf-17a3-4c57-a2f7-754da054d4da",
   "metadata": {},
   "source": [
    "### ngram Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269db124-8bde-4788-983e-0879d03a5f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc607789-4401-4e69-b888-14d7f7485a74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b2143a-22ba-4535-801d-37fb06ec47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparams used as params for LSA and PCA\n",
    "n_hidden = 256\n",
    "batch_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801312e8-aed1-41a9-85cb-9b220b850255",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c738d7-37a5-4109-a15b-644e15e3b41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)\n",
    "dct.filter_extremes(no_below=5, no_above=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80adadbb-835d-4055-a762-47c687301689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4073c810-0ca7-427f-9688-9962f7fd728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5485 unique tokens: ['across', 'all', 'annotation', 'arabic', 'baselines']...)\n"
     ]
    }
   ],
   "source": [
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3864db32-5e75-4e37-bd9e-dec3019ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(a) for a in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e1a2d-98a7-45c5-8dd6-7851aa323e51",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffbc817-9458-491f-a8d1-ee7d9fabb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus, id2word=dct, num_topics=n_hidden, decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef742f30-a219-408b-a39a-bc77dc2e8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.862*\"e\" + 0.293*\"de\" + 0.163*\"d\" + 0.125*\"les\" + 0.122*\"la\" + 0.121*\"des\" + 0.108*\"l\" + 0.095*\"et\" + 0.087*\"s\" + 0.081*\"le\"'),\n",
       " (1,\n",
       "  '0.177*\"word\" + 0.151*\"text\" + 0.137*\"learning\" + 0.135*\"using\" + 0.134*\"information\" + 0.132*\"it\" + 0.130*\"performance\" + 0.126*\"tasks\" + 0.122*\"training\" + 0.121*\"or\"'),\n",
       " (2,\n",
       "  '-0.777*\"word\" + -0.237*\"embeddings\" + -0.202*\"words\" + 0.142*\"text\" + -0.107*\"languages\" + 0.100*\"domain\" + 0.096*\"knowledge\" + 0.094*\"dataset\" + 0.091*\"question\" + 0.083*\"training\"'),\n",
       " (3,\n",
       "  '0.663*\"translation\" + 0.271*\"english\" + 0.271*\"machine\" + 0.168*\"nmt\" + 0.158*\"languages\" + -0.126*\"information\" + 0.118*\"parallel\" + 0.117*\"mt\" + -0.110*\"knowledge\" + 0.096*\"source\"'),\n",
       " (4,\n",
       "  '-0.305*\"corpus\" + 0.185*\"neural\" + 0.184*\"training\" + -0.180*\"languages\" + 0.162*\"translation\" + -0.154*\"speech\" + 0.152*\"tasks\" + 0.144*\"propose\" + -0.135*\"system\" + 0.134*\"sentence\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b70180a-f67f-4092-9569-40997bf0b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('translation', 0.6633031598226204),\n",
       " ('english', 0.2709285453655678),\n",
       " ('machine', 0.27051568908349555),\n",
       " ('nmt', 0.1682297018972616),\n",
       " ('languages', 0.1579911776102907),\n",
       " ('information', -0.12648798661584168),\n",
       " ('parallel', 0.11843316153211839),\n",
       " ('mt', 0.11745082696863827),\n",
       " ('knowledge', -0.11032758426646193),\n",
       " ('source', 0.09550392386188096)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topic(3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d2d4cc-f09c-4096-9c53-1e6d7d05eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_topics = np.transpose(lsi.projection.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae62faf-ac59-4f61-87a7-bf2de09c82cb",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fd01de-ac01-43ef-a370-6bd234108e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_topics = PCA(n_components=n_layers*batch_size, svd_solver='full').fit_transform(trans_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d50757e-d8ca-482a-88b7-99636cf4f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_trans = np.transpose(pca_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1570d-87e9-4581-9f83-3c9b54956822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a02c32-c923-46f1-9228-c75df69508c7",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39435312-dd5f-4751-a2b6-ae70dcd5b66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden),\n",
    "                 torch.zeros(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dbd655cd-b894-46cc-85c7-f3c5096c39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "lstm = LSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82a109-0dc1-4cef-9de3-ec6ebfd01039",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd666eda-cd61-4695-8136-2de8fc400a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden),\n",
    "                 torch.zeros(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f12ca673-c356-4ca4-99f7-c8d5e7bae887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Word2Vec(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "lstm_w2v = LSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers, drop_prob=0)\n",
    "\n",
    "print(lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08093e-f769-4dc9-8c2e-2753e70772d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conditioned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82a6de5d-722f-447c-be5e-3c1b4f8b9dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conditioned_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.zeros(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "385453f3-8dc1-46e1-b9c2-d37088ab4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioned_LSTM(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm = Conditioned_LSTM(n_hidden=n_hidden, n_layers=n_layers)\n",
    "\n",
    "print(cond_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e0f1-f4e5-46cf-afed-4f9941b79994",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Condtiioned LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a9b0329-f90a-4121-9308-931b5c2b6b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conditioned_LSTM_Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding.from_pretrained(w2v_tensors)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        x = x.long()\n",
    "\n",
    "        # pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        # get outputs and new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        # flatten out\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        # put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "\n",
    "        hidden = (torch.FloatTensor(pca_trans.reshape(self.n_layers, batch_size, self.n_hidden)),\n",
    "                 torch.zeros(self.n_layers, batch_size, self.n_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "318c4949-55c3-40d9-a382-9cc1ef9b9d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioned_LSTM_Word2Vec(\n",
      "  (emb_layer): Embedding(17862, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "cond_lstm_w2v = Conditioned_LSTM_Word2Vec(n_hidden=n_hidden, n_layers=n_layers, drop_prob=0)\n",
    "\n",
    "print(cond_lstm_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e7eba-ecc4-4c76-b121-7b59b496bbdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3dc1f2f-8f97-402d-a896-af196d1e88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # loss values\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # dict to assing to right loss list according to phase\n",
    "    losses = {'train': train_loss , 'val': val_loss}\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train(True) if phase == 'train' else model.train(False)\n",
    "            \n",
    "            batch = 0\n",
    "\n",
    "            train_epoch_loss = []\n",
    "            val_epoch_loss = []\n",
    "            \n",
    "            # dict to assing to right epoch loss list according to phase\n",
    "            epoch_loss = {'train': train_epoch_loss , 'val': val_epoch_loss} \n",
    "\n",
    "            for x, y in batches(x_data[phase], y_data[phase], batch_size):\n",
    "\n",
    "                batch += 1\n",
    "\n",
    "                # initialize hidden state\n",
    "                h = model.init_hidden(batch_size)\n",
    "\n",
    "                # convert numpy arrays to PyTorch arrays\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                # detach hidden states\n",
    "                h = tuple([each.data for each in h])\n",
    "\n",
    "                # zero accumulated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # get the output from the model\n",
    "                output, h = model(inputs, h)\n",
    "\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, targets.view(-1).long())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # back-propagate error\n",
    "                    loss.backward()\n",
    "\n",
    "                    # `clip_grad_norm` helps prevent exploding gradient\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                    # update weigths\n",
    "                    opt.step()\n",
    "                \n",
    "                # add current batch loss to epoch loss list\n",
    "                epoch_loss[phase].append(loss.item())\n",
    "\n",
    "                # show epoch - batch - loss every n batches\n",
    "                if batch % print_every == 0:\n",
    "\n",
    "                    tot_batches = int(x_data[phase].shape[0] / batch_size)\n",
    "\n",
    "                    print(\"Epoch: {}/{} -\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{} -\".format(batch, tot_batches),\n",
    "                          \"{} loss: {:.5f}\".format(phase.capitalize(), loss))\n",
    "                    \n",
    "            # calculate average epoch loss\n",
    "            avg_epoch_loss = sum(epoch_loss[phase])/len(epoch_loss[phase])\n",
    "                    \n",
    "            # print average train and val loss at the end of each epoch\n",
    "            print(\"\\nEpoch: {}/{} -\".format(e+1, epochs),\n",
    "                  \"Average {} loss: {:.5f}\\n\".format(phase, avg_epoch_loss))           \n",
    "\n",
    "            # save average epoch loss for training and validation\n",
    "            losses[phase].append(avg_epoch_loss)\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58eb0a03-1ec7-49c7-9dfc-530a10219de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "path = 'weights/cond_lstm.pt'\n",
    "loss = 0.2\n",
    "\n",
    "optimizer = torch.optim.Adam(cond_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9171143-818e-423f-a030-0a1fe9393c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\giuse\\Conda\\envs\\thesis\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save checkpoints\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': cond_lstm.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83fc79af-27cf-4951-9d4d-6f47b15a8e3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 5/73 - Train loss: 6.98827\n",
      "Epoch: 1/20 - Batch: 10/73 - Train loss: 6.88689\n",
      "Epoch: 1/20 - Batch: 15/73 - Train loss: 6.75206\n",
      "Epoch: 1/20 - Batch: 20/73 - Train loss: 6.93269\n",
      "Epoch: 1/20 - Batch: 25/73 - Train loss: 6.92339\n",
      "Epoch: 1/20 - Batch: 30/73 - Train loss: 6.88088\n",
      "Epoch: 1/20 - Batch: 35/73 - Train loss: 6.93053\n",
      "Epoch: 1/20 - Batch: 40/73 - Train loss: 6.85434\n",
      "Epoch: 1/20 - Batch: 45/73 - Train loss: 6.80789\n",
      "Epoch: 1/20 - Batch: 50/73 - Train loss: 6.81354\n",
      "Epoch: 1/20 - Batch: 55/73 - Train loss: 6.84916\n",
      "Epoch: 1/20 - Batch: 60/73 - Train loss: 7.45116\n",
      "Epoch: 1/20 - Batch: 65/73 - Train loss: 6.81456\n",
      "Epoch: 1/20 - Batch: 70/73 - Train loss: 7.00605\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 6.96200\n",
      "\n",
      "Epoch: 1/20 - Batch: 5/18 - Val loss: 6.92356\n",
      "Epoch: 1/20 - Batch: 10/18 - Val loss: 7.06453\n",
      "Epoch: 1/20 - Batch: 15/18 - Val loss: 7.02465\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.95191\n",
      "\n",
      "Epoch: 2/20 - Batch: 5/73 - Train loss: 6.87585\n",
      "Epoch: 2/20 - Batch: 10/73 - Train loss: 6.80784\n",
      "Epoch: 2/20 - Batch: 15/73 - Train loss: 6.64605\n",
      "Epoch: 2/20 - Batch: 20/73 - Train loss: 6.79792\n",
      "Epoch: 2/20 - Batch: 25/73 - Train loss: 6.81003\n",
      "Epoch: 2/20 - Batch: 30/73 - Train loss: 6.75389\n",
      "Epoch: 2/20 - Batch: 35/73 - Train loss: 6.79371\n",
      "Epoch: 2/20 - Batch: 40/73 - Train loss: 6.71035\n",
      "Epoch: 2/20 - Batch: 45/73 - Train loss: 6.67359\n",
      "Epoch: 2/20 - Batch: 50/73 - Train loss: 6.65822\n",
      "Epoch: 2/20 - Batch: 55/73 - Train loss: 6.67253\n",
      "Epoch: 2/20 - Batch: 60/73 - Train loss: 7.84488\n",
      "Epoch: 2/20 - Batch: 65/73 - Train loss: 6.64560\n",
      "Epoch: 2/20 - Batch: 70/73 - Train loss: 6.81306\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 6.81085\n",
      "\n",
      "Epoch: 2/20 - Batch: 5/18 - Val loss: 6.76571\n",
      "Epoch: 2/20 - Batch: 10/18 - Val loss: 6.88773\n",
      "Epoch: 2/20 - Batch: 15/18 - Val loss: 6.81833\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 6.77475\n",
      "\n",
      "Epoch: 3/20 - Batch: 5/73 - Train loss: 6.72196\n",
      "Epoch: 3/20 - Batch: 10/73 - Train loss: 6.63370\n",
      "Epoch: 3/20 - Batch: 15/73 - Train loss: 6.49890\n",
      "Epoch: 3/20 - Batch: 20/73 - Train loss: 6.62262\n",
      "Epoch: 3/20 - Batch: 25/73 - Train loss: 6.64551\n",
      "Epoch: 3/20 - Batch: 30/73 - Train loss: 6.57138\n",
      "Epoch: 3/20 - Batch: 35/73 - Train loss: 6.61777\n",
      "Epoch: 3/20 - Batch: 40/73 - Train loss: 6.54357\n",
      "Epoch: 3/20 - Batch: 45/73 - Train loss: 6.50248\n",
      "Epoch: 3/20 - Batch: 50/73 - Train loss: 6.49183\n",
      "Epoch: 3/20 - Batch: 55/73 - Train loss: 6.51486\n",
      "Epoch: 3/20 - Batch: 60/73 - Train loss: 6.71276\n",
      "Epoch: 3/20 - Batch: 65/73 - Train loss: 6.48777\n",
      "Epoch: 3/20 - Batch: 70/73 - Train loss: 6.65648\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.60429\n",
      "\n",
      "Epoch: 3/20 - Batch: 5/18 - Val loss: 6.60205\n",
      "Epoch: 3/20 - Batch: 10/18 - Val loss: 6.68307\n",
      "Epoch: 3/20 - Batch: 15/18 - Val loss: 6.56789\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.58778\n",
      "\n",
      "Epoch: 4/20 - Batch: 5/73 - Train loss: 6.55618\n",
      "Epoch: 4/20 - Batch: 10/73 - Train loss: 6.49780\n",
      "Epoch: 4/20 - Batch: 15/73 - Train loss: 6.37021\n",
      "Epoch: 4/20 - Batch: 20/73 - Train loss: 6.48112\n",
      "Epoch: 4/20 - Batch: 25/73 - Train loss: 6.49418\n",
      "Epoch: 4/20 - Batch: 30/73 - Train loss: 6.40660\n",
      "Epoch: 4/20 - Batch: 35/73 - Train loss: 6.48195\n",
      "Epoch: 4/20 - Batch: 40/73 - Train loss: 6.40363\n",
      "Epoch: 4/20 - Batch: 45/73 - Train loss: 6.35210\n",
      "Epoch: 4/20 - Batch: 50/73 - Train loss: 6.34088\n",
      "Epoch: 4/20 - Batch: 55/73 - Train loss: 6.35718\n",
      "Epoch: 4/20 - Batch: 60/73 - Train loss: 6.36913\n",
      "Epoch: 4/20 - Batch: 65/73 - Train loss: 6.33574\n",
      "Epoch: 4/20 - Batch: 70/73 - Train loss: 6.52350\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 6.43522\n",
      "\n",
      "Epoch: 4/20 - Batch: 5/18 - Val loss: 6.47707\n",
      "Epoch: 4/20 - Batch: 10/18 - Val loss: 6.55845\n",
      "Epoch: 4/20 - Batch: 15/18 - Val loss: 6.42827\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 6.46100\n",
      "\n",
      "Epoch: 5/20 - Batch: 5/73 - Train loss: 6.41393\n",
      "Epoch: 5/20 - Batch: 10/73 - Train loss: 6.35745\n",
      "Epoch: 5/20 - Batch: 15/73 - Train loss: 6.23887\n",
      "Epoch: 5/20 - Batch: 20/73 - Train loss: 6.35032\n",
      "Epoch: 5/20 - Batch: 25/73 - Train loss: 6.35191\n",
      "Epoch: 5/20 - Batch: 30/73 - Train loss: 6.26610\n",
      "Epoch: 5/20 - Batch: 35/73 - Train loss: 6.34397\n",
      "Epoch: 5/20 - Batch: 40/73 - Train loss: 6.26418\n",
      "Epoch: 5/20 - Batch: 45/73 - Train loss: 6.22254\n",
      "Epoch: 5/20 - Batch: 50/73 - Train loss: 6.21352\n",
      "Epoch: 5/20 - Batch: 55/73 - Train loss: 6.21144\n",
      "Epoch: 5/20 - Batch: 60/73 - Train loss: 6.21350\n",
      "Epoch: 5/20 - Batch: 65/73 - Train loss: 6.20175\n",
      "Epoch: 5/20 - Batch: 70/73 - Train loss: 6.39601\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 6.29901\n",
      "\n",
      "Epoch: 5/20 - Batch: 5/18 - Val loss: 6.37458\n",
      "Epoch: 5/20 - Batch: 10/18 - Val loss: 6.46051\n",
      "Epoch: 5/20 - Batch: 15/18 - Val loss: 6.32470\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 6.35875\n",
      "\n",
      "Epoch: 6/20 - Batch: 5/73 - Train loss: 6.29920\n",
      "Epoch: 6/20 - Batch: 10/73 - Train loss: 6.23447\n",
      "Epoch: 6/20 - Batch: 15/73 - Train loss: 6.12174\n",
      "Epoch: 6/20 - Batch: 20/73 - Train loss: 6.23299\n",
      "Epoch: 6/20 - Batch: 25/73 - Train loss: 6.22596\n",
      "Epoch: 6/20 - Batch: 30/73 - Train loss: 6.14073\n",
      "Epoch: 6/20 - Batch: 35/73 - Train loss: 6.22735\n",
      "Epoch: 6/20 - Batch: 40/73 - Train loss: 6.14321\n",
      "Epoch: 6/20 - Batch: 45/73 - Train loss: 6.08776\n",
      "Epoch: 6/20 - Batch: 50/73 - Train loss: 6.09190\n",
      "Epoch: 6/20 - Batch: 55/73 - Train loss: 6.09092\n",
      "Epoch: 6/20 - Batch: 60/73 - Train loss: 6.09748\n",
      "Epoch: 6/20 - Batch: 65/73 - Train loss: 6.07571\n",
      "Epoch: 6/20 - Batch: 70/73 - Train loss: 6.29521\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 6.17869\n",
      "\n",
      "Epoch: 6/20 - Batch: 5/18 - Val loss: 6.27414\n",
      "Epoch: 6/20 - Batch: 10/18 - Val loss: 6.36814\n",
      "Epoch: 6/20 - Batch: 15/18 - Val loss: 6.23102\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 6.26023\n",
      "\n",
      "Epoch: 7/20 - Batch: 5/73 - Train loss: 6.17669\n",
      "Epoch: 7/20 - Batch: 10/73 - Train loss: 6.10286\n",
      "Epoch: 7/20 - Batch: 15/73 - Train loss: 5.99127\n",
      "Epoch: 7/20 - Batch: 20/73 - Train loss: 6.11434\n",
      "Epoch: 7/20 - Batch: 25/73 - Train loss: 6.09895\n",
      "Epoch: 7/20 - Batch: 30/73 - Train loss: 6.01993\n",
      "Epoch: 7/20 - Batch: 35/73 - Train loss: 6.11129\n",
      "Epoch: 7/20 - Batch: 40/73 - Train loss: 6.01103\n",
      "Epoch: 7/20 - Batch: 45/73 - Train loss: 5.96809\n",
      "Epoch: 7/20 - Batch: 50/73 - Train loss: 5.97612\n",
      "Epoch: 7/20 - Batch: 55/73 - Train loss: 5.95939\n",
      "Epoch: 7/20 - Batch: 60/73 - Train loss: 5.98646\n",
      "Epoch: 7/20 - Batch: 65/73 - Train loss: 5.94781\n",
      "Epoch: 7/20 - Batch: 70/73 - Train loss: 6.17751\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 6.05844\n",
      "\n",
      "Epoch: 7/20 - Batch: 5/18 - Val loss: 6.17618\n",
      "Epoch: 7/20 - Batch: 10/18 - Val loss: 6.27739\n",
      "Epoch: 7/20 - Batch: 15/18 - Val loss: 6.13716\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 6.16452\n",
      "\n",
      "Epoch: 8/20 - Batch: 5/73 - Train loss: 6.06877\n",
      "Epoch: 8/20 - Batch: 10/73 - Train loss: 5.98193\n",
      "Epoch: 8/20 - Batch: 15/73 - Train loss: 5.87343\n",
      "Epoch: 8/20 - Batch: 20/73 - Train loss: 6.00941\n",
      "Epoch: 8/20 - Batch: 25/73 - Train loss: 5.97447\n",
      "Epoch: 8/20 - Batch: 30/73 - Train loss: 5.90123\n",
      "Epoch: 8/20 - Batch: 35/73 - Train loss: 5.99730\n",
      "Epoch: 8/20 - Batch: 40/73 - Train loss: 5.89109\n",
      "Epoch: 8/20 - Batch: 45/73 - Train loss: 5.85589\n",
      "Epoch: 8/20 - Batch: 50/73 - Train loss: 5.86320\n",
      "Epoch: 8/20 - Batch: 55/73 - Train loss: 5.84470\n",
      "Epoch: 8/20 - Batch: 60/73 - Train loss: 5.90729\n",
      "Epoch: 8/20 - Batch: 65/73 - Train loss: 5.84283\n",
      "Epoch: 8/20 - Batch: 70/73 - Train loss: 6.08012\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 5.94862\n",
      "\n",
      "Epoch: 8/20 - Batch: 5/18 - Val loss: 6.09116\n",
      "Epoch: 8/20 - Batch: 10/18 - Val loss: 6.19542\n",
      "Epoch: 8/20 - Batch: 15/18 - Val loss: 6.05677\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 6.08086\n",
      "\n",
      "Epoch: 9/20 - Batch: 5/73 - Train loss: 5.95370\n",
      "Epoch: 9/20 - Batch: 10/73 - Train loss: 5.87726\n",
      "Epoch: 9/20 - Batch: 15/73 - Train loss: 5.75748\n",
      "Epoch: 9/20 - Batch: 20/73 - Train loss: 5.92083\n",
      "Epoch: 9/20 - Batch: 25/73 - Train loss: 5.87796\n",
      "Epoch: 9/20 - Batch: 30/73 - Train loss: 5.79220\n",
      "Epoch: 9/20 - Batch: 35/73 - Train loss: 5.90315\n",
      "Epoch: 9/20 - Batch: 40/73 - Train loss: 5.77823\n",
      "Epoch: 9/20 - Batch: 45/73 - Train loss: 5.75687\n",
      "Epoch: 9/20 - Batch: 50/73 - Train loss: 5.76802\n",
      "Epoch: 9/20 - Batch: 55/73 - Train loss: 5.74733\n",
      "Epoch: 9/20 - Batch: 60/73 - Train loss: 5.82879\n",
      "Epoch: 9/20 - Batch: 65/73 - Train loss: 5.75480\n",
      "Epoch: 9/20 - Batch: 70/73 - Train loss: 5.98525\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 5.85100\n",
      "\n",
      "Epoch: 9/20 - Batch: 5/18 - Val loss: 6.02079\n",
      "Epoch: 9/20 - Batch: 10/18 - Val loss: 6.12769\n",
      "Epoch: 9/20 - Batch: 15/18 - Val loss: 5.99039\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 6.01109\n",
      "\n",
      "Epoch: 10/20 - Batch: 5/73 - Train loss: 5.85599\n",
      "Epoch: 10/20 - Batch: 10/73 - Train loss: 5.77952\n",
      "Epoch: 10/20 - Batch: 15/73 - Train loss: 5.65400\n",
      "Epoch: 10/20 - Batch: 20/73 - Train loss: 5.82232\n",
      "Epoch: 10/20 - Batch: 25/73 - Train loss: 5.79963\n",
      "Epoch: 10/20 - Batch: 30/73 - Train loss: 5.71172\n",
      "Epoch: 10/20 - Batch: 35/73 - Train loss: 5.81538\n",
      "Epoch: 10/20 - Batch: 40/73 - Train loss: 5.68508\n",
      "Epoch: 10/20 - Batch: 45/73 - Train loss: 5.66196\n",
      "Epoch: 10/20 - Batch: 50/73 - Train loss: 5.68046\n",
      "Epoch: 10/20 - Batch: 55/73 - Train loss: 5.66139\n",
      "Epoch: 10/20 - Batch: 60/73 - Train loss: 5.76069\n",
      "Epoch: 10/20 - Batch: 65/73 - Train loss: 5.66015\n",
      "Epoch: 10/20 - Batch: 70/73 - Train loss: 5.90978\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 5.76218\n",
      "\n",
      "Epoch: 10/20 - Batch: 5/18 - Val loss: 5.96820\n",
      "Epoch: 10/20 - Batch: 10/18 - Val loss: 6.07786\n",
      "Epoch: 10/20 - Batch: 15/18 - Val loss: 5.94260\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 5.95901\n",
      "\n",
      "Epoch: 11/20 - Batch: 5/73 - Train loss: 5.78100\n",
      "Epoch: 11/20 - Batch: 10/73 - Train loss: 5.69005\n",
      "Epoch: 11/20 - Batch: 15/73 - Train loss: 5.56585\n",
      "Epoch: 11/20 - Batch: 20/73 - Train loss: 5.73915\n",
      "Epoch: 11/20 - Batch: 25/73 - Train loss: 5.71517\n",
      "Epoch: 11/20 - Batch: 30/73 - Train loss: 5.64420\n",
      "Epoch: 11/20 - Batch: 35/73 - Train loss: 5.73302\n",
      "Epoch: 11/20 - Batch: 40/73 - Train loss: 5.62534\n",
      "Epoch: 11/20 - Batch: 45/73 - Train loss: 5.59457\n",
      "Epoch: 11/20 - Batch: 50/73 - Train loss: 5.60710\n",
      "Epoch: 11/20 - Batch: 55/73 - Train loss: 5.58044\n",
      "Epoch: 11/20 - Batch: 60/73 - Train loss: 5.69643\n",
      "Epoch: 11/20 - Batch: 65/73 - Train loss: 5.58542\n",
      "Epoch: 11/20 - Batch: 70/73 - Train loss: 5.84683\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.68744\n",
      "\n",
      "Epoch: 11/20 - Batch: 5/18 - Val loss: 5.92478\n",
      "Epoch: 11/20 - Batch: 10/18 - Val loss: 6.03676\n",
      "Epoch: 11/20 - Batch: 15/18 - Val loss: 5.89988\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.91680\n",
      "\n",
      "Epoch: 12/20 - Batch: 5/73 - Train loss: 5.72510\n",
      "Epoch: 12/20 - Batch: 10/73 - Train loss: 5.61529\n",
      "Epoch: 12/20 - Batch: 15/73 - Train loss: 5.48069\n",
      "Epoch: 12/20 - Batch: 20/73 - Train loss: 5.66961\n",
      "Epoch: 12/20 - Batch: 25/73 - Train loss: 5.63661\n",
      "Epoch: 12/20 - Batch: 30/73 - Train loss: 5.54890\n",
      "Epoch: 12/20 - Batch: 35/73 - Train loss: 5.66708\n",
      "Epoch: 12/20 - Batch: 40/73 - Train loss: 5.54753\n",
      "Epoch: 12/20 - Batch: 45/73 - Train loss: 5.53079\n",
      "Epoch: 12/20 - Batch: 50/73 - Train loss: 5.53073\n",
      "Epoch: 12/20 - Batch: 55/73 - Train loss: 5.51966\n",
      "Epoch: 12/20 - Batch: 60/73 - Train loss: 5.62406\n",
      "Epoch: 12/20 - Batch: 65/73 - Train loss: 5.51809\n",
      "Epoch: 12/20 - Batch: 70/73 - Train loss: 5.77588\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 5.61530\n",
      "\n",
      "Epoch: 12/20 - Batch: 5/18 - Val loss: 5.88617\n",
      "Epoch: 12/20 - Batch: 10/18 - Val loss: 5.99472\n",
      "Epoch: 12/20 - Batch: 15/18 - Val loss: 5.86322\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.87756\n",
      "\n",
      "Epoch: 13/20 - Batch: 5/73 - Train loss: 5.65832\n",
      "Epoch: 13/20 - Batch: 10/73 - Train loss: 5.57114\n",
      "Epoch: 13/20 - Batch: 15/73 - Train loss: 5.40963\n",
      "Epoch: 13/20 - Batch: 20/73 - Train loss: 5.61954\n",
      "Epoch: 13/20 - Batch: 25/73 - Train loss: 5.56821\n",
      "Epoch: 13/20 - Batch: 30/73 - Train loss: 5.48550\n",
      "Epoch: 13/20 - Batch: 35/73 - Train loss: 5.59284\n",
      "Epoch: 13/20 - Batch: 40/73 - Train loss: 5.49558\n",
      "Epoch: 13/20 - Batch: 45/73 - Train loss: 5.46725\n",
      "Epoch: 13/20 - Batch: 50/73 - Train loss: 5.48624\n",
      "Epoch: 13/20 - Batch: 55/73 - Train loss: 5.44634\n",
      "Epoch: 13/20 - Batch: 60/73 - Train loss: 5.58038\n",
      "Epoch: 13/20 - Batch: 65/73 - Train loss: 5.47310\n",
      "Epoch: 13/20 - Batch: 70/73 - Train loss: 5.73730\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 5.55548\n",
      "\n",
      "Epoch: 13/20 - Batch: 5/18 - Val loss: 5.86727\n",
      "Epoch: 13/20 - Batch: 10/18 - Val loss: 5.97339\n",
      "Epoch: 13/20 - Batch: 15/18 - Val loss: 5.85036\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.86050\n",
      "\n",
      "Epoch: 14/20 - Batch: 5/73 - Train loss: 5.58617\n",
      "Epoch: 14/20 - Batch: 10/73 - Train loss: 5.50545\n",
      "Epoch: 14/20 - Batch: 15/73 - Train loss: 5.35419\n",
      "Epoch: 14/20 - Batch: 20/73 - Train loss: 5.55858\n",
      "Epoch: 14/20 - Batch: 25/73 - Train loss: 5.52239\n",
      "Epoch: 14/20 - Batch: 30/73 - Train loss: 5.44277\n",
      "Epoch: 14/20 - Batch: 35/73 - Train loss: 5.54551\n",
      "Epoch: 14/20 - Batch: 40/73 - Train loss: 5.43538\n",
      "Epoch: 14/20 - Batch: 45/73 - Train loss: 5.40152\n",
      "Epoch: 14/20 - Batch: 50/73 - Train loss: 5.42042\n",
      "Epoch: 14/20 - Batch: 55/73 - Train loss: 5.39637\n",
      "Epoch: 14/20 - Batch: 60/73 - Train loss: 5.51774\n",
      "Epoch: 14/20 - Batch: 65/73 - Train loss: 5.41237\n",
      "Epoch: 14/20 - Batch: 70/73 - Train loss: 5.69767\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 5.50111\n",
      "\n",
      "Epoch: 14/20 - Batch: 5/18 - Val loss: 5.83401\n",
      "Epoch: 14/20 - Batch: 10/18 - Val loss: 5.93928\n",
      "Epoch: 14/20 - Batch: 15/18 - Val loss: 5.81411\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.82317\n",
      "\n",
      "Epoch: 15/20 - Batch: 5/73 - Train loss: 5.52324\n",
      "Epoch: 15/20 - Batch: 10/73 - Train loss: 5.43565\n",
      "Epoch: 15/20 - Batch: 15/73 - Train loss: 5.29740\n",
      "Epoch: 15/20 - Batch: 20/73 - Train loss: 5.51181\n",
      "Epoch: 15/20 - Batch: 25/73 - Train loss: 5.46395\n",
      "Epoch: 15/20 - Batch: 30/73 - Train loss: 5.38380\n",
      "Epoch: 15/20 - Batch: 35/73 - Train loss: 5.48141\n",
      "Epoch: 15/20 - Batch: 40/73 - Train loss: 5.37476\n",
      "Epoch: 15/20 - Batch: 45/73 - Train loss: 5.34684\n",
      "Epoch: 15/20 - Batch: 50/73 - Train loss: 5.37618\n",
      "Epoch: 15/20 - Batch: 55/73 - Train loss: 5.33648\n",
      "Epoch: 15/20 - Batch: 60/73 - Train loss: 5.45080\n",
      "Epoch: 15/20 - Batch: 65/73 - Train loss: 5.34336\n",
      "Epoch: 15/20 - Batch: 70/73 - Train loss: 5.63327\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 5.44202\n",
      "\n",
      "Epoch: 15/20 - Batch: 5/18 - Val loss: 5.79235\n",
      "Epoch: 15/20 - Batch: 10/18 - Val loss: 5.89524\n",
      "Epoch: 15/20 - Batch: 15/18 - Val loss: 5.77327\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.77962\n",
      "\n",
      "Epoch: 16/20 - Batch: 5/73 - Train loss: 5.45977\n",
      "Epoch: 16/20 - Batch: 10/73 - Train loss: 5.37923\n",
      "Epoch: 16/20 - Batch: 15/73 - Train loss: 5.23107\n",
      "Epoch: 16/20 - Batch: 20/73 - Train loss: 5.44742\n",
      "Epoch: 16/20 - Batch: 25/73 - Train loss: 5.43041\n",
      "Epoch: 16/20 - Batch: 30/73 - Train loss: 5.33359\n",
      "Epoch: 16/20 - Batch: 35/73 - Train loss: 5.42374\n",
      "Epoch: 16/20 - Batch: 40/73 - Train loss: 5.32382\n",
      "Epoch: 16/20 - Batch: 45/73 - Train loss: 5.29735\n",
      "Epoch: 16/20 - Batch: 50/73 - Train loss: 5.31732\n",
      "Epoch: 16/20 - Batch: 55/73 - Train loss: 5.28785\n",
      "Epoch: 16/20 - Batch: 60/73 - Train loss: 5.37962\n",
      "Epoch: 16/20 - Batch: 65/73 - Train loss: 5.29239\n",
      "Epoch: 16/20 - Batch: 70/73 - Train loss: 5.59089\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 5.38355\n",
      "\n",
      "Epoch: 16/20 - Batch: 5/18 - Val loss: 5.76351\n",
      "Epoch: 16/20 - Batch: 10/18 - Val loss: 5.86374\n",
      "Epoch: 16/20 - Batch: 15/18 - Val loss: 5.74240\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.74965\n",
      "\n",
      "Epoch: 17/20 - Batch: 5/73 - Train loss: 5.39529\n",
      "Epoch: 17/20 - Batch: 10/73 - Train loss: 5.32471\n",
      "Epoch: 17/20 - Batch: 15/73 - Train loss: 5.18466\n",
      "Epoch: 17/20 - Batch: 20/73 - Train loss: 5.39439\n",
      "Epoch: 17/20 - Batch: 25/73 - Train loss: 5.36179\n",
      "Epoch: 17/20 - Batch: 30/73 - Train loss: 5.26892\n",
      "Epoch: 17/20 - Batch: 35/73 - Train loss: 5.37059\n",
      "Epoch: 17/20 - Batch: 40/73 - Train loss: 5.26952\n",
      "Epoch: 17/20 - Batch: 45/73 - Train loss: 5.23416\n",
      "Epoch: 17/20 - Batch: 50/73 - Train loss: 5.25590\n",
      "Epoch: 17/20 - Batch: 55/73 - Train loss: 5.23155\n",
      "Epoch: 17/20 - Batch: 60/73 - Train loss: 5.31818\n",
      "Epoch: 17/20 - Batch: 65/73 - Train loss: 5.24468\n",
      "Epoch: 17/20 - Batch: 70/73 - Train loss: 5.53931\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 5.32947\n",
      "\n",
      "Epoch: 17/20 - Batch: 5/18 - Val loss: 5.73782\n",
      "Epoch: 17/20 - Batch: 10/18 - Val loss: 5.84020\n",
      "Epoch: 17/20 - Batch: 15/18 - Val loss: 5.71786\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.72505\n",
      "\n",
      "Epoch: 18/20 - Batch: 5/73 - Train loss: 5.34575\n",
      "Epoch: 18/20 - Batch: 10/73 - Train loss: 5.26854\n",
      "Epoch: 18/20 - Batch: 15/73 - Train loss: 5.11003\n",
      "Epoch: 18/20 - Batch: 20/73 - Train loss: 5.34896\n",
      "Epoch: 18/20 - Batch: 25/73 - Train loss: 5.31544\n",
      "Epoch: 18/20 - Batch: 30/73 - Train loss: 5.23122\n",
      "Epoch: 18/20 - Batch: 35/73 - Train loss: 5.32770\n",
      "Epoch: 18/20 - Batch: 40/73 - Train loss: 5.22762\n",
      "Epoch: 18/20 - Batch: 45/73 - Train loss: 5.18670\n",
      "Epoch: 18/20 - Batch: 50/73 - Train loss: 5.20879\n",
      "Epoch: 18/20 - Batch: 55/73 - Train loss: 5.17485\n",
      "Epoch: 18/20 - Batch: 60/73 - Train loss: 5.27095\n",
      "Epoch: 18/20 - Batch: 65/73 - Train loss: 5.20230\n",
      "Epoch: 18/20 - Batch: 70/73 - Train loss: 5.48213\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 5.27749\n",
      "\n",
      "Epoch: 18/20 - Batch: 5/18 - Val loss: 5.71965\n",
      "Epoch: 18/20 - Batch: 10/18 - Val loss: 5.81780\n",
      "Epoch: 18/20 - Batch: 15/18 - Val loss: 5.70050\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.70522\n",
      "\n",
      "Epoch: 19/20 - Batch: 5/73 - Train loss: 5.29607\n",
      "Epoch: 19/20 - Batch: 10/73 - Train loss: 5.21439\n",
      "Epoch: 19/20 - Batch: 15/73 - Train loss: 5.06888\n",
      "Epoch: 19/20 - Batch: 20/73 - Train loss: 5.30771\n",
      "Epoch: 19/20 - Batch: 25/73 - Train loss: 5.26788\n",
      "Epoch: 19/20 - Batch: 30/73 - Train loss: 5.17595\n",
      "Epoch: 19/20 - Batch: 35/73 - Train loss: 5.26990\n",
      "Epoch: 19/20 - Batch: 40/73 - Train loss: 5.17834\n",
      "Epoch: 19/20 - Batch: 45/73 - Train loss: 5.13710\n",
      "Epoch: 19/20 - Batch: 50/73 - Train loss: 5.16292\n",
      "Epoch: 19/20 - Batch: 55/73 - Train loss: 5.13416\n",
      "Epoch: 19/20 - Batch: 60/73 - Train loss: 5.21002\n",
      "Epoch: 19/20 - Batch: 65/73 - Train loss: 5.14496\n",
      "Epoch: 19/20 - Batch: 70/73 - Train loss: 5.44610\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 5.22814\n",
      "\n",
      "Epoch: 19/20 - Batch: 5/18 - Val loss: 5.69326\n",
      "Epoch: 19/20 - Batch: 10/18 - Val loss: 5.79410\n",
      "Epoch: 19/20 - Batch: 15/18 - Val loss: 5.68112\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.68163\n",
      "\n",
      "Epoch: 20/20 - Batch: 5/73 - Train loss: 5.24671\n",
      "Epoch: 20/20 - Batch: 10/73 - Train loss: 5.16470\n",
      "Epoch: 20/20 - Batch: 15/73 - Train loss: 5.02131\n",
      "Epoch: 20/20 - Batch: 20/73 - Train loss: 5.26564\n",
      "Epoch: 20/20 - Batch: 25/73 - Train loss: 5.21869\n",
      "Epoch: 20/20 - Batch: 30/73 - Train loss: 5.12103\n",
      "Epoch: 20/20 - Batch: 35/73 - Train loss: 5.22435\n",
      "Epoch: 20/20 - Batch: 40/73 - Train loss: 5.13008\n",
      "Epoch: 20/20 - Batch: 45/73 - Train loss: 5.09439\n",
      "Epoch: 20/20 - Batch: 50/73 - Train loss: 5.11408\n",
      "Epoch: 20/20 - Batch: 55/73 - Train loss: 5.09366\n",
      "Epoch: 20/20 - Batch: 60/73 - Train loss: 5.16331\n",
      "Epoch: 20/20 - Batch: 65/73 - Train loss: 5.09764\n",
      "Epoch: 20/20 - Batch: 70/73 - Train loss: 5.40740\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 5.18257\n",
      "\n",
      "Epoch: 20/20 - Batch: 5/18 - Val loss: 5.67840\n",
      "Epoch: 20/20 - Batch: 10/18 - Val loss: 5.77795\n",
      "Epoch: 20/20 - Batch: 15/18 - Val loss: 5.66820\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.66657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ones\n",
    "train_loss, val_loss = train(cond_lstm, batch_size=batch_size, epochs=20, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b900b276-f2c6-430c-97c8-8a5aac95971e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 5/73 - Train loss: 9.62338\n",
      "Epoch: 1/20 - Batch: 10/73 - Train loss: 8.07467\n",
      "Epoch: 1/20 - Batch: 15/73 - Train loss: 7.10355\n",
      "Epoch: 1/20 - Batch: 20/73 - Train loss: 6.98821\n",
      "Epoch: 1/20 - Batch: 25/73 - Train loss: 7.01397\n",
      "Epoch: 1/20 - Batch: 30/73 - Train loss: 6.96979\n",
      "Epoch: 1/20 - Batch: 35/73 - Train loss: 6.98377\n",
      "Epoch: 1/20 - Batch: 40/73 - Train loss: 6.90053\n",
      "Epoch: 1/20 - Batch: 45/73 - Train loss: 6.84488\n",
      "Epoch: 1/20 - Batch: 50/73 - Train loss: 6.85529\n",
      "Epoch: 1/20 - Batch: 55/73 - Train loss: 6.88553\n",
      "Epoch: 1/20 - Batch: 60/73 - Train loss: 7.63706\n",
      "Epoch: 1/20 - Batch: 65/73 - Train loss: 6.86007\n",
      "Epoch: 1/20 - Batch: 70/73 - Train loss: 7.03847\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 7.37194\n",
      "\n",
      "Epoch: 1/20 - Batch: 5/18 - Val loss: 6.96791\n",
      "Epoch: 1/20 - Batch: 10/18 - Val loss: 7.08659\n",
      "Epoch: 1/20 - Batch: 15/18 - Val loss: 7.06550\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.98385\n",
      "\n",
      "Epoch: 2/20 - Batch: 5/73 - Train loss: 7.02748\n",
      "Epoch: 2/20 - Batch: 10/73 - Train loss: 6.89685\n",
      "Epoch: 2/20 - Batch: 15/73 - Train loss: 6.73007\n",
      "Epoch: 2/20 - Batch: 20/73 - Train loss: 6.88781\n",
      "Epoch: 2/20 - Batch: 25/73 - Train loss: 6.90111\n",
      "Epoch: 2/20 - Batch: 30/73 - Train loss: 6.85056\n",
      "Epoch: 2/20 - Batch: 35/73 - Train loss: 6.90862\n",
      "Epoch: 2/20 - Batch: 40/73 - Train loss: 6.83945\n",
      "Epoch: 2/20 - Batch: 45/73 - Train loss: 6.80018\n",
      "Epoch: 2/20 - Batch: 50/73 - Train loss: 6.80388\n",
      "Epoch: 2/20 - Batch: 55/73 - Train loss: 6.84814\n",
      "Epoch: 2/20 - Batch: 60/73 - Train loss: 7.56288\n",
      "Epoch: 2/20 - Batch: 65/73 - Train loss: 6.82043\n",
      "Epoch: 2/20 - Batch: 70/73 - Train loss: 6.99648\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 6.95523\n",
      "\n",
      "Epoch: 2/20 - Batch: 5/18 - Val loss: 6.97393\n",
      "Epoch: 2/20 - Batch: 10/18 - Val loss: 7.09924\n",
      "Epoch: 2/20 - Batch: 15/18 - Val loss: 7.07043\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 6.99123\n",
      "\n",
      "Epoch: 3/20 - Batch: 5/73 - Train loss: 7.00883\n",
      "Epoch: 3/20 - Batch: 10/73 - Train loss: 6.91076\n",
      "Epoch: 3/20 - Batch: 15/73 - Train loss: 6.74670\n",
      "Epoch: 3/20 - Batch: 20/73 - Train loss: 6.89422\n",
      "Epoch: 3/20 - Batch: 25/73 - Train loss: 6.90548\n",
      "Epoch: 3/20 - Batch: 30/73 - Train loss: 6.85446\n",
      "Epoch: 3/20 - Batch: 35/73 - Train loss: 6.90805\n",
      "Epoch: 3/20 - Batch: 40/73 - Train loss: 6.83737\n",
      "Epoch: 3/20 - Batch: 45/73 - Train loss: 6.80076\n",
      "Epoch: 3/20 - Batch: 50/73 - Train loss: 6.79889\n",
      "Epoch: 3/20 - Batch: 55/73 - Train loss: 6.84319\n",
      "Epoch: 3/20 - Batch: 60/73 - Train loss: 7.29426\n",
      "Epoch: 3/20 - Batch: 65/73 - Train loss: 6.80846\n",
      "Epoch: 3/20 - Batch: 70/73 - Train loss: 6.98485\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.92772\n",
      "\n",
      "Epoch: 3/20 - Batch: 5/18 - Val loss: 6.95304\n",
      "Epoch: 3/20 - Batch: 10/18 - Val loss: 7.04771\n",
      "Epoch: 3/20 - Batch: 15/18 - Val loss: 6.98678\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.95579\n",
      "\n",
      "Epoch: 4/20 - Batch: 5/73 - Train loss: 6.98118\n",
      "Epoch: 4/20 - Batch: 10/73 - Train loss: 6.89652\n",
      "Epoch: 4/20 - Batch: 15/73 - Train loss: 6.73539\n",
      "Epoch: 4/20 - Batch: 20/73 - Train loss: 6.88254\n",
      "Epoch: 4/20 - Batch: 25/73 - Train loss: 6.89467\n",
      "Epoch: 4/20 - Batch: 30/73 - Train loss: 6.84557\n",
      "Epoch: 4/20 - Batch: 35/73 - Train loss: 6.90195\n",
      "Epoch: 4/20 - Batch: 40/73 - Train loss: 6.82883\n",
      "Epoch: 4/20 - Batch: 45/73 - Train loss: 6.79684\n",
      "Epoch: 4/20 - Batch: 50/73 - Train loss: 6.79501\n",
      "Epoch: 4/20 - Batch: 55/73 - Train loss: 6.83864\n",
      "Epoch: 4/20 - Batch: 60/73 - Train loss: 7.05395\n",
      "Epoch: 4/20 - Batch: 65/73 - Train loss: 6.80192\n",
      "Epoch: 4/20 - Batch: 70/73 - Train loss: 6.97957\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 6.89325\n",
      "\n",
      "Epoch: 4/20 - Batch: 5/18 - Val loss: 6.93870\n",
      "Epoch: 4/20 - Batch: 10/18 - Val loss: 7.00708\n",
      "Epoch: 4/20 - Batch: 15/18 - Val loss: 6.91658\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 6.92598\n",
      "\n",
      "Epoch: 5/20 - Batch: 5/73 - Train loss: 6.93251\n",
      "Epoch: 5/20 - Batch: 10/73 - Train loss: 6.88621\n",
      "Epoch: 5/20 - Batch: 15/73 - Train loss: 6.72767\n",
      "Epoch: 5/20 - Batch: 20/73 - Train loss: 6.86866\n",
      "Epoch: 5/20 - Batch: 25/73 - Train loss: 6.88250\n",
      "Epoch: 5/20 - Batch: 30/73 - Train loss: 6.83516\n",
      "Epoch: 5/20 - Batch: 35/73 - Train loss: 6.89032\n",
      "Epoch: 5/20 - Batch: 40/73 - Train loss: 6.81856\n",
      "Epoch: 5/20 - Batch: 45/73 - Train loss: 6.78464\n",
      "Epoch: 5/20 - Batch: 50/73 - Train loss: 6.78178\n",
      "Epoch: 5/20 - Batch: 55/73 - Train loss: 6.81728\n",
      "Epoch: 5/20 - Batch: 60/73 - Train loss: 6.71907\n",
      "Epoch: 5/20 - Batch: 65/73 - Train loss: 6.76019\n",
      "Epoch: 5/20 - Batch: 70/73 - Train loss: 6.90908\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 6.84555\n",
      "\n",
      "Epoch: 5/20 - Batch: 5/18 - Val loss: 6.86080\n",
      "Epoch: 5/20 - Batch: 10/18 - Val loss: 6.91628\n",
      "Epoch: 5/20 - Batch: 15/18 - Val loss: 6.80369\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 6.84158\n",
      "\n",
      "Epoch: 6/20 - Batch: 5/73 - Train loss: 6.83787\n",
      "Epoch: 6/20 - Batch: 10/73 - Train loss: 6.79055\n",
      "Epoch: 6/20 - Batch: 15/73 - Train loss: 6.62531\n",
      "Epoch: 6/20 - Batch: 25/73 - Train loss: 6.75858\n",
      "Epoch: 6/20 - Batch: 30/73 - Train loss: 6.69456\n",
      "Epoch: 6/20 - Batch: 35/73 - Train loss: 6.73449\n",
      "Epoch: 6/20 - Batch: 40/73 - Train loss: 6.64582\n",
      "Epoch: 6/20 - Batch: 45/73 - Train loss: 6.60389\n",
      "Epoch: 6/20 - Batch: 50/73 - Train loss: 6.60223\n",
      "Epoch: 6/20 - Batch: 55/73 - Train loss: 6.62005\n",
      "Epoch: 6/20 - Batch: 60/73 - Train loss: 6.49419\n",
      "Epoch: 6/20 - Batch: 65/73 - Train loss: 6.57769\n",
      "Epoch: 6/20 - Batch: 70/73 - Train loss: 6.74163\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 6.69075\n",
      "\n",
      "Epoch: 6/20 - Batch: 5/18 - Val loss: 6.69512\n",
      "Epoch: 6/20 - Batch: 10/18 - Val loss: 6.75882\n",
      "Epoch: 6/20 - Batch: 15/18 - Val loss: 6.62932\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 6.67409\n",
      "\n",
      "Epoch: 7/20 - Batch: 5/73 - Train loss: 6.67014\n",
      "Epoch: 7/20 - Batch: 10/73 - Train loss: 6.60848\n",
      "Epoch: 7/20 - Batch: 15/73 - Train loss: 6.46357\n",
      "Epoch: 7/20 - Batch: 25/73 - Train loss: 6.57466\n",
      "Epoch: 7/20 - Batch: 30/73 - Train loss: 6.50505\n",
      "Epoch: 7/20 - Batch: 35/73 - Train loss: 6.55705\n",
      "Epoch: 7/20 - Batch: 40/73 - Train loss: 6.47232\n",
      "Epoch: 7/20 - Batch: 45/73 - Train loss: 6.42809\n",
      "Epoch: 7/20 - Batch: 50/73 - Train loss: 6.42264\n",
      "Epoch: 7/20 - Batch: 55/73 - Train loss: 6.43663\n",
      "Epoch: 7/20 - Batch: 60/73 - Train loss: 6.33213\n",
      "Epoch: 7/20 - Batch: 65/73 - Train loss: 6.38736\n",
      "Epoch: 7/20 - Batch: 70/73 - Train loss: 6.57859\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 6.51407\n",
      "\n",
      "Epoch: 7/20 - Batch: 5/18 - Val loss: 6.50585\n",
      "Epoch: 7/20 - Batch: 10/18 - Val loss: 6.57861\n",
      "Epoch: 7/20 - Batch: 15/18 - Val loss: 6.44347\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 6.48696\n",
      "\n",
      "Epoch: 8/20 - Batch: 5/73 - Train loss: 6.48426\n",
      "Epoch: 8/20 - Batch: 10/73 - Train loss: 6.41034\n",
      "Epoch: 8/20 - Batch: 15/73 - Train loss: 6.27101\n",
      "Epoch: 8/20 - Batch: 20/73 - Train loss: 6.38597\n",
      "Epoch: 8/20 - Batch: 25/73 - Train loss: 6.38693\n",
      "Epoch: 8/20 - Batch: 30/73 - Train loss: 6.31094\n",
      "Epoch: 8/20 - Batch: 35/73 - Train loss: 6.37571\n",
      "Epoch: 8/20 - Batch: 40/73 - Train loss: 6.28702\n",
      "Epoch: 8/20 - Batch: 45/73 - Train loss: 6.24220\n",
      "Epoch: 8/20 - Batch: 50/73 - Train loss: 6.24353\n",
      "Epoch: 8/20 - Batch: 55/73 - Train loss: 6.25104\n",
      "Epoch: 8/20 - Batch: 60/73 - Train loss: 6.17684\n",
      "Epoch: 8/20 - Batch: 65/73 - Train loss: 6.21309\n",
      "Epoch: 8/20 - Batch: 70/73 - Train loss: 6.41925\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 6.33300\n",
      "\n",
      "Epoch: 8/20 - Batch: 5/18 - Val loss: 6.35936\n",
      "Epoch: 8/20 - Batch: 10/18 - Val loss: 6.43145\n",
      "Epoch: 8/20 - Batch: 15/18 - Val loss: 6.29925\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 6.33556\n",
      "\n",
      "Epoch: 9/20 - Batch: 5/73 - Train loss: 6.31540\n",
      "Epoch: 9/20 - Batch: 10/73 - Train loss: 6.23266\n",
      "Epoch: 9/20 - Batch: 15/73 - Train loss: 6.10054\n",
      "Epoch: 9/20 - Batch: 20/73 - Train loss: 6.22532\n",
      "Epoch: 9/20 - Batch: 25/73 - Train loss: 6.20928\n",
      "Epoch: 9/20 - Batch: 30/73 - Train loss: 6.12965\n",
      "Epoch: 9/20 - Batch: 35/73 - Train loss: 6.20763\n",
      "Epoch: 9/20 - Batch: 40/73 - Train loss: 6.10704\n",
      "Epoch: 9/20 - Batch: 45/73 - Train loss: 6.05707\n",
      "Epoch: 9/20 - Batch: 50/73 - Train loss: 6.07197\n",
      "Epoch: 9/20 - Batch: 55/73 - Train loss: 6.06928\n",
      "Epoch: 9/20 - Batch: 60/73 - Train loss: 6.02438\n",
      "Epoch: 9/20 - Batch: 65/73 - Train loss: 6.04183\n",
      "Epoch: 9/20 - Batch: 70/73 - Train loss: 6.25862\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 6.16054\n",
      "\n",
      "Epoch: 9/20 - Batch: 5/18 - Val loss: 6.20655\n",
      "Epoch: 9/20 - Batch: 10/18 - Val loss: 6.28129\n",
      "Epoch: 9/20 - Batch: 15/18 - Val loss: 6.14730\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 6.18059\n",
      "\n",
      "Epoch: 10/20 - Batch: 5/73 - Train loss: 6.14501\n",
      "Epoch: 10/20 - Batch: 10/73 - Train loss: 6.05768\n",
      "Epoch: 10/20 - Batch: 15/73 - Train loss: 5.93844\n",
      "Epoch: 10/20 - Batch: 20/73 - Train loss: 6.07052\n",
      "Epoch: 10/20 - Batch: 25/73 - Train loss: 6.03930\n",
      "Epoch: 10/20 - Batch: 30/73 - Train loss: 5.95457\n",
      "Epoch: 10/20 - Batch: 35/73 - Train loss: 6.03802\n",
      "Epoch: 10/20 - Batch: 40/73 - Train loss: 5.93713\n",
      "Epoch: 10/20 - Batch: 45/73 - Train loss: 5.88470\n",
      "Epoch: 10/20 - Batch: 50/73 - Train loss: 5.91022\n",
      "Epoch: 10/20 - Batch: 55/73 - Train loss: 5.89575\n",
      "Epoch: 10/20 - Batch: 60/73 - Train loss: 5.89945\n",
      "Epoch: 10/20 - Batch: 65/73 - Train loss: 5.88252\n",
      "Epoch: 10/20 - Batch: 70/73 - Train loss: 6.12206\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 5.99678\n",
      "\n",
      "Epoch: 10/20 - Batch: 5/18 - Val loss: 6.07588\n",
      "Epoch: 10/20 - Batch: 10/18 - Val loss: 6.14979\n",
      "Epoch: 10/20 - Batch: 15/18 - Val loss: 6.02123\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 6.04817\n",
      "\n",
      "Epoch: 11/20 - Batch: 5/73 - Train loss: 5.98961\n",
      "Epoch: 11/20 - Batch: 10/73 - Train loss: 5.89237\n",
      "Epoch: 11/20 - Batch: 15/73 - Train loss: 5.78544\n",
      "Epoch: 11/20 - Batch: 20/73 - Train loss: 5.93111\n",
      "Epoch: 11/20 - Batch: 25/73 - Train loss: 5.88557\n",
      "Epoch: 11/20 - Batch: 30/73 - Train loss: 5.79360\n",
      "Epoch: 11/20 - Batch: 35/73 - Train loss: 5.88567\n",
      "Epoch: 11/20 - Batch: 40/73 - Train loss: 5.78832\n",
      "Epoch: 11/20 - Batch: 45/73 - Train loss: 5.74003\n",
      "Epoch: 11/20 - Batch: 50/73 - Train loss: 5.76765\n",
      "Epoch: 11/20 - Batch: 55/73 - Train loss: 5.74813\n",
      "Epoch: 11/20 - Batch: 60/73 - Train loss: 5.77246\n",
      "Epoch: 11/20 - Batch: 65/73 - Train loss: 5.74964\n",
      "Epoch: 11/20 - Batch: 70/73 - Train loss: 6.00324\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.85145\n",
      "\n",
      "Epoch: 11/20 - Batch: 5/18 - Val loss: 5.97370\n",
      "Epoch: 11/20 - Batch: 10/18 - Val loss: 6.04713\n",
      "Epoch: 11/20 - Batch: 15/18 - Val loss: 5.92059\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.94487\n",
      "\n",
      "Epoch: 12/20 - Batch: 5/73 - Train loss: 5.85618\n",
      "Epoch: 12/20 - Batch: 10/73 - Train loss: 5.75568\n",
      "Epoch: 12/20 - Batch: 15/73 - Train loss: 5.64606\n",
      "Epoch: 12/20 - Batch: 20/73 - Train loss: 5.81557\n",
      "Epoch: 12/20 - Batch: 25/73 - Train loss: 5.75961\n",
      "Epoch: 12/20 - Batch: 30/73 - Train loss: 5.66442\n",
      "Epoch: 12/20 - Batch: 35/73 - Train loss: 5.76360\n",
      "Epoch: 12/20 - Batch: 40/73 - Train loss: 5.66200\n",
      "Epoch: 12/20 - Batch: 45/73 - Train loss: 5.61513\n",
      "Epoch: 12/20 - Batch: 50/73 - Train loss: 5.64629\n",
      "Epoch: 12/20 - Batch: 55/73 - Train loss: 5.63391\n",
      "Epoch: 12/20 - Batch: 60/73 - Train loss: 5.68372\n",
      "Epoch: 12/20 - Batch: 65/73 - Train loss: 5.63049\n",
      "Epoch: 12/20 - Batch: 70/73 - Train loss: 5.90541\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 5.73120\n",
      "\n",
      "Epoch: 12/20 - Batch: 5/18 - Val loss: 5.88642\n",
      "Epoch: 12/20 - Batch: 10/18 - Val loss: 5.96357\n",
      "Epoch: 12/20 - Batch: 15/18 - Val loss: 5.84016\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.85929\n",
      "\n",
      "Epoch: 13/20 - Batch: 5/73 - Train loss: 5.74338\n",
      "Epoch: 13/20 - Batch: 10/73 - Train loss: 5.64490\n",
      "Epoch: 13/20 - Batch: 15/73 - Train loss: 5.52783\n",
      "Epoch: 13/20 - Batch: 20/73 - Train loss: 5.71952\n",
      "Epoch: 13/20 - Batch: 25/73 - Train loss: 5.65203\n",
      "Epoch: 13/20 - Batch: 30/73 - Train loss: 5.55905\n",
      "Epoch: 13/20 - Batch: 35/73 - Train loss: 5.65974\n",
      "Epoch: 13/20 - Batch: 40/73 - Train loss: 5.55299\n",
      "Epoch: 13/20 - Batch: 45/73 - Train loss: 5.50866\n",
      "Epoch: 13/20 - Batch: 50/73 - Train loss: 5.54166\n",
      "Epoch: 13/20 - Batch: 55/73 - Train loss: 5.53041\n",
      "Epoch: 13/20 - Batch: 60/73 - Train loss: 5.60097\n",
      "Epoch: 13/20 - Batch: 65/73 - Train loss: 5.52344\n",
      "Epoch: 13/20 - Batch: 70/73 - Train loss: 5.82135\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 5.62835\n",
      "\n",
      "Epoch: 13/20 - Batch: 5/18 - Val loss: 5.82263\n",
      "Epoch: 13/20 - Batch: 10/18 - Val loss: 5.90169\n",
      "Epoch: 13/20 - Batch: 15/18 - Val loss: 5.78101\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.79756\n",
      "\n",
      "Epoch: 14/20 - Batch: 5/73 - Train loss: 5.64371\n",
      "Epoch: 14/20 - Batch: 10/73 - Train loss: 5.54812\n",
      "Epoch: 14/20 - Batch: 15/73 - Train loss: 5.41764\n",
      "Epoch: 14/20 - Batch: 20/73 - Train loss: 5.63632\n",
      "Epoch: 14/20 - Batch: 25/73 - Train loss: 5.55150\n",
      "Epoch: 14/20 - Batch: 30/73 - Train loss: 5.46806\n",
      "Epoch: 14/20 - Batch: 35/73 - Train loss: 5.56603\n",
      "Epoch: 14/20 - Batch: 40/73 - Train loss: 5.45420\n",
      "Epoch: 14/20 - Batch: 45/73 - Train loss: 5.41585\n",
      "Epoch: 14/20 - Batch: 50/73 - Train loss: 5.44717\n",
      "Epoch: 14/20 - Batch: 55/73 - Train loss: 5.43821\n",
      "Epoch: 14/20 - Batch: 60/73 - Train loss: 5.52538\n",
      "Epoch: 14/20 - Batch: 65/73 - Train loss: 5.42952\n",
      "Epoch: 14/20 - Batch: 70/73 - Train loss: 5.74055\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 5.53626\n",
      "\n",
      "Epoch: 14/20 - Batch: 5/18 - Val loss: 5.76930\n",
      "Epoch: 14/20 - Batch: 10/18 - Val loss: 5.84569\n",
      "Epoch: 14/20 - Batch: 15/18 - Val loss: 5.72964\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.74474\n",
      "\n",
      "Epoch: 15/20 - Batch: 5/73 - Train loss: 5.55318\n",
      "Epoch: 15/20 - Batch: 10/73 - Train loss: 5.45851\n",
      "Epoch: 15/20 - Batch: 15/73 - Train loss: 5.31302\n",
      "Epoch: 15/20 - Batch: 20/73 - Train loss: 5.56913\n",
      "Epoch: 15/20 - Batch: 25/73 - Train loss: 5.46038\n",
      "Epoch: 15/20 - Batch: 30/73 - Train loss: 5.38115\n",
      "Epoch: 15/20 - Batch: 35/73 - Train loss: 5.48978\n",
      "Epoch: 15/20 - Batch: 40/73 - Train loss: 5.37070\n",
      "Epoch: 15/20 - Batch: 45/73 - Train loss: 5.33281\n",
      "Epoch: 15/20 - Batch: 50/73 - Train loss: 5.36686\n",
      "Epoch: 15/20 - Batch: 55/73 - Train loss: 5.35553\n",
      "Epoch: 15/20 - Batch: 60/73 - Train loss: 5.45693\n",
      "Epoch: 15/20 - Batch: 65/73 - Train loss: 5.34840\n",
      "Epoch: 15/20 - Batch: 70/73 - Train loss: 5.66775\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 5.45357\n",
      "\n",
      "Epoch: 15/20 - Batch: 5/18 - Val loss: 5.72230\n",
      "Epoch: 15/20 - Batch: 10/18 - Val loss: 5.79618\n",
      "Epoch: 15/20 - Batch: 15/18 - Val loss: 5.68367\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.69803\n",
      "\n",
      "Epoch: 16/20 - Batch: 5/73 - Train loss: 5.47028\n",
      "Epoch: 16/20 - Batch: 10/73 - Train loss: 5.36646\n",
      "Epoch: 16/20 - Batch: 15/73 - Train loss: 5.22586\n",
      "Epoch: 16/20 - Batch: 20/73 - Train loss: 5.49805\n",
      "Epoch: 16/20 - Batch: 25/73 - Train loss: 5.39726\n",
      "Epoch: 16/20 - Batch: 30/73 - Train loss: 5.29170\n",
      "Epoch: 16/20 - Batch: 35/73 - Train loss: 5.41254\n",
      "Epoch: 16/20 - Batch: 40/73 - Train loss: 5.30369\n",
      "Epoch: 16/20 - Batch: 45/73 - Train loss: 5.25558\n",
      "Epoch: 16/20 - Batch: 50/73 - Train loss: 5.28819\n",
      "Epoch: 16/20 - Batch: 55/73 - Train loss: 5.28935\n",
      "Epoch: 16/20 - Batch: 60/73 - Train loss: 5.40426\n",
      "Epoch: 16/20 - Batch: 65/73 - Train loss: 5.27194\n",
      "Epoch: 16/20 - Batch: 70/73 - Train loss: 5.59294\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 5.37787\n",
      "\n",
      "Epoch: 16/20 - Batch: 5/18 - Val loss: 5.67621\n",
      "Epoch: 16/20 - Batch: 10/18 - Val loss: 5.74593\n",
      "Epoch: 16/20 - Batch: 15/18 - Val loss: 5.63883\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.64888\n",
      "\n",
      "Epoch: 17/20 - Batch: 5/73 - Train loss: 5.38472\n",
      "Epoch: 17/20 - Batch: 10/73 - Train loss: 5.27486\n",
      "Epoch: 17/20 - Batch: 15/73 - Train loss: 5.14119\n",
      "Epoch: 17/20 - Batch: 20/73 - Train loss: 5.41826\n",
      "Epoch: 17/20 - Batch: 25/73 - Train loss: 5.32366\n",
      "Epoch: 17/20 - Batch: 30/73 - Train loss: 5.22812\n",
      "Epoch: 17/20 - Batch: 35/73 - Train loss: 5.33282\n",
      "Epoch: 17/20 - Batch: 40/73 - Train loss: 5.22704\n",
      "Epoch: 17/20 - Batch: 45/73 - Train loss: 5.18685\n",
      "Epoch: 17/20 - Batch: 50/73 - Train loss: 5.22668\n",
      "Epoch: 17/20 - Batch: 55/73 - Train loss: 5.21369\n",
      "Epoch: 17/20 - Batch: 60/73 - Train loss: 5.35024\n",
      "Epoch: 17/20 - Batch: 65/73 - Train loss: 5.20402\n",
      "Epoch: 17/20 - Batch: 70/73 - Train loss: 5.52643\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 5.30481\n",
      "\n",
      "Epoch: 17/20 - Batch: 5/18 - Val loss: 5.64890\n",
      "Epoch: 17/20 - Batch: 10/18 - Val loss: 5.71856\n",
      "Epoch: 17/20 - Batch: 15/18 - Val loss: 5.61309\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.62022\n",
      "\n",
      "Epoch: 18/20 - Batch: 5/73 - Train loss: 5.31645\n",
      "Epoch: 18/20 - Batch: 10/73 - Train loss: 5.20131\n",
      "Epoch: 18/20 - Batch: 15/73 - Train loss: 5.04956\n",
      "Epoch: 18/20 - Batch: 20/73 - Train loss: 5.34768\n",
      "Epoch: 18/20 - Batch: 25/73 - Train loss: 5.24682\n",
      "Epoch: 18/20 - Batch: 30/73 - Train loss: 5.16063\n",
      "Epoch: 18/20 - Batch: 35/73 - Train loss: 5.27466\n",
      "Epoch: 18/20 - Batch: 40/73 - Train loss: 5.16148\n",
      "Epoch: 18/20 - Batch: 45/73 - Train loss: 5.11064\n",
      "Epoch: 18/20 - Batch: 50/73 - Train loss: 5.14536\n",
      "Epoch: 18/20 - Batch: 55/73 - Train loss: 5.14029\n",
      "Epoch: 18/20 - Batch: 60/73 - Train loss: 5.28750\n",
      "Epoch: 18/20 - Batch: 65/73 - Train loss: 5.14181\n",
      "Epoch: 18/20 - Batch: 70/73 - Train loss: 5.46876\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 5.23818\n",
      "\n",
      "Epoch: 18/20 - Batch: 5/18 - Val loss: 5.62766\n",
      "Epoch: 18/20 - Batch: 10/18 - Val loss: 5.69996\n",
      "Epoch: 18/20 - Batch: 15/18 - Val loss: 5.59679\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.60170\n",
      "\n",
      "Epoch: 19/20 - Batch: 5/73 - Train loss: 5.26286\n",
      "Epoch: 19/20 - Batch: 10/73 - Train loss: 5.14785\n",
      "Epoch: 19/20 - Batch: 15/73 - Train loss: 4.97330\n",
      "Epoch: 19/20 - Batch: 20/73 - Train loss: 5.28498\n",
      "Epoch: 19/20 - Batch: 25/73 - Train loss: 5.17504\n",
      "Epoch: 19/20 - Batch: 30/73 - Train loss: 5.08237\n",
      "Epoch: 19/20 - Batch: 35/73 - Train loss: 5.19584\n",
      "Epoch: 19/20 - Batch: 40/73 - Train loss: 5.09266\n",
      "Epoch: 19/20 - Batch: 45/73 - Train loss: 5.04886\n",
      "Epoch: 19/20 - Batch: 50/73 - Train loss: 5.08498\n",
      "Epoch: 19/20 - Batch: 55/73 - Train loss: 5.08154\n",
      "Epoch: 19/20 - Batch: 60/73 - Train loss: 5.23141\n",
      "Epoch: 19/20 - Batch: 65/73 - Train loss: 5.07717\n",
      "Epoch: 19/20 - Batch: 70/73 - Train loss: 5.40892\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 5.17223\n",
      "\n",
      "Epoch: 19/20 - Batch: 5/18 - Val loss: 5.60548\n",
      "Epoch: 19/20 - Batch: 10/18 - Val loss: 5.67664\n",
      "Epoch: 19/20 - Batch: 15/18 - Val loss: 5.57168\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.57802\n",
      "\n",
      "Epoch: 20/20 - Batch: 5/73 - Train loss: 5.19195\n",
      "Epoch: 20/20 - Batch: 10/73 - Train loss: 5.08881\n",
      "Epoch: 20/20 - Batch: 15/73 - Train loss: 4.91180\n",
      "Epoch: 20/20 - Batch: 20/73 - Train loss: 5.23785\n",
      "Epoch: 20/20 - Batch: 25/73 - Train loss: 5.11346\n",
      "Epoch: 20/20 - Batch: 30/73 - Train loss: 5.02198\n",
      "Epoch: 20/20 - Batch: 35/73 - Train loss: 5.13379\n",
      "Epoch: 20/20 - Batch: 40/73 - Train loss: 5.03083\n",
      "Epoch: 20/20 - Batch: 45/73 - Train loss: 4.98477\n",
      "Epoch: 20/20 - Batch: 50/73 - Train loss: 5.02596\n",
      "Epoch: 20/20 - Batch: 55/73 - Train loss: 5.02932\n",
      "Epoch: 20/20 - Batch: 60/73 - Train loss: 5.19876\n",
      "Epoch: 20/20 - Batch: 65/73 - Train loss: 5.02390\n",
      "Epoch: 20/20 - Batch: 70/73 - Train loss: 5.34927\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 5.11233\n",
      "\n",
      "Epoch: 20/20 - Batch: 5/18 - Val loss: 5.57801\n",
      "Epoch: 20/20 - Batch: 10/18 - Val loss: 5.64966\n",
      "Epoch: 20/20 - Batch: 15/18 - Val loss: 5.54289\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.55018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ones\n",
    "train_loss_w2v, val_loss_w2v = train(cond_lstm_w2v, batch_size=batch_size, epochs=20, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5544417b-2df0-41f9-96f1-641e75a94c02",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 - Batch: 9/73 - Train loss: 5.06064\n",
      "Epoch: 1/50 - Batch: 18/73 - Train loss: 5.31354\n",
      "Epoch: 1/50 - Batch: 27/73 - Train loss: 5.05585\n",
      "Epoch: 1/50 - Batch: 36/73 - Train loss: 5.09107\n",
      "Epoch: 1/50 - Batch: 45/73 - Train loss: 4.96276\n",
      "Epoch: 1/50 - Batch: 54/73 - Train loss: 4.85034\n",
      "Epoch: 1/50 - Batch: 63/73 - Train loss: 4.96236\n",
      "Epoch: 1/50 - Batch: 72/73 - Train loss: 4.96441\n",
      "\n",
      "Epoch: 1/50 - Average train loss: 5.10028\n",
      "\n",
      "Epoch: 1/50 - Batch: 9/18 - Val loss: 5.55602\n",
      "Epoch: 1/50 - Batch: 18/18 - Val loss: 5.53312\n",
      "\n",
      "Epoch: 1/50 - Average val loss: 5.55398\n",
      "\n",
      "Epoch: 2/50 - Batch: 9/73 - Train loss: 4.95294\n",
      "Epoch: 2/50 - Batch: 18/73 - Train loss: 5.18676\n",
      "Epoch: 2/50 - Batch: 27/73 - Train loss: 4.95267\n",
      "Epoch: 2/50 - Batch: 36/73 - Train loss: 4.98276\n",
      "Epoch: 2/50 - Batch: 45/73 - Train loss: 4.87594\n",
      "Epoch: 2/50 - Batch: 54/73 - Train loss: 4.76573\n",
      "Epoch: 2/50 - Batch: 63/73 - Train loss: 4.86437\n",
      "Epoch: 2/50 - Batch: 72/73 - Train loss: 4.87721\n",
      "\n",
      "Epoch: 2/50 - Average train loss: 4.99007\n",
      "\n",
      "Epoch: 2/50 - Batch: 9/18 - Val loss: 5.54532\n",
      "Epoch: 2/50 - Batch: 18/18 - Val loss: 5.51739\n",
      "\n",
      "Epoch: 2/50 - Average val loss: 5.53955\n",
      "\n",
      "Epoch: 3/50 - Batch: 9/73 - Train loss: 4.89986\n",
      "Epoch: 3/50 - Batch: 18/73 - Train loss: 5.11660\n",
      "Epoch: 3/50 - Batch: 27/73 - Train loss: 4.87566\n",
      "Epoch: 3/50 - Batch: 36/73 - Train loss: 4.90272\n",
      "Epoch: 3/50 - Batch: 45/73 - Train loss: 4.80322\n",
      "Epoch: 3/50 - Batch: 54/73 - Train loss: 4.70272\n",
      "Epoch: 3/50 - Batch: 63/73 - Train loss: 4.79531\n",
      "Epoch: 3/50 - Batch: 72/73 - Train loss: 4.80927\n",
      "\n",
      "Epoch: 3/50 - Average train loss: 4.91992\n",
      "\n",
      "Epoch: 3/50 - Batch: 9/18 - Val loss: 5.54936\n",
      "Epoch: 3/50 - Batch: 18/18 - Val loss: 5.51894\n",
      "\n",
      "Epoch: 3/50 - Average val loss: 5.54045\n",
      "\n",
      "Epoch: 4/50 - Batch: 9/73 - Train loss: 4.83819\n",
      "Epoch: 4/50 - Batch: 18/73 - Train loss: 5.04231\n",
      "Epoch: 4/50 - Batch: 27/73 - Train loss: 4.81120\n",
      "Epoch: 4/50 - Batch: 36/73 - Train loss: 4.83088\n",
      "Epoch: 4/50 - Batch: 45/73 - Train loss: 4.73837\n",
      "Epoch: 4/50 - Batch: 54/73 - Train loss: 4.64488\n",
      "Epoch: 4/50 - Batch: 63/73 - Train loss: 4.72801\n",
      "Epoch: 4/50 - Batch: 72/73 - Train loss: 4.74660\n",
      "\n",
      "Epoch: 4/50 - Average train loss: 4.85461\n",
      "\n",
      "Epoch: 4/50 - Batch: 9/18 - Val loss: 5.55636\n",
      "Epoch: 4/50 - Batch: 18/18 - Val loss: 5.52222\n",
      "\n",
      "Epoch: 4/50 - Average val loss: 5.54323\n",
      "\n",
      "Epoch: 5/50 - Batch: 9/73 - Train loss: 4.78144\n",
      "Epoch: 5/50 - Batch: 18/73 - Train loss: 4.97572\n",
      "Epoch: 5/50 - Batch: 27/73 - Train loss: 4.75133\n",
      "Epoch: 5/50 - Batch: 36/73 - Train loss: 4.75889\n",
      "Epoch: 5/50 - Batch: 45/73 - Train loss: 4.67961\n",
      "Epoch: 5/50 - Batch: 54/73 - Train loss: 4.59129\n",
      "Epoch: 5/50 - Batch: 63/73 - Train loss: 4.66779\n",
      "Epoch: 5/50 - Batch: 72/73 - Train loss: 4.69130\n",
      "\n",
      "Epoch: 5/50 - Average train loss: 4.79254\n",
      "\n",
      "Epoch: 5/50 - Batch: 9/18 - Val loss: 5.56450\n",
      "Epoch: 5/50 - Batch: 18/18 - Val loss: 5.52665\n",
      "\n",
      "Epoch: 5/50 - Average val loss: 5.54709\n",
      "\n",
      "Epoch: 6/50 - Batch: 9/73 - Train loss: 4.72830\n",
      "Epoch: 6/50 - Batch: 18/73 - Train loss: 4.91310\n",
      "Epoch: 6/50 - Batch: 27/73 - Train loss: 4.69372\n",
      "Epoch: 6/50 - Batch: 36/73 - Train loss: 4.69519\n",
      "Epoch: 6/50 - Batch: 45/73 - Train loss: 4.62692\n",
      "Epoch: 6/50 - Batch: 54/73 - Train loss: 4.53984\n",
      "Epoch: 6/50 - Batch: 63/73 - Train loss: 4.60847\n",
      "Epoch: 6/50 - Batch: 72/73 - Train loss: 4.63742\n",
      "\n",
      "Epoch: 6/50 - Average train loss: 4.73451\n",
      "\n",
      "Epoch: 6/50 - Batch: 9/18 - Val loss: 5.57636\n",
      "Epoch: 6/50 - Batch: 18/18 - Val loss: 5.53488\n",
      "\n",
      "Epoch: 6/50 - Average val loss: 5.55512\n",
      "\n",
      "Epoch: 7/50 - Batch: 9/73 - Train loss: 4.67949\n",
      "Epoch: 7/50 - Batch: 18/73 - Train loss: 4.85088\n",
      "Epoch: 7/50 - Batch: 27/73 - Train loss: 4.64110\n",
      "Epoch: 7/50 - Batch: 36/73 - Train loss: 4.63304\n",
      "Epoch: 7/50 - Batch: 45/73 - Train loss: 4.57417\n",
      "Epoch: 7/50 - Batch: 54/73 - Train loss: 4.49289\n",
      "Epoch: 7/50 - Batch: 63/73 - Train loss: 4.55580\n",
      "Epoch: 7/50 - Batch: 72/73 - Train loss: 4.58866\n",
      "\n",
      "Epoch: 7/50 - Average train loss: 4.67888\n",
      "\n",
      "Epoch: 7/50 - Batch: 9/18 - Val loss: 5.58694\n",
      "Epoch: 7/50 - Batch: 18/18 - Val loss: 5.54522\n",
      "\n",
      "Epoch: 7/50 - Average val loss: 5.56279\n",
      "\n",
      "Epoch: 8/50 - Batch: 9/73 - Train loss: 4.63145\n",
      "Epoch: 8/50 - Batch: 18/73 - Train loss: 4.79014\n",
      "Epoch: 8/50 - Batch: 27/73 - Train loss: 4.58907\n",
      "Epoch: 8/50 - Batch: 36/73 - Train loss: 4.58125\n",
      "Epoch: 8/50 - Batch: 45/73 - Train loss: 4.52563\n",
      "Epoch: 8/50 - Batch: 54/73 - Train loss: 4.45015\n",
      "Epoch: 8/50 - Batch: 63/73 - Train loss: 4.50690\n",
      "Epoch: 8/50 - Batch: 72/73 - Train loss: 4.54230\n",
      "\n",
      "Epoch: 8/50 - Average train loss: 4.62699\n",
      "\n",
      "Epoch: 8/50 - Batch: 9/18 - Val loss: 5.60326\n",
      "Epoch: 8/50 - Batch: 18/18 - Val loss: 5.55759\n",
      "\n",
      "Epoch: 8/50 - Average val loss: 5.57417\n",
      "\n",
      "Epoch: 9/50 - Batch: 9/73 - Train loss: 4.58753\n",
      "Epoch: 9/50 - Batch: 18/73 - Train loss: 4.73502\n",
      "Epoch: 9/50 - Batch: 27/73 - Train loss: 4.54505\n",
      "Epoch: 9/50 - Batch: 36/73 - Train loss: 4.53476\n",
      "Epoch: 9/50 - Batch: 45/73 - Train loss: 4.47948\n",
      "Epoch: 9/50 - Batch: 54/73 - Train loss: 4.40804\n",
      "Epoch: 9/50 - Batch: 63/73 - Train loss: 4.46508\n",
      "Epoch: 9/50 - Batch: 72/73 - Train loss: 4.49849\n",
      "\n",
      "Epoch: 9/50 - Average train loss: 4.58268\n",
      "\n",
      "Epoch: 9/50 - Batch: 9/18 - Val loss: 5.62068\n",
      "Epoch: 9/50 - Batch: 18/18 - Val loss: 5.56994\n",
      "\n",
      "Epoch: 9/50 - Average val loss: 5.58748\n",
      "\n",
      "Epoch: 10/50 - Batch: 9/73 - Train loss: 4.53838\n",
      "Epoch: 10/50 - Batch: 18/73 - Train loss: 4.67768\n",
      "Epoch: 10/50 - Batch: 27/73 - Train loss: 4.50235\n",
      "Epoch: 10/50 - Batch: 36/73 - Train loss: 4.49253\n",
      "Epoch: 10/50 - Batch: 45/73 - Train loss: 4.44725\n",
      "Epoch: 10/50 - Batch: 54/73 - Train loss: 4.37596\n",
      "Epoch: 10/50 - Batch: 63/73 - Train loss: 4.42069\n",
      "Epoch: 10/50 - Batch: 72/73 - Train loss: 4.46419\n",
      "\n",
      "Epoch: 10/50 - Average train loss: 4.54243\n",
      "\n",
      "Epoch: 10/50 - Batch: 9/18 - Val loss: 5.64561\n",
      "Epoch: 10/50 - Batch: 18/18 - Val loss: 5.59362\n",
      "\n",
      "Epoch: 10/50 - Average val loss: 5.60875\n",
      "\n",
      "Epoch: 11/50 - Batch: 9/73 - Train loss: 4.51010\n",
      "Epoch: 11/50 - Batch: 18/73 - Train loss: 4.63138\n",
      "Epoch: 11/50 - Batch: 27/73 - Train loss: 4.44874\n",
      "Epoch: 11/50 - Batch: 36/73 - Train loss: 4.45143\n",
      "Epoch: 11/50 - Batch: 45/73 - Train loss: 4.43002\n",
      "Epoch: 11/50 - Batch: 54/73 - Train loss: 4.35554\n",
      "Epoch: 11/50 - Batch: 63/73 - Train loss: 4.39942\n",
      "Epoch: 11/50 - Batch: 72/73 - Train loss: 4.44498\n",
      "\n",
      "Epoch: 11/50 - Average train loss: 4.51623\n",
      "\n",
      "Epoch: 11/50 - Batch: 9/18 - Val loss: 5.68418\n",
      "Epoch: 11/50 - Batch: 18/18 - Val loss: 5.63120\n",
      "\n",
      "Epoch: 11/50 - Average val loss: 5.64077\n",
      "\n",
      "Epoch: 12/50 - Batch: 9/73 - Train loss: 4.49954\n",
      "Epoch: 12/50 - Batch: 18/73 - Train loss: 4.62547\n",
      "Epoch: 12/50 - Batch: 27/73 - Train loss: 4.45622\n",
      "Epoch: 12/50 - Batch: 36/73 - Train loss: 4.49991\n",
      "Epoch: 12/50 - Batch: 45/73 - Train loss: 4.45099\n",
      "Epoch: 12/50 - Batch: 54/73 - Train loss: 4.35050\n",
      "Epoch: 12/50 - Batch: 63/73 - Train loss: 4.36238\n",
      "Epoch: 12/50 - Batch: 72/73 - Train loss: 4.41631\n",
      "\n",
      "Epoch: 12/50 - Average train loss: 4.49873\n",
      "\n",
      "Epoch: 12/50 - Batch: 9/18 - Val loss: 5.63263\n",
      "Epoch: 12/50 - Batch: 18/18 - Val loss: 5.60356\n",
      "\n",
      "Epoch: 12/50 - Average val loss: 5.60662\n",
      "\n",
      "Epoch: 13/50 - Batch: 9/73 - Train loss: 4.46853\n",
      "Epoch: 13/50 - Batch: 18/73 - Train loss: 4.59640\n",
      "Epoch: 13/50 - Batch: 27/73 - Train loss: 4.42155\n",
      "Epoch: 13/50 - Batch: 36/73 - Train loss: 4.41034\n",
      "Epoch: 13/50 - Batch: 45/73 - Train loss: 4.37525\n",
      "Epoch: 13/50 - Batch: 54/73 - Train loss: 4.30366\n",
      "Epoch: 13/50 - Batch: 63/73 - Train loss: 4.33489\n",
      "Epoch: 13/50 - Batch: 72/73 - Train loss: 4.39186\n",
      "\n",
      "Epoch: 13/50 - Average train loss: 4.45423\n",
      "\n",
      "Epoch: 13/50 - Batch: 9/18 - Val loss: 5.64809\n",
      "Epoch: 13/50 - Batch: 18/18 - Val loss: 5.62291\n",
      "\n",
      "Epoch: 13/50 - Average val loss: 5.62245\n",
      "\n",
      "Epoch: 14/50 - Batch: 9/73 - Train loss: 4.44285\n",
      "Epoch: 14/50 - Batch: 18/73 - Train loss: 4.54801\n",
      "Epoch: 14/50 - Batch: 27/73 - Train loss: 4.39561\n",
      "Epoch: 14/50 - Batch: 36/73 - Train loss: 4.35289\n",
      "Epoch: 14/50 - Batch: 45/73 - Train loss: 4.34472\n",
      "Epoch: 14/50 - Batch: 54/73 - Train loss: 4.26551\n",
      "Epoch: 14/50 - Batch: 63/73 - Train loss: 4.30007\n",
      "Epoch: 14/50 - Batch: 72/73 - Train loss: 4.34565\n",
      "\n",
      "Epoch: 14/50 - Average train loss: 4.41731\n",
      "\n",
      "Epoch: 14/50 - Batch: 9/18 - Val loss: 5.66194\n",
      "Epoch: 14/50 - Batch: 18/18 - Val loss: 5.63591\n",
      "\n",
      "Epoch: 14/50 - Average val loss: 5.62925\n",
      "\n",
      "Epoch: 15/50 - Batch: 9/73 - Train loss: 4.42837\n",
      "Epoch: 15/50 - Batch: 18/73 - Train loss: 4.50190\n",
      "Epoch: 15/50 - Batch: 27/73 - Train loss: 4.35871\n",
      "Epoch: 15/50 - Batch: 36/73 - Train loss: 4.31318\n",
      "Epoch: 15/50 - Batch: 45/73 - Train loss: 4.31894\n",
      "Epoch: 15/50 - Batch: 54/73 - Train loss: 4.26171\n",
      "Epoch: 15/50 - Batch: 63/73 - Train loss: 4.26370\n",
      "Epoch: 15/50 - Batch: 72/73 - Train loss: 4.30923\n",
      "\n",
      "Epoch: 15/50 - Average train loss: 4.38132\n",
      "\n",
      "Epoch: 15/50 - Batch: 9/18 - Val loss: 5.66391\n",
      "Epoch: 15/50 - Batch: 18/18 - Val loss: 5.64898\n",
      "\n",
      "Epoch: 15/50 - Average val loss: 5.63547\n",
      "\n",
      "Epoch: 16/50 - Batch: 9/73 - Train loss: 4.38195\n",
      "Epoch: 16/50 - Batch: 18/73 - Train loss: 4.47582\n",
      "Epoch: 16/50 - Batch: 27/73 - Train loss: 4.32065\n",
      "Epoch: 16/50 - Batch: 36/73 - Train loss: 4.26611\n",
      "Epoch: 16/50 - Batch: 45/73 - Train loss: 4.27439\n",
      "Epoch: 16/50 - Batch: 54/73 - Train loss: 4.23382\n",
      "Epoch: 16/50 - Batch: 63/73 - Train loss: 4.24588\n",
      "Epoch: 16/50 - Batch: 72/73 - Train loss: 4.26871\n",
      "\n",
      "Epoch: 16/50 - Average train loss: 4.34160\n",
      "\n",
      "Epoch: 16/50 - Batch: 9/18 - Val loss: 5.66471\n",
      "Epoch: 16/50 - Batch: 18/18 - Val loss: 5.65519\n",
      "\n",
      "Epoch: 16/50 - Average val loss: 5.64186\n",
      "\n",
      "Epoch: 17/50 - Batch: 9/73 - Train loss: 4.33939\n",
      "Epoch: 17/50 - Batch: 18/73 - Train loss: 4.42241\n",
      "Epoch: 17/50 - Batch: 27/73 - Train loss: 4.29094\n",
      "Epoch: 17/50 - Batch: 36/73 - Train loss: 4.22310\n",
      "Epoch: 17/50 - Batch: 45/73 - Train loss: 4.24584\n",
      "Epoch: 17/50 - Batch: 54/73 - Train loss: 4.17632\n",
      "Epoch: 17/50 - Batch: 63/73 - Train loss: 4.22160\n",
      "Epoch: 17/50 - Batch: 72/73 - Train loss: 4.24363\n",
      "\n",
      "Epoch: 17/50 - Average train loss: 4.30680\n",
      "\n",
      "Epoch: 17/50 - Batch: 9/18 - Val loss: 5.66895\n",
      "Epoch: 17/50 - Batch: 18/18 - Val loss: 5.65569\n",
      "\n",
      "Epoch: 17/50 - Average val loss: 5.64815\n",
      "\n",
      "Epoch: 18/50 - Batch: 9/73 - Train loss: 4.30892\n",
      "Epoch: 18/50 - Batch: 18/73 - Train loss: 4.37286\n",
      "Epoch: 18/50 - Batch: 27/73 - Train loss: 4.24080\n",
      "Epoch: 18/50 - Batch: 36/73 - Train loss: 4.17610\n",
      "Epoch: 18/50 - Batch: 45/73 - Train loss: 4.20931\n",
      "Epoch: 18/50 - Batch: 54/73 - Train loss: 4.14218\n",
      "Epoch: 18/50 - Batch: 63/73 - Train loss: 4.17667\n",
      "Epoch: 18/50 - Batch: 72/73 - Train loss: 4.22794\n",
      "\n",
      "Epoch: 18/50 - Average train loss: 4.26858\n",
      "\n",
      "Epoch: 18/50 - Batch: 9/18 - Val loss: 5.67788\n",
      "Epoch: 18/50 - Batch: 18/18 - Val loss: 5.66004\n",
      "\n",
      "Epoch: 18/50 - Average val loss: 5.65407\n",
      "\n",
      "Epoch: 19/50 - Batch: 9/73 - Train loss: 4.26760\n",
      "Epoch: 19/50 - Batch: 18/73 - Train loss: 4.34383\n",
      "Epoch: 19/50 - Batch: 27/73 - Train loss: 4.19965\n",
      "Epoch: 19/50 - Batch: 36/73 - Train loss: 4.15753\n",
      "Epoch: 19/50 - Batch: 45/73 - Train loss: 4.16607\n",
      "Epoch: 19/50 - Batch: 54/73 - Train loss: 4.11398\n",
      "Epoch: 19/50 - Batch: 63/73 - Train loss: 4.14151\n",
      "Epoch: 19/50 - Batch: 72/73 - Train loss: 4.20198\n",
      "\n",
      "Epoch: 19/50 - Average train loss: 4.23499\n",
      "\n",
      "Epoch: 19/50 - Batch: 9/18 - Val loss: 5.70378\n",
      "Epoch: 19/50 - Batch: 18/18 - Val loss: 5.68757\n",
      "\n",
      "Epoch: 19/50 - Average val loss: 5.67875\n",
      "\n",
      "Epoch: 20/50 - Batch: 9/73 - Train loss: 4.23008\n",
      "Epoch: 20/50 - Batch: 18/73 - Train loss: 4.31629\n",
      "Epoch: 20/50 - Batch: 27/73 - Train loss: 4.16804\n",
      "Epoch: 20/50 - Batch: 36/73 - Train loss: 4.14060\n",
      "Epoch: 20/50 - Batch: 45/73 - Train loss: 4.14939\n",
      "Epoch: 20/50 - Batch: 54/73 - Train loss: 4.07558\n",
      "Epoch: 20/50 - Batch: 63/73 - Train loss: 4.11598\n",
      "Epoch: 20/50 - Batch: 72/73 - Train loss: 4.16590\n",
      "\n",
      "Epoch: 20/50 - Average train loss: 4.20283\n",
      "\n",
      "Epoch: 20/50 - Batch: 9/18 - Val loss: 5.71516\n",
      "Epoch: 20/50 - Batch: 18/18 - Val loss: 5.70896\n",
      "\n",
      "Epoch: 20/50 - Average val loss: 5.69564\n",
      "\n",
      "Epoch: 21/50 - Batch: 9/73 - Train loss: 4.21387\n",
      "Epoch: 21/50 - Batch: 18/73 - Train loss: 4.27332\n",
      "Epoch: 21/50 - Batch: 27/73 - Train loss: 4.14714\n",
      "Epoch: 21/50 - Batch: 36/73 - Train loss: 4.09509\n",
      "Epoch: 21/50 - Batch: 45/73 - Train loss: 4.13351\n",
      "Epoch: 21/50 - Batch: 54/73 - Train loss: 4.04903\n",
      "Epoch: 21/50 - Batch: 63/73 - Train loss: 4.07830\n",
      "Epoch: 21/50 - Batch: 72/73 - Train loss: 4.13964\n",
      "\n",
      "Epoch: 21/50 - Average train loss: 4.17112\n",
      "\n",
      "Epoch: 21/50 - Batch: 9/18 - Val loss: 5.73176\n",
      "Epoch: 21/50 - Batch: 18/18 - Val loss: 5.73031\n",
      "\n",
      "Epoch: 21/50 - Average val loss: 5.71244\n",
      "\n",
      "Epoch: 22/50 - Batch: 9/73 - Train loss: 4.16888\n",
      "Epoch: 22/50 - Batch: 18/73 - Train loss: 4.25134\n",
      "Epoch: 22/50 - Batch: 27/73 - Train loss: 4.11734\n",
      "Epoch: 22/50 - Batch: 36/73 - Train loss: 4.05441\n",
      "Epoch: 22/50 - Batch: 45/73 - Train loss: 4.08893\n",
      "Epoch: 22/50 - Batch: 54/73 - Train loss: 4.02986\n",
      "Epoch: 22/50 - Batch: 63/73 - Train loss: 4.05136\n",
      "Epoch: 22/50 - Batch: 72/73 - Train loss: 4.11170\n",
      "\n",
      "Epoch: 22/50 - Average train loss: 4.14158\n",
      "\n",
      "Epoch: 22/50 - Batch: 9/18 - Val loss: 5.75697\n",
      "Epoch: 22/50 - Batch: 18/18 - Val loss: 5.75395\n",
      "\n",
      "Epoch: 22/50 - Average val loss: 5.73410\n",
      "\n",
      "Epoch: 23/50 - Batch: 9/73 - Train loss: 4.13695\n",
      "Epoch: 23/50 - Batch: 18/73 - Train loss: 4.22735\n",
      "Epoch: 23/50 - Batch: 27/73 - Train loss: 4.08307\n",
      "Epoch: 23/50 - Batch: 36/73 - Train loss: 4.02016\n",
      "Epoch: 23/50 - Batch: 45/73 - Train loss: 4.03927\n",
      "Epoch: 23/50 - Batch: 54/73 - Train loss: 4.00784\n",
      "Epoch: 23/50 - Batch: 63/73 - Train loss: 4.03686\n",
      "Epoch: 23/50 - Batch: 72/73 - Train loss: 4.07898\n",
      "\n",
      "Epoch: 23/50 - Average train loss: 4.11229\n",
      "\n",
      "Epoch: 23/50 - Batch: 9/18 - Val loss: 5.77484\n",
      "Epoch: 23/50 - Batch: 18/18 - Val loss: 5.77072\n",
      "\n",
      "Epoch: 23/50 - Average val loss: 5.75102\n",
      "\n",
      "Epoch: 24/50 - Batch: 9/73 - Train loss: 4.14340\n",
      "Epoch: 24/50 - Batch: 18/73 - Train loss: 4.18433\n",
      "Epoch: 24/50 - Batch: 27/73 - Train loss: 4.07165\n",
      "Epoch: 24/50 - Batch: 36/73 - Train loss: 3.97890\n",
      "Epoch: 24/50 - Batch: 45/73 - Train loss: 4.02187\n",
      "Epoch: 24/50 - Batch: 54/73 - Train loss: 3.97024\n",
      "Epoch: 24/50 - Batch: 63/73 - Train loss: 4.01638\n",
      "Epoch: 24/50 - Batch: 72/73 - Train loss: 4.04332\n",
      "\n",
      "Epoch: 24/50 - Average train loss: 4.08523\n",
      "\n",
      "Epoch: 24/50 - Batch: 9/18 - Val loss: 5.77295\n",
      "Epoch: 24/50 - Batch: 18/18 - Val loss: 5.76716\n",
      "\n",
      "Epoch: 24/50 - Average val loss: 5.74997\n",
      "\n",
      "Epoch: 25/50 - Batch: 9/73 - Train loss: 4.13369\n",
      "Epoch: 25/50 - Batch: 18/73 - Train loss: 4.16114\n",
      "Epoch: 25/50 - Batch: 27/73 - Train loss: 4.03250\n",
      "Epoch: 25/50 - Batch: 36/73 - Train loss: 3.95313\n",
      "Epoch: 25/50 - Batch: 45/73 - Train loss: 3.99722\n",
      "Epoch: 25/50 - Batch: 54/73 - Train loss: 3.95031\n",
      "Epoch: 25/50 - Batch: 63/73 - Train loss: 3.98032\n",
      "Epoch: 25/50 - Batch: 72/73 - Train loss: 4.01965\n",
      "\n",
      "Epoch: 25/50 - Average train loss: 4.05628\n",
      "\n",
      "Epoch: 25/50 - Batch: 9/18 - Val loss: 5.77363\n",
      "Epoch: 25/50 - Batch: 18/18 - Val loss: 5.77164\n",
      "\n",
      "Epoch: 25/50 - Average val loss: 5.75285\n",
      "\n",
      "Epoch: 26/50 - Batch: 9/73 - Train loss: 4.08200\n",
      "Epoch: 26/50 - Batch: 18/73 - Train loss: 4.13913\n",
      "Epoch: 26/50 - Batch: 27/73 - Train loss: 3.99976\n",
      "Epoch: 26/50 - Batch: 36/73 - Train loss: 3.90849\n",
      "Epoch: 26/50 - Batch: 45/73 - Train loss: 3.96834\n",
      "Epoch: 26/50 - Batch: 54/73 - Train loss: 3.92512\n",
      "Epoch: 26/50 - Batch: 63/73 - Train loss: 3.95697\n",
      "Epoch: 26/50 - Batch: 72/73 - Train loss: 4.00135\n",
      "\n",
      "Epoch: 26/50 - Average train loss: 4.02646\n",
      "\n",
      "Epoch: 26/50 - Batch: 9/18 - Val loss: 5.78823\n",
      "Epoch: 26/50 - Batch: 18/18 - Val loss: 5.78191\n",
      "\n",
      "Epoch: 26/50 - Average val loss: 5.76372\n",
      "\n",
      "Epoch: 27/50 - Batch: 9/73 - Train loss: 4.03990\n",
      "Epoch: 27/50 - Batch: 18/73 - Train loss: 4.10542\n",
      "Epoch: 27/50 - Batch: 27/73 - Train loss: 3.98834\n",
      "Epoch: 27/50 - Batch: 36/73 - Train loss: 3.87198\n",
      "Epoch: 27/50 - Batch: 45/73 - Train loss: 3.94598\n",
      "Epoch: 27/50 - Batch: 54/73 - Train loss: 3.91752\n",
      "Epoch: 27/50 - Batch: 63/73 - Train loss: 3.95004\n",
      "Epoch: 27/50 - Batch: 72/73 - Train loss: 3.98811\n",
      "\n",
      "Epoch: 27/50 - Average train loss: 4.00540\n",
      "\n",
      "Epoch: 27/50 - Batch: 9/18 - Val loss: 5.80876\n",
      "Epoch: 27/50 - Batch: 18/18 - Val loss: 5.79731\n",
      "\n",
      "Epoch: 27/50 - Average val loss: 5.77864\n",
      "\n",
      "Epoch: 28/50 - Batch: 9/73 - Train loss: 4.00031\n",
      "Epoch: 28/50 - Batch: 18/73 - Train loss: 4.08868\n",
      "Epoch: 28/50 - Batch: 27/73 - Train loss: 3.95932\n",
      "Epoch: 28/50 - Batch: 36/73 - Train loss: 3.86622\n",
      "Epoch: 28/50 - Batch: 45/73 - Train loss: 3.91170\n",
      "Epoch: 28/50 - Batch: 54/73 - Train loss: 3.90825\n",
      "Epoch: 28/50 - Batch: 63/73 - Train loss: 3.97010\n",
      "Epoch: 28/50 - Batch: 72/73 - Train loss: 3.98146\n",
      "\n",
      "Epoch: 28/50 - Average train loss: 3.98666\n",
      "\n",
      "Epoch: 28/50 - Batch: 9/18 - Val loss: 5.82914\n",
      "Epoch: 28/50 - Batch: 18/18 - Val loss: 5.80725\n",
      "\n",
      "Epoch: 28/50 - Average val loss: 5.78717\n",
      "\n",
      "Epoch: 29/50 - Batch: 9/73 - Train loss: 3.97293\n",
      "Epoch: 29/50 - Batch: 18/73 - Train loss: 4.06128\n",
      "Epoch: 29/50 - Batch: 27/73 - Train loss: 3.91214\n",
      "Epoch: 29/50 - Batch: 36/73 - Train loss: 3.85999\n",
      "Epoch: 29/50 - Batch: 45/73 - Train loss: 3.89540\n",
      "Epoch: 29/50 - Batch: 54/73 - Train loss: 3.86781\n",
      "Epoch: 29/50 - Batch: 63/73 - Train loss: 3.92286\n",
      "Epoch: 29/50 - Batch: 72/73 - Train loss: 3.97856\n",
      "\n",
      "Epoch: 29/50 - Average train loss: 3.96122\n",
      "\n",
      "Epoch: 29/50 - Batch: 9/18 - Val loss: 5.86605\n",
      "Epoch: 29/50 - Batch: 18/18 - Val loss: 5.83010\n",
      "\n",
      "Epoch: 29/50 - Average val loss: 5.80873\n",
      "\n",
      "Epoch: 30/50 - Batch: 9/73 - Train loss: 3.96397\n",
      "Epoch: 30/50 - Batch: 18/73 - Train loss: 4.02157\n",
      "Epoch: 30/50 - Batch: 27/73 - Train loss: 3.87001\n",
      "Epoch: 30/50 - Batch: 36/73 - Train loss: 3.83417\n",
      "Epoch: 30/50 - Batch: 45/73 - Train loss: 3.88440\n",
      "Epoch: 30/50 - Batch: 54/73 - Train loss: 3.82686\n",
      "Epoch: 30/50 - Batch: 63/73 - Train loss: 3.90142\n",
      "Epoch: 30/50 - Batch: 72/73 - Train loss: 3.92213\n",
      "\n",
      "Epoch: 30/50 - Average train loss: 3.93256\n",
      "\n",
      "Epoch: 30/50 - Batch: 9/18 - Val loss: 5.91646\n",
      "Epoch: 30/50 - Batch: 18/18 - Val loss: 5.87378\n",
      "\n",
      "Epoch: 30/50 - Average val loss: 5.84901\n",
      "\n",
      "Epoch: 31/50 - Batch: 9/73 - Train loss: 4.00658\n",
      "Epoch: 31/50 - Batch: 18/73 - Train loss: 3.97541\n",
      "Epoch: 31/50 - Batch: 27/73 - Train loss: 3.86000\n",
      "Epoch: 31/50 - Batch: 36/73 - Train loss: 3.76464\n",
      "Epoch: 31/50 - Batch: 45/73 - Train loss: 3.85364\n",
      "Epoch: 31/50 - Batch: 54/73 - Train loss: 3.78174\n",
      "Epoch: 31/50 - Batch: 63/73 - Train loss: 3.84614\n",
      "Epoch: 31/50 - Batch: 72/73 - Train loss: 3.88481\n",
      "\n",
      "Epoch: 31/50 - Average train loss: 3.90091\n",
      "\n",
      "Epoch: 31/50 - Batch: 9/18 - Val loss: 5.89089\n",
      "Epoch: 31/50 - Batch: 18/18 - Val loss: 5.85700\n",
      "\n",
      "Epoch: 31/50 - Average val loss: 5.83212\n",
      "\n",
      "Epoch: 32/50 - Batch: 9/73 - Train loss: 3.93463\n",
      "Epoch: 32/50 - Batch: 18/73 - Train loss: 3.93930\n",
      "Epoch: 32/50 - Batch: 27/73 - Train loss: 3.80536\n",
      "Epoch: 32/50 - Batch: 36/73 - Train loss: 3.72731\n",
      "Epoch: 32/50 - Batch: 45/73 - Train loss: 3.80279\n",
      "Epoch: 32/50 - Batch: 54/73 - Train loss: 3.74972\n",
      "Epoch: 32/50 - Batch: 63/73 - Train loss: 3.78929\n",
      "Epoch: 32/50 - Batch: 72/73 - Train loss: 3.84207\n",
      "\n",
      "Epoch: 32/50 - Average train loss: 3.84657\n",
      "\n",
      "Epoch: 32/50 - Batch: 9/18 - Val loss: 5.89800\n",
      "Epoch: 32/50 - Batch: 18/18 - Val loss: 5.86752\n",
      "\n",
      "Epoch: 32/50 - Average val loss: 5.84171\n",
      "\n",
      "Epoch: 33/50 - Batch: 9/73 - Train loss: 3.86331\n",
      "Epoch: 33/50 - Batch: 18/73 - Train loss: 3.89017\n",
      "Epoch: 33/50 - Batch: 27/73 - Train loss: 3.76370\n",
      "Epoch: 33/50 - Batch: 36/73 - Train loss: 3.69562\n",
      "Epoch: 33/50 - Batch: 45/73 - Train loss: 3.77323\n",
      "Epoch: 33/50 - Batch: 54/73 - Train loss: 3.72314\n",
      "Epoch: 33/50 - Batch: 63/73 - Train loss: 3.74955\n",
      "Epoch: 33/50 - Batch: 72/73 - Train loss: 3.80742\n",
      "\n",
      "Epoch: 33/50 - Average train loss: 3.80450\n",
      "\n",
      "Epoch: 33/50 - Batch: 9/18 - Val loss: 5.91508\n",
      "Epoch: 33/50 - Batch: 18/18 - Val loss: 5.88389\n",
      "\n",
      "Epoch: 33/50 - Average val loss: 5.85917\n",
      "\n",
      "Epoch: 34/50 - Batch: 9/73 - Train loss: 3.81884\n",
      "Epoch: 34/50 - Batch: 18/73 - Train loss: 3.84912\n",
      "Epoch: 34/50 - Batch: 27/73 - Train loss: 3.73299\n",
      "Epoch: 34/50 - Batch: 36/73 - Train loss: 3.66578\n",
      "Epoch: 34/50 - Batch: 45/73 - Train loss: 3.75050\n",
      "Epoch: 34/50 - Batch: 54/73 - Train loss: 3.69978\n",
      "Epoch: 34/50 - Batch: 63/73 - Train loss: 3.71803\n",
      "Epoch: 34/50 - Batch: 72/73 - Train loss: 3.78073\n",
      "\n",
      "Epoch: 34/50 - Average train loss: 3.77042\n",
      "\n",
      "Epoch: 34/50 - Batch: 9/18 - Val loss: 5.93295\n",
      "Epoch: 34/50 - Batch: 18/18 - Val loss: 5.90306\n",
      "\n",
      "Epoch: 34/50 - Average val loss: 5.87686\n",
      "\n",
      "Epoch: 35/50 - Batch: 9/73 - Train loss: 3.78518\n",
      "Epoch: 35/50 - Batch: 18/73 - Train loss: 3.81335\n",
      "Epoch: 35/50 - Batch: 27/73 - Train loss: 3.70443\n",
      "Epoch: 35/50 - Batch: 36/73 - Train loss: 3.64029\n",
      "Epoch: 35/50 - Batch: 45/73 - Train loss: 3.73006\n",
      "Epoch: 35/50 - Batch: 54/73 - Train loss: 3.68312\n",
      "Epoch: 35/50 - Batch: 63/73 - Train loss: 3.69673\n",
      "Epoch: 35/50 - Batch: 72/73 - Train loss: 3.75972\n",
      "\n",
      "Epoch: 35/50 - Average train loss: 3.74343\n",
      "\n",
      "Epoch: 35/50 - Batch: 9/18 - Val loss: 5.95009\n",
      "Epoch: 35/50 - Batch: 18/18 - Val loss: 5.91711\n",
      "\n",
      "Epoch: 35/50 - Average val loss: 5.89353\n",
      "\n",
      "Epoch: 36/50 - Batch: 9/73 - Train loss: 3.76078\n",
      "Epoch: 36/50 - Batch: 18/73 - Train loss: 3.78823\n",
      "Epoch: 36/50 - Batch: 27/73 - Train loss: 3.67531\n",
      "Epoch: 36/50 - Batch: 36/73 - Train loss: 3.61461\n",
      "Epoch: 36/50 - Batch: 45/73 - Train loss: 3.70875\n",
      "Epoch: 36/50 - Batch: 54/73 - Train loss: 3.66974\n",
      "Epoch: 36/50 - Batch: 63/73 - Train loss: 3.68665\n",
      "Epoch: 36/50 - Batch: 72/73 - Train loss: 3.74715\n",
      "\n",
      "Epoch: 36/50 - Average train loss: 3.71841\n",
      "\n",
      "Epoch: 36/50 - Batch: 9/18 - Val loss: 5.96788\n",
      "Epoch: 36/50 - Batch: 18/18 - Val loss: 5.93612\n",
      "\n",
      "Epoch: 36/50 - Average val loss: 5.90979\n",
      "\n",
      "Epoch: 37/50 - Batch: 9/73 - Train loss: 3.74294\n",
      "Epoch: 37/50 - Batch: 18/73 - Train loss: 3.77524\n",
      "Epoch: 37/50 - Batch: 27/73 - Train loss: 3.65321\n",
      "Epoch: 37/50 - Batch: 36/73 - Train loss: 3.59442\n",
      "Epoch: 37/50 - Batch: 45/73 - Train loss: 3.68142\n",
      "Epoch: 37/50 - Batch: 54/73 - Train loss: 3.65665\n",
      "Epoch: 37/50 - Batch: 63/73 - Train loss: 3.68329\n",
      "Epoch: 37/50 - Batch: 72/73 - Train loss: 3.74203\n",
      "\n",
      "Epoch: 37/50 - Average train loss: 3.70036\n",
      "\n",
      "Epoch: 37/50 - Batch: 9/18 - Val loss: 5.99133\n",
      "Epoch: 37/50 - Batch: 18/18 - Val loss: 5.95310\n",
      "\n",
      "Epoch: 37/50 - Average val loss: 5.92799\n",
      "\n",
      "Epoch: 38/50 - Batch: 9/73 - Train loss: 3.73510\n",
      "Epoch: 38/50 - Batch: 18/73 - Train loss: 3.76666\n",
      "Epoch: 38/50 - Batch: 27/73 - Train loss: 3.64399\n",
      "Epoch: 38/50 - Batch: 36/73 - Train loss: 3.58093\n",
      "Epoch: 38/50 - Batch: 45/73 - Train loss: 3.66157\n",
      "Epoch: 38/50 - Batch: 54/73 - Train loss: 3.63836\n",
      "Epoch: 38/50 - Batch: 63/73 - Train loss: 3.67383\n",
      "Epoch: 38/50 - Batch: 72/73 - Train loss: 3.72916\n",
      "\n",
      "Epoch: 38/50 - Average train loss: 3.68473\n",
      "\n",
      "Epoch: 38/50 - Batch: 9/18 - Val loss: 6.02247\n",
      "Epoch: 38/50 - Batch: 18/18 - Val loss: 5.97886\n",
      "\n",
      "Epoch: 38/50 - Average val loss: 5.95301\n",
      "\n",
      "Epoch: 39/50 - Batch: 9/73 - Train loss: 3.74224\n",
      "Epoch: 39/50 - Batch: 18/73 - Train loss: 3.76953\n",
      "Epoch: 39/50 - Batch: 27/73 - Train loss: 3.64006\n",
      "Epoch: 39/50 - Batch: 36/73 - Train loss: 3.56441\n",
      "Epoch: 39/50 - Batch: 45/73 - Train loss: 3.64488\n",
      "Epoch: 39/50 - Batch: 54/73 - Train loss: 3.61707\n",
      "Epoch: 39/50 - Batch: 63/73 - Train loss: 3.66161\n",
      "Epoch: 39/50 - Batch: 72/73 - Train loss: 3.69711\n",
      "\n",
      "Epoch: 39/50 - Average train loss: 3.67576\n",
      "\n",
      "Epoch: 39/50 - Batch: 9/18 - Val loss: 6.05504\n",
      "Epoch: 39/50 - Batch: 18/18 - Val loss: 6.01077\n",
      "\n",
      "Epoch: 39/50 - Average val loss: 5.98475\n",
      "\n",
      "Epoch: 40/50 - Batch: 9/73 - Train loss: 3.73866\n",
      "Epoch: 40/50 - Batch: 18/73 - Train loss: 3.78318\n",
      "Epoch: 40/50 - Batch: 27/73 - Train loss: 3.65583\n",
      "Epoch: 40/50 - Batch: 36/73 - Train loss: 3.54687\n",
      "Epoch: 40/50 - Batch: 45/73 - Train loss: 3.64568\n",
      "Epoch: 40/50 - Batch: 54/73 - Train loss: 3.60452\n",
      "Epoch: 40/50 - Batch: 63/73 - Train loss: 3.67163\n",
      "Epoch: 40/50 - Batch: 72/73 - Train loss: 3.68805\n",
      "\n",
      "Epoch: 40/50 - Average train loss: 3.67310\n",
      "\n",
      "Epoch: 40/50 - Batch: 9/18 - Val loss: 6.06149\n",
      "Epoch: 40/50 - Batch: 18/18 - Val loss: 6.01874\n",
      "\n",
      "Epoch: 40/50 - Average val loss: 5.99675\n",
      "\n",
      "Epoch: 41/50 - Batch: 9/73 - Train loss: 3.68922\n",
      "Epoch: 41/50 - Batch: 18/73 - Train loss: 3.74765\n",
      "Epoch: 41/50 - Batch: 27/73 - Train loss: 3.64169\n",
      "Epoch: 41/50 - Batch: 36/73 - Train loss: 3.53588\n",
      "Epoch: 41/50 - Batch: 45/73 - Train loss: 3.65253\n",
      "Epoch: 41/50 - Batch: 54/73 - Train loss: 3.63300\n",
      "Epoch: 41/50 - Batch: 63/73 - Train loss: 3.67573\n",
      "Epoch: 41/50 - Batch: 72/73 - Train loss: 3.70676\n",
      "\n",
      "Epoch: 41/50 - Average train loss: 3.66323\n",
      "\n",
      "Epoch: 41/50 - Batch: 9/18 - Val loss: 6.08934\n",
      "Epoch: 41/50 - Batch: 18/18 - Val loss: 6.05282\n",
      "\n",
      "Epoch: 41/50 - Average val loss: 6.02214\n",
      "\n",
      "Epoch: 42/50 - Batch: 9/73 - Train loss: 3.65715\n",
      "Epoch: 42/50 - Batch: 18/73 - Train loss: 3.69615\n",
      "Epoch: 42/50 - Batch: 27/73 - Train loss: 3.60655\n",
      "Epoch: 42/50 - Batch: 36/73 - Train loss: 3.51895\n",
      "Epoch: 42/50 - Batch: 45/73 - Train loss: 3.63909\n",
      "Epoch: 42/50 - Batch: 54/73 - Train loss: 3.61877\n",
      "Epoch: 42/50 - Batch: 63/73 - Train loss: 3.64374\n",
      "Epoch: 42/50 - Batch: 72/73 - Train loss: 3.67300\n",
      "\n",
      "Epoch: 42/50 - Average train loss: 3.64444\n",
      "\n",
      "Epoch: 42/50 - Batch: 9/18 - Val loss: 6.11801\n",
      "Epoch: 42/50 - Batch: 18/18 - Val loss: 6.08672\n",
      "\n",
      "Epoch: 42/50 - Average val loss: 6.05262\n",
      "\n",
      "Epoch: 43/50 - Batch: 9/73 - Train loss: 3.69562\n",
      "Epoch: 43/50 - Batch: 18/73 - Train loss: 3.66908\n",
      "Epoch: 43/50 - Batch: 27/73 - Train loss: 3.57793\n",
      "Epoch: 43/50 - Batch: 36/73 - Train loss: 3.49552\n",
      "Epoch: 43/50 - Batch: 45/73 - Train loss: 3.61676\n",
      "Epoch: 43/50 - Batch: 54/73 - Train loss: 3.58337\n",
      "Epoch: 43/50 - Batch: 63/73 - Train loss: 3.58623\n",
      "Epoch: 43/50 - Batch: 72/73 - Train loss: 3.63600\n",
      "\n",
      "Epoch: 43/50 - Average train loss: 3.62647\n",
      "\n",
      "Epoch: 43/50 - Batch: 9/18 - Val loss: 6.10827\n",
      "Epoch: 43/50 - Batch: 18/18 - Val loss: 6.08328\n",
      "\n",
      "Epoch: 43/50 - Average val loss: 6.05099\n",
      "\n",
      "Epoch: 44/50 - Batch: 9/73 - Train loss: 3.69669\n",
      "Epoch: 44/50 - Batch: 18/73 - Train loss: 3.66935\n",
      "Epoch: 44/50 - Batch: 27/73 - Train loss: 3.55783\n",
      "Epoch: 44/50 - Batch: 36/73 - Train loss: 3.47898\n",
      "Epoch: 44/50 - Batch: 45/73 - Train loss: 3.60791\n",
      "Epoch: 44/50 - Batch: 54/73 - Train loss: 3.57026\n",
      "Epoch: 44/50 - Batch: 63/73 - Train loss: 3.55540\n",
      "Epoch: 44/50 - Batch: 72/73 - Train loss: 3.62597\n",
      "\n",
      "Epoch: 44/50 - Average train loss: 3.60830\n",
      "\n",
      "Epoch: 44/50 - Batch: 9/18 - Val loss: 6.09499\n",
      "Epoch: 44/50 - Batch: 18/18 - Val loss: 6.06391\n",
      "\n",
      "Epoch: 44/50 - Average val loss: 6.04429\n",
      "\n",
      "Epoch: 45/50 - Batch: 9/73 - Train loss: 3.66334\n",
      "Epoch: 45/50 - Batch: 18/73 - Train loss: 3.67139\n",
      "Epoch: 45/50 - Batch: 27/73 - Train loss: 3.54600\n",
      "Epoch: 45/50 - Batch: 36/73 - Train loss: 3.47766\n",
      "Epoch: 45/50 - Batch: 45/73 - Train loss: 3.59597\n",
      "Epoch: 45/50 - Batch: 54/73 - Train loss: 3.55951\n",
      "Epoch: 45/50 - Batch: 63/73 - Train loss: 3.55857\n",
      "Epoch: 45/50 - Batch: 72/73 - Train loss: 3.60195\n",
      "\n",
      "Epoch: 45/50 - Average train loss: 3.58879\n",
      "\n",
      "Epoch: 45/50 - Batch: 9/18 - Val loss: 6.09355\n",
      "Epoch: 45/50 - Batch: 18/18 - Val loss: 6.05608\n",
      "\n",
      "Epoch: 45/50 - Average val loss: 6.04605\n",
      "\n",
      "Epoch: 46/50 - Batch: 9/73 - Train loss: 3.64770\n",
      "Epoch: 46/50 - Batch: 18/73 - Train loss: 3.66283\n",
      "Epoch: 46/50 - Batch: 27/73 - Train loss: 3.52559\n",
      "Epoch: 46/50 - Batch: 36/73 - Train loss: 3.46769\n",
      "Epoch: 46/50 - Batch: 45/73 - Train loss: 3.55861\n",
      "Epoch: 46/50 - Batch: 54/73 - Train loss: 3.52232\n",
      "Epoch: 46/50 - Batch: 63/73 - Train loss: 3.54709\n",
      "Epoch: 46/50 - Batch: 72/73 - Train loss: 3.58448\n",
      "\n",
      "Epoch: 46/50 - Average train loss: 3.56807\n",
      "\n",
      "Epoch: 46/50 - Batch: 9/18 - Val loss: 6.09482\n",
      "Epoch: 46/50 - Batch: 18/18 - Val loss: 6.05119\n",
      "\n",
      "Epoch: 46/50 - Average val loss: 6.04412\n",
      "\n",
      "Epoch: 47/50 - Batch: 9/73 - Train loss: 3.63216\n",
      "Epoch: 47/50 - Batch: 18/73 - Train loss: 3.60254\n",
      "Epoch: 47/50 - Batch: 27/73 - Train loss: 3.49638\n",
      "Epoch: 47/50 - Batch: 36/73 - Train loss: 3.45631\n",
      "Epoch: 47/50 - Batch: 45/73 - Train loss: 3.54514\n",
      "Epoch: 47/50 - Batch: 54/73 - Train loss: 3.47407\n",
      "Epoch: 47/50 - Batch: 63/73 - Train loss: 3.52777\n",
      "Epoch: 47/50 - Batch: 72/73 - Train loss: 3.57732\n",
      "\n",
      "Epoch: 47/50 - Average train loss: 3.54317\n",
      "\n",
      "Epoch: 47/50 - Batch: 9/18 - Val loss: 6.10358\n",
      "Epoch: 47/50 - Batch: 18/18 - Val loss: 6.06513\n",
      "\n",
      "Epoch: 47/50 - Average val loss: 6.05448\n",
      "\n",
      "Epoch: 48/50 - Batch: 9/73 - Train loss: 3.59812\n",
      "Epoch: 48/50 - Batch: 18/73 - Train loss: 3.55119\n",
      "Epoch: 48/50 - Batch: 27/73 - Train loss: 3.48737\n",
      "Epoch: 48/50 - Batch: 36/73 - Train loss: 3.43931\n",
      "Epoch: 48/50 - Batch: 45/73 - Train loss: 3.52424\n",
      "Epoch: 48/50 - Batch: 54/73 - Train loss: 3.44326\n",
      "Epoch: 48/50 - Batch: 63/73 - Train loss: 3.51751\n",
      "Epoch: 48/50 - Batch: 72/73 - Train loss: 3.55613\n",
      "\n",
      "Epoch: 48/50 - Average train loss: 3.51495\n",
      "\n",
      "Epoch: 48/50 - Batch: 9/18 - Val loss: 6.12807\n",
      "Epoch: 48/50 - Batch: 18/18 - Val loss: 6.09570\n",
      "\n",
      "Epoch: 48/50 - Average val loss: 6.07509\n",
      "\n",
      "Epoch: 49/50 - Batch: 9/73 - Train loss: 3.54383\n",
      "Epoch: 49/50 - Batch: 18/73 - Train loss: 3.53362\n",
      "Epoch: 49/50 - Batch: 27/73 - Train loss: 3.48078\n",
      "Epoch: 49/50 - Batch: 36/73 - Train loss: 3.39446\n",
      "Epoch: 49/50 - Batch: 45/73 - Train loss: 3.49681\n",
      "Epoch: 49/50 - Batch: 54/73 - Train loss: 3.43947\n",
      "Epoch: 49/50 - Batch: 63/73 - Train loss: 3.49586\n",
      "Epoch: 49/50 - Batch: 72/73 - Train loss: 3.51563\n",
      "\n",
      "Epoch: 49/50 - Average train loss: 3.49024\n",
      "\n",
      "Epoch: 49/50 - Batch: 9/18 - Val loss: 6.15900\n",
      "Epoch: 49/50 - Batch: 18/18 - Val loss: 6.12668\n",
      "\n",
      "Epoch: 49/50 - Average val loss: 6.10100\n",
      "\n",
      "Epoch: 50/50 - Batch: 9/73 - Train loss: 3.51813\n",
      "Epoch: 50/50 - Batch: 18/73 - Train loss: 3.52027\n",
      "Epoch: 50/50 - Batch: 27/73 - Train loss: 3.43622\n",
      "Epoch: 50/50 - Batch: 36/73 - Train loss: 3.34455\n",
      "Epoch: 50/50 - Batch: 45/73 - Train loss: 3.47511\n",
      "Epoch: 50/50 - Batch: 54/73 - Train loss: 3.43457\n",
      "Epoch: 50/50 - Batch: 63/73 - Train loss: 3.46423\n",
      "Epoch: 50/50 - Batch: 72/73 - Train loss: 3.47871\n",
      "\n",
      "Epoch: 50/50 - Average train loss: 3.46244\n",
      "\n",
      "Epoch: 50/50 - Batch: 9/18 - Val loss: 6.18489\n",
      "Epoch: 50/50 - Batch: 18/18 - Val loss: 6.14857\n",
      "\n",
      "Epoch: 50/50 - Average val loss: 6.12315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ones (50)\n",
    "train_loss_w2v, val_loss_w2v = train(cond_lstm_w2v, batch_size=batch_size, epochs=50, print_every=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fe5a2-a60b-475c-a82e-9d323b290fea",
   "metadata": {},
   "source": [
    "### Training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dde87cb6-819d-468b-8b6e-d7c04a9b76f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 - Batch: 9/73 - Train loss: 8.46450\n",
      "Epoch: 1/10 - Batch: 18/73 - Train loss: 7.20999\n",
      "Epoch: 1/10 - Batch: 27/73 - Train loss: 7.08247\n",
      "Epoch: 1/10 - Batch: 36/73 - Train loss: 7.03150\n",
      "Epoch: 1/10 - Batch: 45/73 - Train loss: 6.88186\n",
      "Epoch: 1/10 - Batch: 54/73 - Train loss: 6.87250\n",
      "Epoch: 1/10 - Batch: 63/73 - Train loss: 6.84805\n",
      "Epoch: 1/10 - Batch: 72/73 - Train loss: 6.87861\n",
      "\n",
      "Epoch: 1/10 - Average train loss: 7.43174\n",
      "\n",
      "Epoch: 1/10 - Batch: 9/18 - Val loss: 7.05984\n",
      "Epoch: 1/10 - Batch: 18/18 - Val loss: 7.06798\n",
      "\n",
      "Epoch: 1/10 - Average val loss: 7.00296\n",
      "\n",
      "Epoch: 2/10 - Batch: 9/73 - Train loss: 6.86131\n",
      "Epoch: 2/10 - Batch: 18/73 - Train loss: 7.04414\n",
      "Epoch: 2/10 - Batch: 27/73 - Train loss: 6.94994\n",
      "Epoch: 2/10 - Batch: 36/73 - Train loss: 6.95191\n",
      "Epoch: 2/10 - Batch: 45/73 - Train loss: 6.82791\n",
      "Epoch: 2/10 - Batch: 54/73 - Train loss: 6.82035\n",
      "Epoch: 2/10 - Batch: 63/73 - Train loss: 6.80660\n",
      "Epoch: 2/10 - Batch: 72/73 - Train loss: 6.83378\n",
      "\n",
      "Epoch: 2/10 - Average train loss: 6.99658\n",
      "\n",
      "Epoch: 2/10 - Batch: 9/18 - Val loss: 7.05332\n",
      "Epoch: 2/10 - Batch: 18/18 - Val loss: 7.06366\n",
      "\n",
      "Epoch: 2/10 - Average val loss: 6.99680\n",
      "\n",
      "Epoch: 3/10 - Batch: 9/73 - Train loss: 6.84921\n",
      "Epoch: 3/10 - Batch: 18/73 - Train loss: 7.04115\n",
      "Epoch: 3/10 - Batch: 27/73 - Train loss: 6.94621\n",
      "Epoch: 3/10 - Batch: 36/73 - Train loss: 6.93883\n",
      "Epoch: 3/10 - Batch: 45/73 - Train loss: 6.82572\n",
      "Epoch: 3/10 - Batch: 54/73 - Train loss: 6.81538\n",
      "Epoch: 3/10 - Batch: 63/73 - Train loss: 6.79122\n",
      "Epoch: 3/10 - Batch: 72/73 - Train loss: 6.82790\n",
      "\n",
      "Epoch: 3/10 - Average train loss: 6.98343\n",
      "\n",
      "Epoch: 3/10 - Batch: 9/18 - Val loss: 7.05709\n",
      "Epoch: 3/10 - Batch: 18/18 - Val loss: 7.06887\n",
      "\n",
      "Epoch: 3/10 - Average val loss: 6.99920\n",
      "\n",
      "Epoch: 4/10 - Batch: 9/73 - Train loss: 6.84522\n",
      "Epoch: 4/10 - Batch: 18/73 - Train loss: 7.03295\n",
      "Epoch: 4/10 - Batch: 27/73 - Train loss: 6.93234\n",
      "Epoch: 4/10 - Batch: 36/73 - Train loss: 6.92282\n",
      "Epoch: 4/10 - Batch: 45/73 - Train loss: 6.81074\n",
      "Epoch: 4/10 - Batch: 54/73 - Train loss: 6.80506\n",
      "Epoch: 4/10 - Batch: 63/73 - Train loss: 6.78781\n",
      "Epoch: 4/10 - Batch: 72/73 - Train loss: 6.81371\n",
      "\n",
      "Epoch: 4/10 - Average train loss: 6.97593\n",
      "\n",
      "Epoch: 4/10 - Batch: 9/18 - Val loss: 7.06067\n",
      "Epoch: 4/10 - Batch: 18/18 - Val loss: 7.07281\n",
      "\n",
      "Epoch: 4/10 - Average val loss: 7.00171\n",
      "\n",
      "Epoch: 5/10 - Batch: 9/73 - Train loss: 6.84075\n",
      "Epoch: 5/10 - Batch: 18/73 - Train loss: 7.02575\n",
      "Epoch: 5/10 - Batch: 27/73 - Train loss: 6.93301\n",
      "Epoch: 5/10 - Batch: 36/73 - Train loss: 6.92466\n",
      "Epoch: 5/10 - Batch: 45/73 - Train loss: 6.81853\n",
      "Epoch: 5/10 - Batch: 54/73 - Train loss: 6.80370\n",
      "Epoch: 5/10 - Batch: 63/73 - Train loss: 6.79590\n",
      "\n",
      "Epoch: 5/10 - Average train loss: 6.97355\n",
      "\n",
      "Epoch: 5/10 - Batch: 9/18 - Val loss: 7.06570\n",
      "Epoch: 5/10 - Batch: 18/18 - Val loss: 7.07733\n",
      "\n",
      "Epoch: 5/10 - Average val loss: 7.00536\n",
      "\n",
      "Epoch: 6/10 - Batch: 9/73 - Train loss: 6.84125\n",
      "Epoch: 6/10 - Batch: 18/73 - Train loss: 7.02584\n",
      "Epoch: 6/10 - Batch: 27/73 - Train loss: 6.93462\n",
      "Epoch: 6/10 - Batch: 36/73 - Train loss: 6.91144\n",
      "Epoch: 6/10 - Batch: 45/73 - Train loss: 6.81351\n",
      "Epoch: 6/10 - Batch: 54/73 - Train loss: 6.79848\n",
      "Epoch: 6/10 - Batch: 63/73 - Train loss: 6.78281\n",
      "Epoch: 6/10 - Batch: 72/73 - Train loss: 6.80887\n",
      "\n",
      "Epoch: 6/10 - Average train loss: 6.97019\n",
      "\n",
      "Epoch: 6/10 - Batch: 9/18 - Val loss: 7.06984\n",
      "Epoch: 6/10 - Batch: 18/18 - Val loss: 7.08214\n",
      "\n",
      "Epoch: 6/10 - Average val loss: 7.00894\n",
      "\n",
      "Epoch: 7/10 - Batch: 9/73 - Train loss: 6.83598\n",
      "Epoch: 7/10 - Batch: 18/73 - Train loss: 7.02362\n",
      "Epoch: 7/10 - Batch: 27/73 - Train loss: 6.92152\n",
      "Epoch: 7/10 - Batch: 36/73 - Train loss: 6.91451\n",
      "Epoch: 7/10 - Batch: 45/73 - Train loss: 6.80535\n",
      "Epoch: 7/10 - Batch: 54/73 - Train loss: 6.79606\n",
      "Epoch: 7/10 - Batch: 63/73 - Train loss: 6.78162\n",
      "Epoch: 7/10 - Batch: 72/73 - Train loss: 6.81001\n",
      "\n",
      "Epoch: 7/10 - Average train loss: 6.96690\n",
      "\n",
      "Epoch: 7/10 - Batch: 9/18 - Val loss: 7.07281\n",
      "Epoch: 7/10 - Batch: 18/18 - Val loss: 7.08519\n",
      "\n",
      "Epoch: 7/10 - Average val loss: 7.01150\n",
      "\n",
      "Epoch: 8/10 - Batch: 9/73 - Train loss: 6.83221\n",
      "Epoch: 8/10 - Batch: 18/73 - Train loss: 7.01650\n",
      "Epoch: 8/10 - Batch: 27/73 - Train loss: 6.92468\n",
      "Epoch: 8/10 - Batch: 36/73 - Train loss: 6.91993\n",
      "Epoch: 8/10 - Batch: 45/73 - Train loss: 6.80761\n",
      "Epoch: 8/10 - Batch: 54/73 - Train loss: 6.79650\n",
      "Epoch: 8/10 - Batch: 63/73 - Train loss: 6.77712\n",
      "Epoch: 8/10 - Batch: 72/73 - Train loss: 6.81122\n",
      "\n",
      "Epoch: 8/10 - Average train loss: 6.96414\n",
      "\n",
      "Epoch: 8/10 - Batch: 9/18 - Val loss: 7.07606\n",
      "Epoch: 8/10 - Batch: 18/18 - Val loss: 7.08784\n",
      "\n",
      "Epoch: 8/10 - Average val loss: 7.01348\n",
      "\n",
      "Epoch: 9/10 - Batch: 9/73 - Train loss: 6.83437\n",
      "Epoch: 9/10 - Batch: 18/73 - Train loss: 7.01695\n",
      "Epoch: 9/10 - Batch: 27/73 - Train loss: 6.92273\n",
      "Epoch: 9/10 - Batch: 36/73 - Train loss: 6.91102\n",
      "Epoch: 9/10 - Batch: 45/73 - Train loss: 6.81139\n",
      "Epoch: 9/10 - Batch: 54/73 - Train loss: 6.79533\n",
      "Epoch: 9/10 - Batch: 63/73 - Train loss: 6.77789\n",
      "Epoch: 9/10 - Batch: 72/73 - Train loss: 6.80438\n",
      "\n",
      "Epoch: 9/10 - Average train loss: 6.96151\n",
      "\n",
      "Epoch: 9/10 - Batch: 9/18 - Val loss: 7.07579\n",
      "Epoch: 9/10 - Batch: 18/18 - Val loss: 7.08816\n",
      "\n",
      "Epoch: 9/10 - Average val loss: 7.01277\n",
      "\n",
      "Epoch: 10/10 - Batch: 9/73 - Train loss: 6.83128\n",
      "Epoch: 10/10 - Batch: 18/73 - Train loss: 7.00943\n",
      "Epoch: 10/10 - Batch: 27/73 - Train loss: 6.91784\n",
      "Epoch: 10/10 - Batch: 36/73 - Train loss: 6.90414\n",
      "Epoch: 10/10 - Batch: 45/73 - Train loss: 6.80192\n",
      "Epoch: 10/10 - Batch: 54/73 - Train loss: 6.79597\n",
      "Epoch: 10/10 - Batch: 63/73 - Train loss: 6.77461\n",
      "Epoch: 10/10 - Batch: 72/73 - Train loss: 6.80261\n",
      "\n",
      "Epoch: 10/10 - Average train loss: 6.95775\n",
      "\n",
      "Epoch: 10/10 - Batch: 9/18 - Val loss: 7.07640\n",
      "Epoch: 10/10 - Batch: 18/18 - Val loss: 7.08862\n",
      "\n",
      "Epoch: 10/10 - Average val loss: 7.01271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_base, val_loss_base = train(lstm, batch_size=batch_size, epochs=10, print_every=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d21383b-5254-49fc-8fad-9e838e81e00d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 - Batch: 9/73 - Train loss: 6.83064\n",
      "Epoch: 1/20 - Batch: 18/73 - Train loss: 7.16712\n",
      "Epoch: 1/20 - Batch: 27/73 - Train loss: 6.99470\n",
      "Epoch: 1/20 - Batch: 36/73 - Train loss: 6.96883\n",
      "Epoch: 1/20 - Batch: 45/73 - Train loss: 6.83091\n",
      "Epoch: 1/20 - Batch: 54/73 - Train loss: 6.81606\n",
      "Epoch: 1/20 - Batch: 63/73 - Train loss: 6.79231\n",
      "Epoch: 1/20 - Batch: 72/73 - Train loss: 6.81642\n",
      "\n",
      "Epoch: 1/20 - Average train loss: 7.05078\n",
      "\n",
      "Epoch: 1/20 - Batch: 9/18 - Val loss: 7.05578\n",
      "Epoch: 1/20 - Batch: 18/18 - Val loss: 7.06100\n",
      "\n",
      "Epoch: 1/20 - Average val loss: 6.99340\n",
      "\n",
      "Epoch: 2/20 - Batch: 9/73 - Train loss: 6.77742\n",
      "Epoch: 2/20 - Batch: 18/73 - Train loss: 6.97837\n",
      "Epoch: 2/20 - Batch: 27/73 - Train loss: 6.89206\n",
      "Epoch: 2/20 - Batch: 36/73 - Train loss: 6.88186\n",
      "Epoch: 2/20 - Batch: 45/73 - Train loss: 6.78793\n",
      "Epoch: 2/20 - Batch: 54/73 - Train loss: 6.77578\n",
      "Epoch: 2/20 - Batch: 63/73 - Train loss: 6.76455\n",
      "Epoch: 2/20 - Batch: 72/73 - Train loss: 6.78660\n",
      "\n",
      "Epoch: 2/20 - Average train loss: 6.93124\n",
      "\n",
      "Epoch: 2/20 - Batch: 9/18 - Val loss: 6.98865\n",
      "Epoch: 2/20 - Batch: 18/18 - Val loss: 7.02272\n",
      "\n",
      "Epoch: 2/20 - Average val loss: 6.95624\n",
      "\n",
      "Epoch: 3/20 - Batch: 9/73 - Train loss: 6.79461\n",
      "Epoch: 3/20 - Batch: 18/73 - Train loss: 6.97449\n",
      "Epoch: 3/20 - Batch: 27/73 - Train loss: 6.88435\n",
      "Epoch: 3/20 - Batch: 36/73 - Train loss: 6.87371\n",
      "Epoch: 3/20 - Batch: 45/73 - Train loss: 6.78202\n",
      "Epoch: 3/20 - Batch: 54/73 - Train loss: 6.76854\n",
      "Epoch: 3/20 - Batch: 63/73 - Train loss: 6.75384\n",
      "Epoch: 3/20 - Batch: 72/73 - Train loss: 6.77786\n",
      "\n",
      "Epoch: 3/20 - Average train loss: 6.86073\n",
      "\n",
      "Epoch: 3/20 - Batch: 9/18 - Val loss: 6.91752\n",
      "Epoch: 3/20 - Batch: 18/18 - Val loss: 6.97735\n",
      "\n",
      "Epoch: 3/20 - Average val loss: 6.90712\n",
      "\n",
      "Epoch: 4/20 - Batch: 9/73 - Train loss: 6.78976\n",
      "Epoch: 4/20 - Batch: 18/73 - Train loss: 6.96419\n",
      "Epoch: 4/20 - Batch: 27/73 - Train loss: 6.87333\n",
      "Epoch: 4/20 - Batch: 36/73 - Train loss: 6.84602\n",
      "Epoch: 4/20 - Batch: 45/73 - Train loss: 6.76814\n",
      "Epoch: 4/20 - Batch: 54/73 - Train loss: 6.75781\n",
      "Epoch: 4/20 - Batch: 63/73 - Train loss: 6.72868\n",
      "Epoch: 4/20 - Batch: 72/73 - Train loss: 6.72978\n",
      "\n",
      "Epoch: 4/20 - Average train loss: 6.82985\n",
      "\n",
      "Epoch: 4/20 - Batch: 9/18 - Val loss: 6.86756\n",
      "Epoch: 4/20 - Batch: 18/18 - Val loss: 6.92596\n",
      "\n",
      "Epoch: 4/20 - Average val loss: 6.85980\n",
      "\n",
      "Epoch: 5/20 - Batch: 9/73 - Train loss: 6.71591\n",
      "Epoch: 5/20 - Batch: 18/73 - Train loss: 6.84503\n",
      "Epoch: 5/20 - Batch: 27/73 - Train loss: 6.71083\n",
      "Epoch: 5/20 - Batch: 36/73 - Train loss: 6.63642\n",
      "Epoch: 5/20 - Batch: 45/73 - Train loss: 6.50192\n",
      "Epoch: 5/20 - Batch: 54/73 - Train loss: 6.43574\n",
      "Epoch: 5/20 - Batch: 63/73 - Train loss: 6.41931\n",
      "Epoch: 5/20 - Batch: 72/73 - Train loss: 6.37838\n",
      "\n",
      "Epoch: 5/20 - Average train loss: 6.62359\n",
      "\n",
      "Epoch: 5/20 - Batch: 9/18 - Val loss: 6.55535\n",
      "Epoch: 5/20 - Batch: 18/18 - Val loss: 6.59706\n",
      "\n",
      "Epoch: 5/20 - Average val loss: 6.53098\n",
      "\n",
      "Epoch: 6/20 - Batch: 9/73 - Train loss: 6.37039\n",
      "Epoch: 6/20 - Batch: 18/73 - Train loss: 6.54029\n",
      "Epoch: 6/20 - Batch: 27/73 - Train loss: 6.39976\n",
      "Epoch: 6/20 - Batch: 36/73 - Train loss: 6.38008\n",
      "Epoch: 6/20 - Batch: 45/73 - Train loss: 6.21473\n",
      "Epoch: 6/20 - Batch: 54/73 - Train loss: 6.17283\n",
      "Epoch: 6/20 - Batch: 63/73 - Train loss: 6.18531\n",
      "Epoch: 6/20 - Batch: 72/73 - Train loss: 6.15092\n",
      "\n",
      "Epoch: 6/20 - Average train loss: 6.34454\n",
      "\n",
      "Epoch: 6/20 - Batch: 9/18 - Val loss: 6.37064\n",
      "Epoch: 6/20 - Batch: 18/18 - Val loss: 6.39151\n",
      "\n",
      "Epoch: 6/20 - Average val loss: 6.32803\n",
      "\n",
      "Epoch: 7/20 - Batch: 9/73 - Train loss: 6.14235\n",
      "Epoch: 7/20 - Batch: 18/73 - Train loss: 6.32079\n",
      "Epoch: 7/20 - Batch: 27/73 - Train loss: 6.16030\n",
      "Epoch: 7/20 - Batch: 36/73 - Train loss: 6.17419\n",
      "Epoch: 7/20 - Batch: 45/73 - Train loss: 5.98879\n",
      "Epoch: 7/20 - Batch: 54/73 - Train loss: 5.95728\n",
      "Epoch: 7/20 - Batch: 63/73 - Train loss: 5.97690\n",
      "Epoch: 7/20 - Batch: 72/73 - Train loss: 5.94644\n",
      "\n",
      "Epoch: 7/20 - Average train loss: 6.12880\n",
      "\n",
      "Epoch: 7/20 - Batch: 9/18 - Val loss: 6.19068\n",
      "Epoch: 7/20 - Batch: 18/18 - Val loss: 6.19900\n",
      "\n",
      "Epoch: 7/20 - Average val loss: 6.13778\n",
      "\n",
      "Epoch: 8/20 - Batch: 9/73 - Train loss: 5.92002\n",
      "Epoch: 8/20 - Batch: 18/73 - Train loss: 6.12502\n",
      "Epoch: 8/20 - Batch: 27/73 - Train loss: 5.93254\n",
      "Epoch: 8/20 - Batch: 36/73 - Train loss: 5.98418\n",
      "Epoch: 8/20 - Batch: 45/73 - Train loss: 5.77382\n",
      "Epoch: 8/20 - Batch: 54/73 - Train loss: 5.75365\n",
      "Epoch: 8/20 - Batch: 63/73 - Train loss: 5.78282\n",
      "Epoch: 8/20 - Batch: 72/73 - Train loss: 5.75795\n",
      "\n",
      "Epoch: 8/20 - Average train loss: 5.92746\n",
      "\n",
      "Epoch: 8/20 - Batch: 9/18 - Val loss: 6.04166\n",
      "Epoch: 8/20 - Batch: 18/18 - Val loss: 6.04685\n",
      "\n",
      "Epoch: 8/20 - Average val loss: 5.97639\n",
      "\n",
      "Epoch: 9/20 - Batch: 9/73 - Train loss: 5.72282\n",
      "Epoch: 9/20 - Batch: 18/73 - Train loss: 5.95288\n",
      "Epoch: 9/20 - Batch: 27/73 - Train loss: 5.74167\n",
      "Epoch: 9/20 - Batch: 36/73 - Train loss: 5.82084\n",
      "Epoch: 9/20 - Batch: 45/73 - Train loss: 5.59598\n",
      "Epoch: 9/20 - Batch: 54/73 - Train loss: 5.59377\n",
      "Epoch: 9/20 - Batch: 63/73 - Train loss: 5.62982\n",
      "Epoch: 9/20 - Batch: 72/73 - Train loss: 5.60888\n",
      "\n",
      "Epoch: 9/20 - Average train loss: 5.75900\n",
      "\n",
      "Epoch: 9/20 - Batch: 9/18 - Val loss: 5.94312\n",
      "Epoch: 9/20 - Batch: 18/18 - Val loss: 5.93702\n",
      "\n",
      "Epoch: 9/20 - Average val loss: 5.86447\n",
      "\n",
      "Epoch: 10/20 - Batch: 9/73 - Train loss: 5.57447\n",
      "Epoch: 10/20 - Batch: 18/73 - Train loss: 5.81473\n",
      "Epoch: 10/20 - Batch: 27/73 - Train loss: 5.58452\n",
      "Epoch: 10/20 - Batch: 36/73 - Train loss: 5.68014\n",
      "Epoch: 10/20 - Batch: 45/73 - Train loss: 5.44767\n",
      "Epoch: 10/20 - Batch: 54/73 - Train loss: 5.45836\n",
      "Epoch: 10/20 - Batch: 63/73 - Train loss: 5.49920\n",
      "Epoch: 10/20 - Batch: 72/73 - Train loss: 5.48873\n",
      "\n",
      "Epoch: 10/20 - Average train loss: 5.62135\n",
      "\n",
      "Epoch: 10/20 - Batch: 9/18 - Val loss: 5.86128\n",
      "Epoch: 10/20 - Batch: 18/18 - Val loss: 5.84615\n",
      "\n",
      "Epoch: 10/20 - Average val loss: 5.77457\n",
      "\n",
      "Epoch: 11/20 - Batch: 9/73 - Train loss: 5.45129\n",
      "Epoch: 11/20 - Batch: 18/73 - Train loss: 5.69872\n",
      "Epoch: 11/20 - Batch: 27/73 - Train loss: 5.45018\n",
      "Epoch: 11/20 - Batch: 36/73 - Train loss: 5.55370\n",
      "Epoch: 11/20 - Batch: 45/73 - Train loss: 5.33305\n",
      "Epoch: 11/20 - Batch: 54/73 - Train loss: 5.34734\n",
      "Epoch: 11/20 - Batch: 63/73 - Train loss: 5.38154\n",
      "Epoch: 11/20 - Batch: 72/73 - Train loss: 5.37558\n",
      "\n",
      "Epoch: 11/20 - Average train loss: 5.50186\n",
      "\n",
      "Epoch: 11/20 - Batch: 9/18 - Val loss: 5.80279\n",
      "Epoch: 11/20 - Batch: 18/18 - Val loss: 5.77818\n",
      "\n",
      "Epoch: 11/20 - Average val loss: 5.70485\n",
      "\n",
      "Epoch: 12/20 - Batch: 9/73 - Train loss: 5.33155\n",
      "Epoch: 12/20 - Batch: 18/73 - Train loss: 5.59794\n",
      "Epoch: 12/20 - Batch: 27/73 - Train loss: 5.34159\n",
      "Epoch: 12/20 - Batch: 36/73 - Train loss: 5.44845\n",
      "Epoch: 12/20 - Batch: 45/73 - Train loss: 5.21682\n",
      "Epoch: 12/20 - Batch: 54/73 - Train loss: 5.25178\n",
      "Epoch: 12/20 - Batch: 63/73 - Train loss: 5.27976\n",
      "Epoch: 12/20 - Batch: 72/73 - Train loss: 5.27916\n",
      "\n",
      "Epoch: 12/20 - Average train loss: 5.39456\n",
      "\n",
      "Epoch: 12/20 - Batch: 9/18 - Val loss: 5.75780\n",
      "Epoch: 12/20 - Batch: 18/18 - Val loss: 5.72249\n",
      "\n",
      "Epoch: 12/20 - Average val loss: 5.65307\n",
      "\n",
      "Epoch: 13/20 - Batch: 9/73 - Train loss: 5.22195\n",
      "Epoch: 13/20 - Batch: 18/73 - Train loss: 5.50995\n",
      "Epoch: 13/20 - Batch: 27/73 - Train loss: 5.23386\n",
      "Epoch: 13/20 - Batch: 36/73 - Train loss: 5.34079\n",
      "Epoch: 13/20 - Batch: 45/73 - Train loss: 5.10646\n",
      "Epoch: 13/20 - Batch: 54/73 - Train loss: 5.14106\n",
      "Epoch: 13/20 - Batch: 63/73 - Train loss: 5.17812\n",
      "Epoch: 13/20 - Batch: 72/73 - Train loss: 5.18849\n",
      "\n",
      "Epoch: 13/20 - Average train loss: 5.29707\n",
      "\n",
      "Epoch: 13/20 - Batch: 9/18 - Val loss: 5.71472\n",
      "Epoch: 13/20 - Batch: 18/18 - Val loss: 5.67677\n",
      "\n",
      "Epoch: 13/20 - Average val loss: 5.60778\n",
      "\n",
      "Epoch: 14/20 - Batch: 9/73 - Train loss: 5.12880\n",
      "Epoch: 14/20 - Batch: 18/73 - Train loss: 5.41836\n",
      "Epoch: 14/20 - Batch: 27/73 - Train loss: 5.13934\n",
      "Epoch: 14/20 - Batch: 36/73 - Train loss: 5.23872\n",
      "Epoch: 14/20 - Batch: 45/73 - Train loss: 5.01866\n",
      "Epoch: 14/20 - Batch: 54/73 - Train loss: 5.05647\n",
      "Epoch: 14/20 - Batch: 63/73 - Train loss: 5.07473\n",
      "Epoch: 14/20 - Batch: 72/73 - Train loss: 5.09762\n",
      "\n",
      "Epoch: 14/20 - Average train loss: 5.20346\n",
      "\n",
      "Epoch: 14/20 - Batch: 9/18 - Val loss: 5.67430\n",
      "Epoch: 14/20 - Batch: 18/18 - Val loss: 5.62815\n",
      "\n",
      "Epoch: 14/20 - Average val loss: 5.56138\n",
      "\n",
      "Epoch: 15/20 - Batch: 9/73 - Train loss: 5.04986\n",
      "Epoch: 15/20 - Batch: 18/73 - Train loss: 5.32876\n",
      "Epoch: 15/20 - Batch: 27/73 - Train loss: 5.04246\n",
      "Epoch: 15/20 - Batch: 36/73 - Train loss: 5.15554\n",
      "Epoch: 15/20 - Batch: 45/73 - Train loss: 4.93959\n",
      "Epoch: 15/20 - Batch: 54/73 - Train loss: 4.97355\n",
      "Epoch: 15/20 - Batch: 63/73 - Train loss: 4.99141\n",
      "Epoch: 15/20 - Batch: 72/73 - Train loss: 5.01661\n",
      "\n",
      "Epoch: 15/20 - Average train loss: 5.11816\n",
      "\n",
      "Epoch: 15/20 - Batch: 9/18 - Val loss: 5.64941\n",
      "Epoch: 15/20 - Batch: 18/18 - Val loss: 5.59300\n",
      "\n",
      "Epoch: 15/20 - Average val loss: 5.52837\n",
      "\n",
      "Epoch: 16/20 - Batch: 9/73 - Train loss: 4.96639\n",
      "Epoch: 16/20 - Batch: 18/73 - Train loss: 5.24546\n",
      "Epoch: 16/20 - Batch: 27/73 - Train loss: 4.95580\n",
      "Epoch: 16/20 - Batch: 36/73 - Train loss: 5.07931\n",
      "Epoch: 16/20 - Batch: 45/73 - Train loss: 4.85913\n",
      "Epoch: 16/20 - Batch: 54/73 - Train loss: 4.90511\n",
      "Epoch: 16/20 - Batch: 63/73 - Train loss: 4.91164\n",
      "Epoch: 16/20 - Batch: 72/73 - Train loss: 4.94520\n",
      "\n",
      "Epoch: 16/20 - Average train loss: 5.04174\n",
      "\n",
      "Epoch: 16/20 - Batch: 9/18 - Val loss: 5.63719\n",
      "Epoch: 16/20 - Batch: 18/18 - Val loss: 5.57358\n",
      "\n",
      "Epoch: 16/20 - Average val loss: 5.50846\n",
      "\n",
      "Epoch: 17/20 - Batch: 9/73 - Train loss: 4.88901\n",
      "Epoch: 17/20 - Batch: 18/73 - Train loss: 5.17059\n",
      "Epoch: 17/20 - Batch: 27/73 - Train loss: 4.88674\n",
      "Epoch: 17/20 - Batch: 36/73 - Train loss: 5.00307\n",
      "Epoch: 17/20 - Batch: 45/73 - Train loss: 4.79462\n",
      "Epoch: 17/20 - Batch: 54/73 - Train loss: 4.84044\n",
      "Epoch: 17/20 - Batch: 63/73 - Train loss: 4.83729\n",
      "Epoch: 17/20 - Batch: 72/73 - Train loss: 4.87984\n",
      "\n",
      "Epoch: 17/20 - Average train loss: 4.97144\n",
      "\n",
      "Epoch: 17/20 - Batch: 9/18 - Val loss: 5.61905\n",
      "Epoch: 17/20 - Batch: 18/18 - Val loss: 5.55197\n",
      "\n",
      "Epoch: 17/20 - Average val loss: 5.48505\n",
      "\n",
      "Epoch: 18/20 - Batch: 9/73 - Train loss: 4.82706\n",
      "Epoch: 18/20 - Batch: 18/73 - Train loss: 5.10563\n",
      "Epoch: 18/20 - Batch: 27/73 - Train loss: 4.81496\n",
      "Epoch: 18/20 - Batch: 36/73 - Train loss: 4.90733\n",
      "Epoch: 18/20 - Batch: 45/73 - Train loss: 4.72068\n",
      "Epoch: 18/20 - Batch: 54/73 - Train loss: 4.78502\n",
      "Epoch: 18/20 - Batch: 63/73 - Train loss: 4.78412\n",
      "Epoch: 18/20 - Batch: 72/73 - Train loss: 4.82054\n",
      "\n",
      "Epoch: 18/20 - Average train loss: 4.90706\n",
      "\n",
      "Epoch: 18/20 - Batch: 9/18 - Val loss: 5.61918\n",
      "Epoch: 18/20 - Batch: 18/18 - Val loss: 5.54930\n",
      "\n",
      "Epoch: 18/20 - Average val loss: 5.48075\n",
      "\n",
      "Epoch: 19/20 - Batch: 9/73 - Train loss: 4.76815\n",
      "Epoch: 19/20 - Batch: 18/73 - Train loss: 5.04080\n",
      "Epoch: 19/20 - Batch: 27/73 - Train loss: 4.74706\n",
      "Epoch: 19/20 - Batch: 36/73 - Train loss: 4.85752\n",
      "Epoch: 19/20 - Batch: 45/73 - Train loss: 4.67127\n",
      "Epoch: 19/20 - Batch: 54/73 - Train loss: 4.71650\n",
      "Epoch: 19/20 - Batch: 63/73 - Train loss: 4.71949\n",
      "Epoch: 19/20 - Batch: 72/73 - Train loss: 4.76715\n",
      "\n",
      "Epoch: 19/20 - Average train loss: 4.84474\n",
      "\n",
      "Epoch: 19/20 - Batch: 9/18 - Val loss: 5.62329\n",
      "Epoch: 19/20 - Batch: 18/18 - Val loss: 5.55156\n",
      "\n",
      "Epoch: 19/20 - Average val loss: 5.47653\n",
      "\n",
      "Epoch: 20/20 - Batch: 9/73 - Train loss: 4.71804\n",
      "Epoch: 20/20 - Batch: 18/73 - Train loss: 4.97632\n",
      "Epoch: 20/20 - Batch: 27/73 - Train loss: 4.70141\n",
      "Epoch: 20/20 - Batch: 36/73 - Train loss: 4.80317\n",
      "Epoch: 20/20 - Batch: 45/73 - Train loss: 4.61447\n",
      "Epoch: 20/20 - Batch: 63/73 - Train loss: 4.64880\n",
      "Epoch: 20/20 - Batch: 72/73 - Train loss: 4.71232\n",
      "\n",
      "Epoch: 20/20 - Average train loss: 4.78562\n",
      "\n",
      "Epoch: 20/20 - Batch: 9/18 - Val loss: 5.60623\n",
      "Epoch: 20/20 - Batch: 18/18 - Val loss: 5.53615\n",
      "\n",
      "Epoch: 20/20 - Average val loss: 5.45915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_base_w2v, val_loss_base_w2v = train(lstm_w2v, batch_size=batch_size, epochs=20, print_every=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4aad8-2e56-4b3a-af68-e8b1b583acfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_base_w2v, val_loss_base_w2v = train(lstm_w2v, batch_size=batch_size, epochs=20, print_every=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337e62b-9d33-4eea-b44d-5a1698055457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = train(cond_lstm, batch_size=batch_size, epochs=20, print_every=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb7d2b-d2a1-486c-a354-c78b2548f41c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_w2v, val_loss_w2v = train(cond_lstm_w2v, batch_size=batch_size, epochs=20, print_every=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89627b-3be6-44f7-9fda-4fdf55d6c71f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8800402e-da35-4466-9735-896cf7640a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cond_lstm_w2v.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc5115f3-1009-448a-9560-4200dc90928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_lstm_w2v.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b514b226-8530-4fcf-9bf1-1b91cef49268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CondLSTM_Word2Vec(\n",
       "  (emb_layer): Embedding(17862, 100)\n",
       "  (lstm): LSTM(100, 256, num_layers=3, batch_first=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=17862, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_lstm_w2v.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7f627ac-1cca-4628-97a9-731307742eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_layer.weight \t torch.Size([17862, 100])\n",
      "lstm.weight_ih_l0 \t torch.Size([1024, 100])\n",
      "lstm.weight_hh_l0 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 \t torch.Size([1024])\n",
      "lstm.bias_hh_l0 \t torch.Size([1024])\n",
      "lstm.weight_ih_l1 \t torch.Size([1024, 256])\n",
      "lstm.weight_hh_l1 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l1 \t torch.Size([1024])\n",
      "lstm.bias_hh_l1 \t torch.Size([1024])\n",
      "lstm.weight_ih_l2 \t torch.Size([1024, 256])\n",
      "lstm.weight_hh_l2 \t torch.Size([1024, 256])\n",
      "lstm.bias_ih_l2 \t torch.Size([1024])\n",
      "lstm.bias_hh_l2 \t torch.Size([1024])\n",
      "fc.weight \t torch.Size([17862, 256])\n",
      "fc.bias \t torch.Size([17862])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in cond_lstm_w2v.state_dict():\n",
    "    print(param_tensor, \"\\t\", cond_lstm_w2v.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2cfe-fa23-43f0-ac9a-873c02886fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fec25ee-1384-4327-bf8f-2b574865057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word):\n",
    "    return w2v_model.wv.key_to_index[word]\n",
    "\n",
    "def id_to_word(id):\n",
    "    return w2v_model.wv.index_to_key[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1542bc82-cb1a-4ddc-af39-be52eb00bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(model, t, h=None): # default value as None for first iteration\n",
    "         \n",
    "    # tensor inputs\n",
    "    x = np.array([[word_to_id(t)]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top n values\n",
    "    top_ids = p.argsort()[-10:][::-1]\n",
    "\n",
    "    # sample id of next word from top n values\n",
    "    next_id = top_ids[random.sample([0,1,2,3,4,5,6,7,8,9],1)[0]]\n",
    "\n",
    "    # return the value of the predicted word and the hidden state\n",
    "    return id_to_word(next_id), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d200b9a-586c-46af-89f6-2e2f05839baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cdc8a3c-73b7-4139-8c51-38f9fecf82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for generation batch size\n",
    "gen_pca_topics = PCA(n_components=n_layers * gen_batch_size, svd_solver='full').fit_transform(trans_topics)\n",
    "gen_pca_trans = np.transpose(gen_pca_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "913b9806-ea23-469f-89a6-0d7ff46a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate(model=cond_lstm, n=10, prompt='in this paper'):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    h = (torch.FloatTensor(gen_pca_trans.reshape(n_layers, gen_batch_size, n_hidden)),\n",
    "         torch.ones(n_layers, gen_batch_size, n_hidden))\n",
    "\n",
    "    words = prompt.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prompt.split():\n",
    "        token, h = predict(model, t, h)\n",
    "    \n",
    "    words.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(n-1):\n",
    "        token, h = predict(model, words[-1], h)\n",
    "        words.append(token)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac48455d-a6fa-4a85-9ea2-09575bcbb6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper comprehended sslc avoids mwetoolkit mwetoolkit acknowledge there tta there abbreviation'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adab21b4-3b4c-42bb-be3c-a66726aaaafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper show this system for automatic and syntactic representations as the word between text we also use two datasets that are able to achieve more useful performance on both languages with other domains to extract different sentences we evaluate an algorithm for predicting this analysis of text and then show in'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be8a26be-7a2e-4e3c-be86-fff764e819fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here we propose a better approach to this work on a diverse data annotated using large corpora for a common language we present a novel corpus in particular in an unambiguous domain specific language learning however existing tools is expensive to obtain and its impact of insufficient quality we consider them we explore how two categories derived structures are calculated adjectives we observethat a given text corpus crawled on three languages i i are a challenging domain the paper is one of them into any popular real world world data with this data from different sources including a number in a standardized repository to users it is'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=cond_lstm_w2v, n=100, prompt='here we propose a better approach to')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b810e-1244-4a71-99df-e6bfd337b7d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78c1fb7f-38c5-4b5d-b974-884aba2e49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {lstm: 'LSTM', lstm_w2v: 'LSTM + Word2Vec', cond_lstm: 'Conditioned LSTM', cond_lstm_w2v: 'Conditioned LSTM + Word2Vec'}\n",
    "loss = {lstm: val_loss_base, lstm_cond_lstm: val_loss_base_w2v, cond_lstm: val_loss, cond_lstm_w2v: val_loss_w2v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36ea3e07-fa5c-48b8-9950-0299976cedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimimum validation loss within a set num of epochs\n",
    "def min_val_loss(model, max_epochs=100):\n",
    "    return min(loss[model][:max_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "215f2919-77c9-487f-acb4-2bca4b25fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss for Conditioned LSTM: 5.66657\n",
      "Perplexity for model Conditioned LSTM: 289.04\n",
      "\n",
      "Minimum validation loss for Conditioned LSTM + Word2Vec: 5.53955\n",
      "Perplexity for model Conditioned LSTM + Word2Vec: 254.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models.keys():\n",
    "    print(\"Minimum validation loss for {}: {:.5f}\".format(models[m], min_val_loss(m, 50)))\n",
    "    print(\"Perplexity for model {}: {:.2f}\\n\".format(models[m], math.exp(min_val_loss(m))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
